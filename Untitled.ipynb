{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2894c782-4117-40e4-b501-2fbb42b7f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "& was unexpected at this time.\n"
     ]
    }
   ],
   "source": [
    "##  Some installations  ..\n",
    "# !pip install openai datasets pandas nlp numpy fastparquet torch transformers evaluate rouge_score bert_score rouge rouge-metric huggingface_hub\n",
    "# !python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9132427-5c6f-4386-b318-ff29d9a45d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Some Necessary imports  ..\n",
    "import nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc96a428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.</td>\n",
       "      <td>what is rba?</td>\n",
       "      <td>Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$8,000 - $15,000. Cost to patent an electronics invention. “If you've got a mechanical invention, [a patent will] cost between $5,000 to $10,000 including attorney's fees and filing fees. If you're talking about electronics, the cost goes up to more like $8,000 to $15,000. $100,000 or more for worldwide patent rights. “Getting even a simple patent approved -- a process known as ‘prosecuting’ the patent application -- can cost up to $10,000 domestically and $100,000 or more for worldwide rights…” Cost of getting a patent.</td>\n",
       "      <td>cost to get a patent?</td>\n",
       "      <td>The most expensive patents are international patents, which can run up to $100,000 or higher.Domestically the costs can be $10,000 or above.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike the somatic cells, gametes are haploid cells, which carry unpaired chromosomes. A gamete of a particular multicellular organism always carries only half the number of chromosomes carried by a somatic cell of that particular organism. • Somatic cells contain homologous pairs of chromosomes, whereas gametes contain only unpaired chromosomes. • Somatic cells form internal and external structures of the body, whereas gametes do not. • Somatic cells are found almost everywhere in the body, whereas gametes are restricted to certain parts.</td>\n",
       "      <td>in animals somatic cells are produced by and gametic cells are produced by?</td>\n",
       "      <td>Somatic cells are produced by mitosis and gametes produced by most organisms combine to form a zygote with n pairs of chromosomes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Origin of CAMERATA. New Latin, from Latin, neuter plural of cameratus, past participle of camerare to arch, from camera arched roof — more at chamber. This word doesn't usually appear in our free dictionary, but the definition from our premium Unabridged Dictionary is offered here on a limited basis. Note that some information is displayed differently in the Unabridged. To access the complete Unabridged Dictionary, with an additional 300,000 words that aren't in our free dictionary, start a free trial.</td>\n",
       "      <td>what is a camerata?</td>\n",
       "      <td>The Camerata are a group of four powerful and influential individuals in the city of Cloudbank. Established by Grant Kendrell and Royce Bracket, the group circumvents the official administration and democratic nature of Cloudbank, in order to establish some form of stability in the ever-changing city.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHOTOSYNTHESIS. INTRODUCTION. Photosynthesis, process by which green plants and certain other organisms use the energy of light to convert carbon dioxide and water into the simple sugar glucose. In so doing, photosynthesis provides the basic energy source for virtually all organisms. Photosynthesis, process by which green plants and certain other organisms use the energy of light to convert carbon dioxide and water into the simple sugar glucose</td>\n",
       "      <td>what energy is used in photosynthesis?</td>\n",
       "      <td>Photosynthesis is a process used by plants and other organisms to convert light energy, normally from the Sun, into chemical energy that can be later released to fuel the organisms' activities.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Context  \\\n",
       "0                                                       The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.   \n",
       "1                     $8,000 - $15,000. Cost to patent an electronics invention. “If you've got a mechanical invention, [a patent will] cost between $5,000 to $10,000 including attorney's fees and filing fees. If you're talking about electronics, the cost goes up to more like $8,000 to $15,000. $100,000 or more for worldwide patent rights. “Getting even a simple patent approved -- a process known as ‘prosecuting’ the patent application -- can cost up to $10,000 domestically and $100,000 or more for worldwide rights…” Cost of getting a patent.   \n",
       "2  Unlike the somatic cells, gametes are haploid cells, which carry unpaired chromosomes. A gamete of a particular multicellular organism always carries only half the number of chromosomes carried by a somatic cell of that particular organism. • Somatic cells contain homologous pairs of chromosomes, whereas gametes contain only unpaired chromosomes. • Somatic cells form internal and external structures of the body, whereas gametes do not. • Somatic cells are found almost everywhere in the body, whereas gametes are restricted to certain parts.   \n",
       "3                                        Origin of CAMERATA. New Latin, from Latin, neuter plural of cameratus, past participle of camerare to arch, from camera arched roof — more at chamber. This word doesn't usually appear in our free dictionary, but the definition from our premium Unabridged Dictionary is offered here on a limited basis. Note that some information is displayed differently in the Unabridged. To access the complete Unabridged Dictionary, with an additional 300,000 words that aren't in our free dictionary, start a free trial.   \n",
       "4                                                                                                   PHOTOSYNTHESIS. INTRODUCTION. Photosynthesis, process by which green plants and certain other organisms use the energy of light to convert carbon dioxide and water into the simple sugar glucose. In so doing, photosynthesis provides the basic energy source for virtually all organisms. Photosynthesis, process by which green plants and certain other organisms use the energy of light to convert carbon dioxide and water into the simple sugar glucose   \n",
       "\n",
       "                                                                       Question  \\\n",
       "0                                                                 what is rba?    \n",
       "1                                                        cost to get a patent?    \n",
       "2  in animals somatic cells are produced by and gametic cells are produced by?    \n",
       "3                                                          what is a camerata?    \n",
       "4                                       what energy is used in photosynthesis?    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                        Reference  \n",
       "0                                                                                                                Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.  \n",
       "1                                                                                                                                                                    The most expensive patents are international patents, which can run up to $100,000 or higher.Domestically the costs can be $10,000 or above.  \n",
       "2                                                                                                                                                                              Somatic cells are produced by mitosis and gametes produced by most organisms combine to form a zygote with n pairs of chromosomes.  \n",
       "3  The Camerata are a group of four powerful and influential individuals in the city of Cloudbank. Established by Grant Kendrell and Royce Bracket, the group circumvents the official administration and democratic nature of Cloudbank, in order to establish some form of stability in the ever-changing city.  \n",
       "4                                                                                                               Photosynthesis is a process used by plants and other organisms to convert light energy, normally from the Sun, into chemical energy that can be later released to fuel the organisms' activities.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading MS Marco LFQA dataset ..\n",
    "df1 = pd.read_parquet('data.parquet')\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9430aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 'Top Healthcare Stocks' to Buy in March</td>\n",
       "      <td>Find out why our Motley Fool contributors called out these three businesses as top opportunities right now.</td>\n",
       "      <td>\\nThe healthcare market is huge. Americans alone spend more than $3 trillion each year to keep healthy. The numbers only get crazier when you include the rest of the world.\\nA market that big is bound to create opportunities for investors. So which healthcare stocks do we have our eyes on right now? We asked a team of Motley Fool contributors to weigh in, and they called out Insulet (PODD -1.29%), HealthEquity (HQY -0.58%), and CVS Health (CVS -0.64%).\\n\\nImage source: Getty Images.\\n\\nBuying this diabetes stock now could pay off\\nTodd Campbell (Insulet): Investor interest in Insulet slipped following Tandem Diabetes Care's (TNDM -0.82%) foray into automated insulin delivery last year. However, shares could rebound ahead of Insulet's own automated solution launching in 2020.\\nDespite Tandem Diabetes' t:slim X2 rollout, Insulet still managed to deliver solid double-digit sales growth last year. Expanded Medicare and Medicaid access helped full-year revenue increase 22% to a record $563.8 million. Sales growth and improving margins because of volume growth and bringing European sales in-house resulted in Insulet reporting its first full-year operating profit last year, too.\\nIn 2019, shifting to the pharmacy channel will create a $68 million revenue headwind because it eliminates upfront product revenue; however, the move to pay-as-you-go pricing is expected to be revenue neutral for the full year. Insulet expects sales will climb 17% to 22%, to between $662 million and $687 million this year. Also, management expects a mid-single-digit operating margin this year, despite R&amp;D and manufacturing investments.\\n\\n\\n\\n\\nThe launch of its Horizon automated insulin system in the second half of 2020 could be a significant catalyst. Management is targeting over $1 billion in sales and mid-teens operating margin in 2021. The potential for ongoing Omnipod growth and Horizon's availability next year suggest this company's on the cusp of a big move higher over the next couple years.\\nCheck out the latest earnings call transcript for Insulet, HealthEquity, and CVS Health.\\nRiding a mega-trend\\nBrian Feroldi (HealthEquity): I'm a picky investor. I only like to get behind companies that are growing fast, have a great management team, are addressing a massive opportunity, and are already profitable. That's asking a lot of any business, but HealthEquity aces my test with ease.\\nHealthEquity is the No. 2 provider of health savings accounts (HSAs) in the U.S. HSAs have taken off in popularity over the last decade because they offer a triple-tax benefit that helps employers and employees to reduce their health insurance costs.\\nThis might sound like a boring business, but it produces mouth-watering financials. HealthEquity monetizes its customers in four separate ways, which makes its top line very predictable. The company has also reached a scale that allows it to crank out consistent profits and drive its margins higher. These factors have led to amazing growth on the bottom line.\\n\\n\\n\\n\\nHealthEquity's financial statements alone make this a great business to study, but I'm equally as impressed with the people running the show. The company's founder is a former trauma surgeon and also the younger brother of JetBlue's founder. HealthEquity CEO Jon Kessler is an entrepreneur himself and gets high praise from employees on sites like Glassdoor.\\nTo top it all off, HealthEquity believes that it is still in the very early days of its growth phase and that the opportunity ahead of the business is huge. Wall Street agrees and is currently projecting that earnings will grow in excess of 24% annually over the next five years.\\nHealthEquity checks off all of the boxes that I look for in a great investment. When that happens, I don't mind paying up to get my hands on the stock. That's why I think the company is still a great company to check out today, even though shares are trading for more than 60 times next year's earnings estimates.\\nA one-stop shop for the most common healthcare needs\\nChuck Saletta (CVS Health): If you've ever needed more healthcare than a standard checkup or basic preventive services, you probably know how confusing and time-consuming the process can be. The doctor, the insurance company, the pharmacy, the lab service, and virtually everyone and everything else involved all have their own sets of processes, guidelines, and paperwork.\\n\\n\\n\\n\\nWith all that is involved, you almost can't help but wonder how much of the cost is driven by the overhead and other process and paperwork, rather than the actual practice of medicine itself. There's clearly a need to break through all that clutter and drive efficiency, simplification, and streamlined healthcare service delivery. If there's one company that's well positioned to do that, it's CVS.\\nSince its recent acquisition of health insurer Aetna, CVS now operates a substantial vertically integrated healthcare delivery system. You can get insured by CVS/Aetna, seek out basic healthcare at a Minute Clinic inside a CVS store, get your prescription filled at the CVS pharmacy, and find over-the-counter treatments on CVS' shelves. With one company in the mix instead of several, it should be possible to minimize the overhead costs, delivering savings to consumers while still earning a fair return.\\nDespite that incredible potential, CVS' shares can be purchased at a relative bargain of less than eight times the company's anticipated earnings. At that price, combined with its 3.8% yield, even if the company doesn't revolutionize healthcare delivery, investors buying today are getting a decent chance at a reasonable return for their risks.\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0  3 'Top Healthcare Stocks' to Buy in March   \n",
       "\n",
       "                                                                                                          text  \\\n",
       "0  Find out why our Motley Fool contributors called out these three businesses as top opportunities right now.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    description  \n",
       "0  \\nThe healthcare market is huge. Americans alone spend more than $3 trillion each year to keep healthy. The numbers only get crazier when you include the rest of the world.\\nA market that big is bound to create opportunities for investors. So which healthcare stocks do we have our eyes on right now? We asked a team of Motley Fool contributors to weigh in, and they called out Insulet (PODD -1.29%), HealthEquity (HQY -0.58%), and CVS Health (CVS -0.64%).\\n\\nImage source: Getty Images.\\n\\nBuying this diabetes stock now could pay off\\nTodd Campbell (Insulet): Investor interest in Insulet slipped following Tandem Diabetes Care's (TNDM -0.82%) foray into automated insulin delivery last year. However, shares could rebound ahead of Insulet's own automated solution launching in 2020.\\nDespite Tandem Diabetes' t:slim X2 rollout, Insulet still managed to deliver solid double-digit sales growth last year. Expanded Medicare and Medicaid access helped full-year revenue increase 22% to a record $563.8 million. Sales growth and improving margins because of volume growth and bringing European sales in-house resulted in Insulet reporting its first full-year operating profit last year, too.\\nIn 2019, shifting to the pharmacy channel will create a $68 million revenue headwind because it eliminates upfront product revenue; however, the move to pay-as-you-go pricing is expected to be revenue neutral for the full year. Insulet expects sales will climb 17% to 22%, to between $662 million and $687 million this year. Also, management expects a mid-single-digit operating margin this year, despite R&D and manufacturing investments.\\n\\n\\n\\n\\nThe launch of its Horizon automated insulin system in the second half of 2020 could be a significant catalyst. Management is targeting over $1 billion in sales and mid-teens operating margin in 2021. The potential for ongoing Omnipod growth and Horizon's availability next year suggest this company's on the cusp of a big move higher over the next couple years.\\nCheck out the latest earnings call transcript for Insulet, HealthEquity, and CVS Health.\\nRiding a mega-trend\\nBrian Feroldi (HealthEquity): I'm a picky investor. I only like to get behind companies that are growing fast, have a great management team, are addressing a massive opportunity, and are already profitable. That's asking a lot of any business, but HealthEquity aces my test with ease.\\nHealthEquity is the No. 2 provider of health savings accounts (HSAs) in the U.S. HSAs have taken off in popularity over the last decade because they offer a triple-tax benefit that helps employers and employees to reduce their health insurance costs.\\nThis might sound like a boring business, but it produces mouth-watering financials. HealthEquity monetizes its customers in four separate ways, which makes its top line very predictable. The company has also reached a scale that allows it to crank out consistent profits and drive its margins higher. These factors have led to amazing growth on the bottom line.\\n\\n\\n\\n\\nHealthEquity's financial statements alone make this a great business to study, but I'm equally as impressed with the people running the show. The company's founder is a former trauma surgeon and also the younger brother of JetBlue's founder. HealthEquity CEO Jon Kessler is an entrepreneur himself and gets high praise from employees on sites like Glassdoor.\\nTo top it all off, HealthEquity believes that it is still in the very early days of its growth phase and that the opportunity ahead of the business is huge. Wall Street agrees and is currently projecting that earnings will grow in excess of 24% annually over the next five years.\\nHealthEquity checks off all of the boxes that I look for in a great investment. When that happens, I don't mind paying up to get my hands on the stock. That's why I think the company is still a great company to check out today, even though shares are trading for more than 60 times next year's earnings estimates.\\nA one-stop shop for the most common healthcare needs\\nChuck Saletta (CVS Health): If you've ever needed more healthcare than a standard checkup or basic preventive services, you probably know how confusing and time-consuming the process can be. The doctor, the insurance company, the pharmacy, the lab service, and virtually everyone and everything else involved all have their own sets of processes, guidelines, and paperwork.\\n\\n\\n\\n\\nWith all that is involved, you almost can't help but wonder how much of the cost is driven by the overhead and other process and paperwork, rather than the actual practice of medicine itself. There's clearly a need to break through all that clutter and drive efficiency, simplification, and streamlined healthcare service delivery. If there's one company that's well positioned to do that, it's CVS.\\nSince its recent acquisition of health insurer Aetna, CVS now operates a substantial vertically integrated healthcare delivery system. You can get insured by CVS/Aetna, seek out basic healthcare at a Minute Clinic inside a CVS store, get your prescription filled at the CVS pharmacy, and find over-the-counter treatments on CVS' shelves. With one company in the mix instead of several, it should be possible to minimize the overhead costs, delivering savings to consumers while still earning a fair return.\\nDespite that incredible potential, CVS' shares can be purchased at a relative bargain of less than eight times the company's anticipated earnings. At that price, combined with its 3.8% yield, even if the company doesn't revolutionize healthcare delivery, investors buying today are getting a decent chance at a reasonable return for their risks.\\n\\n   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Loading scraped dataset ..\n",
    "df2 = pd.read_excel('data.xlsx')\n",
    "df2.columns = ['news_index', 'image_url', 'title', 'text', 'source_name', 'date', 'topics', 'sentiment', 'type', 'description']\n",
    "df2 = df2[['title', 'text', 'description']]\n",
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07fc74da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Text</th>\n",
       "      <th>Target Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question: what is rba? context: The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.</td>\n",
       "      <td>Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>question: cost to get a patent? context: $8,000 - $15,000. Cost to patent an electronics invention. “If you've got a mechanical invention, [a patent will] cost between $5,000 to $10,000 including attorney's fees and filing fees. If you're talking about electronics, the cost goes up to more like $8,000 to $15,000. $100,000 or more for worldwide patent rights. “Getting even a simple patent approved -- a process known as ‘prosecuting’ the patent application -- can cost up to $10,000 domestically and $100,000 or more for worldwide rights…” Cost of getting a patent.</td>\n",
       "      <td>The most expensive patents are international patents, which can run up to $100,000 or higher.Domestically the costs can be $10,000 or above.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question: in animals somatic cells are produced by and gametic cells are produced by? context: Unlike the somatic cells, gametes are haploid cells, which carry unpaired chromosomes. A gamete of a particular multicellular organism always carries only half the number of chromosomes carried by a somatic cell of that particular organism. • Somatic cells contain homologous pairs of chromosomes, whereas gametes contain only unpaired chromosomes. • Somatic cells form internal and external structures of the body, whereas gametes do not. • Somatic cells are found almost everywhere in the body, whereas gametes are restricted to certain parts.</td>\n",
       "      <td>Somatic cells are produced by mitosis and gametes produced by most organisms combine to form a zygote with n pairs of chromosomes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>question: what is a camerata? context: Origin of CAMERATA. New Latin, from Latin, neuter plural of cameratus, past participle of camerare to arch, from camera arched roof — more at chamber. This word doesn't usually appear in our free dictionary, but the definition from our premium Unabridged Dictionary is offered here on a limited basis. Note that some information is displayed differently in the Unabridged. To access the complete Unabridged Dictionary, with an additional 300,000 words that aren't in our free dictionary, start a free trial.</td>\n",
       "      <td>The Camerata are a group of four powerful and influential individuals in the city of Cloudbank. Established by Grant Kendrell and Royce Bracket, the group circumvents the official administration and democratic nature of Cloudbank, in order to establish some form of stability in the ever-changing city.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>question: what energy is used in photosynthesis? context: PHOTOSYNTHESIS. INTRODUCTION. Photosynthesis, process by which green plants and certain other organisms use the energy of light to convert carbon dioxide and water into the simple sugar glucose. In so doing, photosynthesis provides the basic energy source for virtually all organisms. Photosynthesis, process by which green plants and certain other organisms use the energy of light to convert carbon dioxide and water into the simple sugar glucose</td>\n",
       "      <td>Photosynthesis is a process used by plants and other organisms to convert light energy, normally from the Sun, into chemical energy that can be later released to fuel the organisms' activities.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Input Text  \\\n",
       "0                                                                                                                      question: what is rba? context: The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.   \n",
       "1                                                                           question: cost to get a patent? context: $8,000 - $15,000. Cost to patent an electronics invention. “If you've got a mechanical invention, [a patent will] cost between $5,000 to $10,000 including attorney's fees and filing fees. If you're talking about electronics, the cost goes up to more like $8,000 to $15,000. $100,000 or more for worldwide patent rights. “Getting even a simple patent approved -- a process known as ‘prosecuting’ the patent application -- can cost up to $10,000 domestically and $100,000 or more for worldwide rights…” Cost of getting a patent.   \n",
       "2  question: in animals somatic cells are produced by and gametic cells are produced by? context: Unlike the somatic cells, gametes are haploid cells, which carry unpaired chromosomes. A gamete of a particular multicellular organism always carries only half the number of chromosomes carried by a somatic cell of that particular organism. • Somatic cells contain homologous pairs of chromosomes, whereas gametes contain only unpaired chromosomes. • Somatic cells form internal and external structures of the body, whereas gametes do not. • Somatic cells are found almost everywhere in the body, whereas gametes are restricted to certain parts.   \n",
       "3                                                                                                question: what is a camerata? context: Origin of CAMERATA. New Latin, from Latin, neuter plural of cameratus, past participle of camerare to arch, from camera arched roof — more at chamber. This word doesn't usually appear in our free dictionary, but the definition from our premium Unabridged Dictionary is offered here on a limited basis. Note that some information is displayed differently in the Unabridged. To access the complete Unabridged Dictionary, with an additional 300,000 words that aren't in our free dictionary, start a free trial.   \n",
       "4                                                                                                                                        question: what energy is used in photosynthesis? context: PHOTOSYNTHESIS. INTRODUCTION. Photosynthesis, process by which green plants and certain other organisms use the energy of light to convert carbon dioxide and water into the simple sugar glucose. In so doing, photosynthesis provides the basic energy source for virtually all organisms. Photosynthesis, process by which green plants and certain other organisms use the energy of light to convert carbon dioxide and water into the simple sugar glucose   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      Target Text  \n",
       "0                                                                                                                Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.  \n",
       "1                                                                                                                                                                    The most expensive patents are international patents, which can run up to $100,000 or higher.Domestically the costs can be $10,000 or above.  \n",
       "2                                                                                                                                                                              Somatic cells are produced by mitosis and gametes produced by most organisms combine to form a zygote with n pairs of chromosomes.  \n",
       "3  The Camerata are a group of four powerful and influential individuals in the city of Cloudbank. Established by Grant Kendrell and Royce Bracket, the group circumvents the official administration and democratic nature of Cloudbank, in order to establish some form of stability in the ever-changing city.  \n",
       "4                                                                                                               Photosynthesis is a process used by plants and other organisms to convert light energy, normally from the Sun, into chemical energy that can be later released to fuel the organisms' activities.  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Generating final dataset ..\n",
    "def format_data(dataset):\n",
    "    inputs = []\n",
    "    references = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        text = 'question: ' + dataset.iloc[i]['Question'] + 'context: ' + dataset.iloc[i]['Context']\n",
    "        reference = dataset.iloc[i]['Reference']\n",
    "        inputs.append(text)\n",
    "        references.append(reference)\n",
    "        \n",
    "    df = pd.DataFrame(inputs)\n",
    "    df['references'] = references\n",
    "    df.columns = ['Input Text', 'Target Text']\n",
    "    return df\n",
    "\n",
    "# Driver code\n",
    "data = format_data(df1)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a552625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5639\n",
      "4798\n",
      "5275\n",
      "7884\n",
      "3047\n",
      "4717\n",
      "5486\n",
      "4793\n",
      "5278\n",
      "3321\n",
      "4104\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "4098\n",
      "9744\n",
      "7932\n",
      "10916\n",
      "7509\n",
      "10078\n",
      "5155\n",
      "4137\n",
      "4183\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "5120\n",
      "2136\n",
      "4824\n",
      "3414\n",
      "7260\n",
      "2927\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2684\n",
      "3682\n",
      "3\n",
      "5324\n",
      "2780\n",
      "3788\n",
      "2817\n",
      "3127\n",
      "4938\n",
      "3\n",
      "6738\n",
      "3\n",
      "1751\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "24188\n",
      "20706\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "32767\n",
      "3\n",
      "3\n",
      "10469\n",
      "4959\n",
      "3\n",
      "4020\n",
      "1812\n",
      "1989\n",
      "7270\n",
      "32767\n",
      "3436\n",
      "1968\n",
      "1596\n",
      "4875\n",
      "3954\n",
      "3677\n",
      "2654\n",
      "3834\n",
      "1093\n",
      "1347\n",
      "2948\n",
      "2022\n",
      "3\n",
      "4666\n",
      "2142\n",
      "32767\n",
      "1544\n",
      "2623\n",
      "2153\n",
      "2194\n",
      "3862\n",
      "2123\n",
      "1656\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "31915\n",
      "32767\n",
      "5037\n",
      "2402\n",
      "1301\n",
      "3225\n",
      "6987\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "3\n",
      "3\n",
      "32767\n",
      "20036\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9168\n",
      "3\n",
      "10090\n",
      "22524\n",
      "7855\n",
      "7419\n",
      "7879\n",
      "3\n",
      "7877\n",
      "3\n",
      "23088\n",
      "21030\n",
      "3\n",
      "10189\n",
      "8165\n",
      "16363\n",
      "21470\n",
      "12576\n",
      "8371\n",
      "19091\n",
      "5899\n",
      "12089\n",
      "8174\n",
      "9758\n",
      "8451\n",
      "3\n",
      "11095\n",
      "3\n",
      "2675\n",
      "3304\n",
      "3\n",
      "3\n",
      "4477\n",
      "3\n",
      "5468\n",
      "3\n",
      "2460\n",
      "1928\n",
      "1711\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "23041\n",
      "3\n",
      "3\n",
      "3\n",
      "4351\n",
      "3841\n",
      "1810\n",
      "2430\n",
      "3344\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2173\n",
      "2142\n",
      "3\n",
      "5629\n",
      "3\n",
      "22932\n",
      "3\n",
      "3\n",
      "2154\n",
      "3\n",
      "3\n",
      "3\n",
      "5570\n",
      "4303\n",
      "5423\n",
      "5244\n",
      "3787\n",
      "3599\n",
      "3968\n",
      "4519\n",
      "6248\n",
      "20824\n",
      "7125\n",
      "3332\n",
      "5114\n",
      "10224\n",
      "3225\n",
      "12693\n",
      "5775\n",
      "3008\n",
      "3\n",
      "3\n",
      "3\n",
      "17432\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "18377\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2537\n",
      "5683\n",
      "1689\n",
      "3799\n",
      "8241\n",
      "3535\n",
      "3033\n",
      "5605\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4685\n",
      "1541\n",
      "3972\n",
      "4627\n",
      "5963\n",
      "3121\n",
      "3116\n",
      "3\n",
      "32767\n",
      "3\n",
      "6014\n",
      "3331\n",
      "4528\n",
      "6334\n",
      "4547\n",
      "3152\n",
      "4868\n",
      "30993\n",
      "32767\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "2540\n",
      "5639\n",
      "4798\n",
      "5274\n",
      "7884\n",
      "3047\n",
      "4717\n",
      "5486\n",
      "4793\n",
      "5278\n",
      "3321\n",
      "4104\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "4098\n",
      "9744\n",
      "7932\n",
      "10916\n",
      "7509\n",
      "10078\n",
      "5155\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "3\n",
      "32767\n",
      "32767\n",
      "4772\n",
      "3\n",
      "4480\n",
      "5713\n",
      "3969\n",
      "5438\n",
      "30390\n",
      "8165\n",
      "31772\n",
      "6265\n",
      "13029\n",
      "4621\n",
      "9303\n",
      "25990\n",
      "3\n",
      "13620\n",
      "30211\n",
      "6690\n",
      "13331\n",
      "32767\n",
      "30307\n",
      "9107\n",
      "13148\n",
      "6233\n",
      "8487\n",
      "7146\n",
      "3\n",
      "6243\n",
      "3\n",
      "3\n",
      "15491\n",
      "3\n",
      "4520\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "26628\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5060\n",
      "1266\n",
      "4610\n",
      "29432\n",
      "32767\n",
      "4239\n",
      "3055\n",
      "6065\n",
      "3\n",
      "4223\n",
      "3\n",
      "3778\n",
      "4409\n",
      "13127\n",
      "7756\n",
      "3430\n",
      "4887\n",
      "5098\n",
      "5113\n",
      "3\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "32614\n",
      "3074\n",
      "3150\n",
      "3795\n",
      "2105\n",
      "27031\n",
      "4594\n",
      "5089\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "27098\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "21011\n",
      "4998\n",
      "3070\n",
      "4179\n",
      "4632\n",
      "3\n",
      "2548\n",
      "5200\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1995\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2722\n",
      "2078\n",
      "3696\n",
      "1324\n",
      "1700\n",
      "5539\n",
      "2461\n",
      "3194\n",
      "4116\n",
      "32767\n",
      "2521\n",
      "6990\n",
      "1919\n",
      "2630\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "20970\n",
      "20280\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5292\n",
      "3\n",
      "3124\n",
      "32767\n",
      "32767\n",
      "11038\n",
      "12271\n",
      "8585\n",
      "23535\n",
      "5796\n",
      "25813\n",
      "22081\n",
      "3\n",
      "9117\n",
      "3\n",
      "6569\n",
      "3\n",
      "3\n",
      "12290\n",
      "7064\n",
      "12521\n",
      "11122\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3819\n",
      "2168\n",
      "5004\n",
      "3994\n",
      "32767\n",
      "2612\n",
      "6919\n",
      "10622\n",
      "18441\n",
      "6734\n",
      "8090\n",
      "19202\n",
      "1874\n",
      "9222\n",
      "8315\n",
      "8634\n",
      "32767\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "21985\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "27138\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5919\n",
      "1935\n",
      "5627\n",
      "1504\n",
      "1694\n",
      "1842\n",
      "1311\n",
      "2229\n",
      "3431\n",
      "2376\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "3\n",
      "32244\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "21318\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "14931\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "14052\n",
      "10324\n",
      "13381\n",
      "17379\n",
      "32767\n",
      "3\n",
      "32767\n",
      "11215\n",
      "12946\n",
      "10914\n",
      "6831\n",
      "7813\n",
      "7197\n",
      "29149\n",
      "10050\n",
      "3\n",
      "3188\n",
      "2570\n",
      "2703\n",
      "2271\n",
      "10343\n",
      "2978\n",
      "1739\n",
      "2514\n",
      "1544\n",
      "13076\n",
      "4558\n",
      "2190\n",
      "3\n",
      "3\n",
      "3\n",
      "5064\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4618\n",
      "4995\n",
      "5308\n",
      "3\n",
      "4102\n",
      "6439\n",
      "3859\n",
      "6942\n",
      "3\n",
      "7302\n",
      "3458\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2540\n",
      "2409\n",
      "2547\n",
      "3\n",
      "12410\n",
      "29594\n",
      "32767\n",
      "3002\n",
      "29310\n",
      "32767\n",
      "32767\n",
      "30909\n",
      "4932\n",
      "5500\n",
      "5468\n",
      "3002\n",
      "3002\n",
      "3585\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6642\n",
      "32767\n",
      "3\n",
      "17425\n",
      "7026\n",
      "20180\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2874\n",
      "2712\n",
      "10381\n",
      "2683\n",
      "5313\n",
      "4405\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "21809\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "31583\n",
      "10630\n",
      "3750\n",
      "3681\n",
      "2253\n",
      "1161\n",
      "3\n",
      "3\n",
      "3\n",
      "3580\n",
      "5575\n",
      "2504\n",
      "1762\n",
      "2727\n",
      "3248\n",
      "5104\n",
      "3990\n",
      "3\n",
      "3196\n",
      "4596\n",
      "2241\n",
      "3239\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2033\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "10681\n",
      "5647\n",
      "2775\n",
      "17160\n",
      "3\n",
      "11348\n",
      "3\n",
      "3\n",
      "3\n",
      "7297\n",
      "3\n",
      "16563\n",
      "2375\n",
      "1706\n",
      "3\n",
      "9975\n",
      "10550\n",
      "3\n",
      "14788\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2234\n",
      "4603\n",
      "3\n",
      "1928\n",
      "3218\n",
      "3212\n",
      "3318\n",
      "5262\n",
      "2742\n",
      "3\n",
      "3\n",
      "2617\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "4401\n",
      "3\n",
      "19438\n",
      "3\n",
      "11609\n",
      "17847\n",
      "32767\n",
      "11303\n",
      "5815\n",
      "8479\n",
      "5578\n",
      "1590\n",
      "3\n",
      "3\n",
      "30173\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4626\n",
      "2751\n",
      "32767\n",
      "5417\n",
      "1784\n",
      "3035\n",
      "1259\n",
      "4050\n",
      "2421\n",
      "4533\n",
      "2161\n",
      "1987\n",
      "1058\n",
      "2114\n",
      "2330\n",
      "1687\n",
      "1740\n",
      "2276\n",
      "15486\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1732\n",
      "1654\n",
      "3713\n",
      "4262\n",
      "1767\n",
      "4903\n",
      "4687\n",
      "4197\n",
      "5822\n",
      "3896\n",
      "3011\n",
      "2172\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3845\n",
      "12555\n",
      "15923\n",
      "16369\n",
      "8252\n",
      "25096\n",
      "10344\n",
      "7683\n",
      "7996\n",
      "8370\n",
      "23256\n",
      "6565\n",
      "10847\n",
      "6872\n",
      "3\n",
      "11249\n",
      "17724\n",
      "12822\n",
      "8826\n",
      "7877\n",
      "10576\n",
      "14074\n",
      "19943\n",
      "14240\n",
      "3\n",
      "8816\n",
      "7386\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1998\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4786\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "17587\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4774\n",
      "4094\n",
      "5105\n",
      "4652\n",
      "5678\n",
      "5812\n",
      "5960\n",
      "7638\n",
      "3742\n",
      "4397\n",
      "3871\n",
      "3140\n",
      "3369\n",
      "3\n",
      "6348\n",
      "3\n",
      "3\n",
      "4661\n",
      "3\n",
      "4745\n",
      "5679\n",
      "4624\n",
      "5959\n",
      "4071\n",
      "4722\n",
      "8500\n",
      "4366\n",
      "4967\n",
      "3587\n",
      "3272\n",
      "3\n",
      "2868\n",
      "1958\n",
      "3\n",
      "3\n",
      "2310\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "3\n",
      "32767\n",
      "32767\n",
      "4772\n",
      "3\n",
      "4480\n",
      "5713\n",
      "3969\n",
      "5438\n",
      "30390\n",
      "8165\n",
      "31772\n",
      "6265\n",
      "13029\n",
      "4621\n",
      "9303\n",
      "25990\n",
      "3\n",
      "13620\n",
      "30211\n",
      "6690\n",
      "13331\n",
      "32767\n",
      "30307\n",
      "9107\n",
      "13148\n",
      "6233\n",
      "8487\n",
      "7146\n",
      "3\n",
      "6243\n",
      "3\n",
      "3\n",
      "15491\n",
      "3\n",
      "4520\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "26628\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5060\n",
      "1266\n",
      "4610\n",
      "29432\n",
      "32767\n",
      "4239\n",
      "3055\n",
      "6065\n",
      "3\n",
      "4223\n",
      "3\n",
      "3778\n",
      "4409\n",
      "13127\n",
      "7756\n",
      "3430\n",
      "4887\n",
      "5098\n",
      "5113\n",
      "3\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "32614\n",
      "3074\n",
      "3150\n",
      "3788\n",
      "2105\n",
      "27031\n",
      "4594\n",
      "5089\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "27098\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "21011\n",
      "4998\n",
      "3070\n",
      "4179\n",
      "4632\n",
      "3\n",
      "2548\n",
      "5200\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1995\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2722\n",
      "2078\n",
      "3689\n",
      "1458\n",
      "1700\n",
      "5539\n",
      "2461\n",
      "3194\n",
      "3973\n",
      "32767\n",
      "2521\n",
      "6990\n",
      "1919\n",
      "2630\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "20970\n",
      "20280\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5292\n",
      "3\n",
      "3124\n",
      "32767\n",
      "32767\n",
      "11038\n",
      "12271\n",
      "8585\n",
      "23535\n",
      "5796\n",
      "25813\n",
      "22081\n",
      "3\n",
      "9117\n",
      "3\n",
      "6569\n",
      "3\n",
      "3\n",
      "12290\n",
      "7064\n",
      "12521\n",
      "11122\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3819\n",
      "2168\n",
      "5004\n",
      "3994\n",
      "32767\n",
      "2612\n",
      "6919\n",
      "10622\n",
      "18441\n",
      "6734\n",
      "8090\n",
      "19202\n",
      "1874\n",
      "9222\n",
      "8315\n",
      "8634\n",
      "32767\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "21985\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "27138\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5912\n",
      "1935\n",
      "5627\n",
      "1504\n",
      "1687\n",
      "1842\n",
      "1311\n",
      "2229\n",
      "3431\n",
      "2376\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "3\n",
      "32244\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "21318\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "14931\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "14052\n",
      "10324\n",
      "13381\n",
      "17379\n",
      "32767\n",
      "3\n",
      "32767\n",
      "11215\n",
      "12946\n",
      "10914\n",
      "6831\n",
      "7813\n",
      "7197\n",
      "29149\n",
      "10050\n",
      "3\n",
      "3188\n",
      "2570\n",
      "2576\n",
      "2271\n",
      "10481\n",
      "2978\n",
      "1470\n",
      "2514\n",
      "1544\n",
      "13076\n",
      "4558\n",
      "2190\n",
      "3\n",
      "3\n",
      "3\n",
      "5064\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4618\n",
      "4995\n",
      "5308\n",
      "3\n",
      "4102\n",
      "6439\n",
      "3852\n",
      "6942\n",
      "3\n",
      "7302\n",
      "3458\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "10372\n",
      "10057\n",
      "3\n",
      "32767\n",
      "32767\n",
      "3\n",
      "3\n",
      "32767\n",
      "1721\n",
      "3\n",
      "3\n",
      "3212\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "1460\n",
      "16586\n",
      "23643\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "32767\n",
      "3\n",
      "6295\n",
      "5882\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2889\n",
      "3\n",
      "3\n",
      "1476\n",
      "1811\n",
      "3\n",
      "3\n",
      "3\n",
      "3881\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "8919\n",
      "3\n",
      "2392\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "21461\n",
      "3547\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "1774\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3362\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "6724\n",
      "4643\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "7080\n",
      "3\n",
      "2346\n",
      "2851\n",
      "2332\n",
      "3267\n",
      "3\n",
      "19051\n",
      "32767\n",
      "3\n",
      "3\n",
      "2286\n",
      "3\n",
      "2970\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "16381\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1896\n",
      "3\n",
      "12152\n",
      "2797\n",
      "3\n",
      "2785\n",
      "3205\n",
      "2876\n",
      "3205\n",
      "3205\n",
      "3\n",
      "3205\n",
      "3\n",
      "3\n",
      "3\n",
      "3064\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3819\n",
      "3\n",
      "3448\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4368\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4942\n",
      "7709\n",
      "3\n",
      "5723\n",
      "3\n",
      "3\n",
      "1149\n",
      "1325\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3303\n",
      "3150\n",
      "3\n",
      "6080\n",
      "6080\n",
      "6080\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5375\n",
      "7042\n",
      "2213\n",
      "5211\n",
      "3\n",
      "3420\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3111\n",
      "7419\n",
      "797\n",
      "5456\n",
      "3029\n",
      "32767\n",
      "2417\n",
      "8019\n",
      "2430\n",
      "2517\n",
      "2895\n",
      "2743\n",
      "2562\n",
      "1980\n",
      "5655\n",
      "5655\n",
      "12009\n",
      "2409\n",
      "2409\n",
      "3\n",
      "12410\n",
      "29594\n",
      "32767\n",
      "3002\n",
      "29310\n",
      "32767\n",
      "32767\n",
      "30909\n",
      "4932\n",
      "5500\n",
      "5468\n",
      "3002\n",
      "3002\n",
      "3585\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6503\n",
      "32767\n",
      "3\n",
      "17425\n",
      "7026\n",
      "20180\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2802\n",
      "2712\n",
      "10381\n",
      "2683\n",
      "5313\n",
      "4405\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "21809\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "31583\n",
      "10630\n",
      "3750\n",
      "3681\n",
      "2253\n",
      "1161\n",
      "3\n",
      "3\n",
      "3\n",
      "3580\n",
      "5575\n",
      "2504\n",
      "1762\n",
      "2727\n",
      "3248\n",
      "5104\n",
      "3988\n",
      "3\n",
      "3196\n",
      "4596\n",
      "2241\n",
      "3239\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2033\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "10681\n",
      "5647\n",
      "2775\n",
      "17160\n",
      "3\n",
      "11348\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "16563\n",
      "2374\n",
      "1706\n",
      "3\n",
      "9975\n",
      "10550\n",
      "3\n",
      "14788\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2234\n",
      "4603\n",
      "3\n",
      "1928\n",
      "3218\n",
      "3212\n",
      "3318\n",
      "5262\n",
      "2742\n",
      "3\n",
      "3\n",
      "2617\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "4401\n",
      "3\n",
      "19438\n",
      "3\n",
      "11609\n",
      "17847\n",
      "32767\n",
      "11303\n",
      "5815\n",
      "8479\n",
      "5578\n",
      "1451\n",
      "3\n",
      "3\n",
      "30173\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4626\n",
      "2882\n",
      "32767\n",
      "5417\n",
      "1784\n",
      "3035\n",
      "1259\n",
      "4050\n",
      "2421\n",
      "4533\n",
      "2161\n",
      "1987\n",
      "1058\n",
      "2114\n",
      "2330\n",
      "1687\n",
      "1740\n",
      "2276\n",
      "15486\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1731\n",
      "1654\n",
      "3713\n",
      "4262\n",
      "1767\n",
      "4903\n",
      "4687\n",
      "4197\n",
      "5822\n",
      "3896\n",
      "3011\n",
      "2172\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3845\n",
      "12555\n",
      "15923\n",
      "16369\n",
      "8252\n",
      "25096\n",
      "10344\n",
      "7683\n",
      "7996\n",
      "8370\n",
      "23256\n",
      "6565\n",
      "10847\n",
      "6872\n",
      "3\n",
      "11249\n",
      "17724\n",
      "12822\n",
      "8826\n",
      "7877\n",
      "10576\n",
      "14074\n",
      "19943\n",
      "14240\n",
      "3\n",
      "8816\n",
      "7386\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1998\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4786\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "17587\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4774\n",
      "4093\n",
      "5105\n",
      "4652\n",
      "5678\n",
      "5812\n",
      "5960\n",
      "7638\n",
      "3742\n",
      "4397\n",
      "3871\n",
      "3139\n",
      "3369\n",
      "3\n",
      "6348\n",
      "3\n",
      "3\n",
      "4661\n",
      "3\n",
      "4745\n",
      "5679\n",
      "4624\n",
      "5959\n",
      "4071\n",
      "4722\n",
      "8500\n",
      "4366\n",
      "4967\n",
      "3587\n",
      "3272\n",
      "3\n",
      "3094\n",
      "1958\n",
      "3\n",
      "3\n",
      "2310\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "2491\n",
      "25168\n",
      "32767\n",
      "7898\n",
      "32767\n",
      "32767\n",
      "27279\n",
      "32767\n",
      "23536\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3875\n",
      "3\n",
      "3\n",
      "4247\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9984\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3060\n",
      "8332\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6051\n",
      "2064\n",
      "3\n",
      "3\n",
      "3\n",
      "4758\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3260\n",
      "3260\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "12939\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4449\n",
      "3\n",
      "3\n",
      "24122\n",
      "3\n",
      "3\n",
      "3\n",
      "2205\n",
      "3\n",
      "2306\n",
      "3\n",
      "2800\n",
      "3080\n",
      "10022\n",
      "32767\n",
      "2701\n",
      "3\n",
      "3\n",
      "3\n",
      "4641\n",
      "5403\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5320\n",
      "4498\n",
      "3\n",
      "3\n",
      "1612\n",
      "2116\n",
      "3\n",
      "3\n",
      "32767\n",
      "32767\n",
      "3010\n",
      "3010\n",
      "3010\n",
      "2707\n",
      "2707\n",
      "2707\n",
      "3\n",
      "1395\n",
      "5525\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4589\n",
      "32767\n",
      "8135\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1504\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3769\n",
      "3441\n",
      "3\n",
      "32767\n",
      "32767\n",
      "6593\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "8812\n",
      "3\n",
      "3\n",
      "24679\n",
      "4134\n",
      "3\n",
      "11496\n",
      "3\n",
      "5511\n",
      "12121\n",
      "3\n",
      "3\n",
      "1917\n",
      "3\n",
      "3\n",
      "3\n",
      "1914\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "2513\n",
      "3\n",
      "3\n",
      "3\n",
      "11082\n",
      "3\n",
      "3\n",
      "3\n",
      "4107\n",
      "3\n",
      "32767\n",
      "3\n",
      "3459\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "8755\n",
      "3\n",
      "2782\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "11101\n",
      "3\n",
      "2930\n",
      "3\n",
      "3\n",
      "3\n",
      "2824\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1196\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "16406\n",
      "3\n",
      "3\n",
      "3\n",
      "17421\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4201\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6116\n",
      "3\n",
      "3155\n",
      "3\n",
      "3\n",
      "3\n",
      "4521\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "16374\n",
      "3\n",
      "6886\n",
      "3\n",
      "3\n",
      "3\n",
      "5636\n",
      "3\n",
      "3\n",
      "3\n",
      "1083\n",
      "3978\n",
      "2013\n",
      "2556\n",
      "3\n",
      "3\n",
      "5148\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5102\n",
      "3\n",
      "3\n",
      "7857\n",
      "3\n",
      "3\n",
      "3\n",
      "2008\n",
      "24873\n",
      "3\n",
      "3\n",
      "4184\n",
      "3\n",
      "5879\n",
      "27674\n",
      "3873\n",
      "3\n",
      "32767\n",
      "5980\n",
      "3\n",
      "3\n",
      "3\n",
      "1982\n",
      "3\n",
      "31909\n",
      "3\n",
      "10675\n",
      "17785\n",
      "2149\n",
      "3143\n",
      "3\n",
      "3\n",
      "1767\n",
      "2409\n",
      "15231\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2090\n",
      "3\n",
      "7142\n",
      "5029\n",
      "3\n",
      "22069\n",
      "3\n",
      "3\n",
      "5431\n",
      "3\n",
      "4474\n",
      "16055\n",
      "11475\n",
      "3\n",
      "7959\n",
      "3798\n",
      "32767\n",
      "2495\n",
      "4160\n",
      "3266\n",
      "5809\n",
      "4554\n",
      "3470\n",
      "4606\n",
      "3008\n",
      "4139\n",
      "4292\n",
      "4571\n",
      "6236\n",
      "3\n",
      "5071\n",
      "5328\n",
      "4781\n",
      "10372\n",
      "10057\n",
      "3\n",
      "32767\n",
      "32767\n",
      "3\n",
      "3\n",
      "32767\n",
      "1721\n",
      "3\n",
      "3\n",
      "3212\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "1460\n",
      "16586\n",
      "23643\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "32767\n",
      "3\n",
      "6295\n",
      "5882\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2889\n",
      "3\n",
      "3\n",
      "1345\n",
      "1811\n",
      "3\n",
      "3\n",
      "3\n",
      "3881\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "8919\n",
      "3\n",
      "2502\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "21461\n",
      "3547\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "1774\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3362\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "6724\n",
      "4643\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "7080\n",
      "3\n",
      "2346\n",
      "2724\n",
      "2332\n",
      "3267\n",
      "3\n",
      "19051\n",
      "32767\n",
      "3\n",
      "3\n",
      "2286\n",
      "3\n",
      "2970\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "16381\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1896\n",
      "3\n",
      "12152\n",
      "2797\n",
      "3\n",
      "2785\n",
      "3205\n",
      "2875\n",
      "3205\n",
      "3205\n",
      "3\n",
      "3205\n",
      "3\n",
      "3\n",
      "3\n",
      "3064\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3819\n",
      "3\n",
      "3448\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4368\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4942\n",
      "7709\n",
      "3\n",
      "5723\n",
      "3\n",
      "3\n",
      "1149\n",
      "1325\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3443\n",
      "3150\n",
      "3\n",
      "6081\n",
      "6081\n",
      "6081\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5375\n",
      "7042\n",
      "2213\n",
      "5454\n",
      "3\n",
      "3420\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3111\n",
      "7419\n",
      "1007\n",
      "5456\n",
      "3029\n",
      "32767\n",
      "2417\n",
      "8019\n",
      "2430\n",
      "2517\n",
      "2895\n",
      "2743\n",
      "2562\n",
      "2075\n",
      "5655\n",
      "5655\n",
      "12009\n",
      "4537\n",
      "3950\n",
      "4192\n",
      "5425\n",
      "7887\n",
      "3\n",
      "3\n",
      "4196\n",
      "4196\n",
      "4685\n",
      "3785\n",
      "6418\n",
      "2217\n",
      "5529\n",
      "3\n",
      "16195\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1756\n",
      "3\n",
      "6967\n",
      "3112\n",
      "3112\n",
      "27117\n",
      "3621\n",
      "3\n",
      "3\n",
      "3\n",
      "1846\n",
      "3\n",
      "3\n",
      "3\n",
      "2961\n",
      "2961\n",
      "2961\n",
      "31762\n",
      "3\n",
      "4931\n",
      "3\n",
      "9222\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5025\n",
      "5025\n",
      "5025\n",
      "3\n",
      "3\n",
      "3\n",
      "8491\n",
      "11452\n",
      "2065\n",
      "1492\n",
      "3\n",
      "9172\n",
      "3\n",
      "1858\n",
      "1738\n",
      "3\n",
      "15072\n",
      "2598\n",
      "4424\n",
      "27503\n",
      "3\n",
      "3\n",
      "3386\n",
      "3\n",
      "5470\n",
      "5470\n",
      "5470\n",
      "32767\n",
      "2279\n",
      "3\n",
      "3704\n",
      "3\n",
      "3\n",
      "17582\n",
      "31273\n",
      "3\n",
      "3\n",
      "3\n",
      "2036\n",
      "3\n",
      "3\n",
      "6563\n",
      "3\n",
      "4801\n",
      "26364\n",
      "3\n",
      "2628\n",
      "3\n",
      "3\n",
      "3\n",
      "1585\n",
      "7510\n",
      "16984\n",
      "3\n",
      "3\n",
      "3184\n",
      "3\n",
      "3\n",
      "1970\n",
      "3\n",
      "3\n",
      "7886\n",
      "1692\n",
      "3\n",
      "3\n",
      "1412\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4625\n",
      "3\n",
      "3\n",
      "3971\n",
      "3\n",
      "7557\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9235\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3741\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3624\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "20741\n",
      "3\n",
      "3\n",
      "1833\n",
      "3\n",
      "2926\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3487\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6115\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6281\n",
      "6281\n",
      "3\n",
      "3\n",
      "4520\n",
      "3\n",
      "3\n",
      "13852\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "7058\n",
      "3\n",
      "3\n",
      "3987\n",
      "3\n",
      "4828\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "12269\n",
      "3\n",
      "4322\n",
      "7203\n",
      "7203\n",
      "7203\n",
      "1797\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2121\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "11956\n",
      "3\n",
      "3\n",
      "6328\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2435\n",
      "3337\n",
      "3\n",
      "3\n",
      "3168\n",
      "3\n",
      "3\n",
      "3\n",
      "6439\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "8545\n",
      "3\n",
      "3\n",
      "6393\n",
      "5116\n",
      "5189\n",
      "11208\n",
      "10337\n",
      "3\n",
      "7581\n",
      "3\n",
      "4407\n",
      "3793\n",
      "3\n",
      "3\n",
      "3\n",
      "7065\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5274\n",
      "5274\n",
      "5274\n",
      "3\n",
      "3\n",
      "17094\n",
      "3246\n",
      "3\n",
      "18874\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4754\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4364\n",
      "17858\n",
      "9103\n",
      "32767\n",
      "32767\n",
      "2491\n",
      "25168\n",
      "32767\n",
      "7898\n",
      "32767\n",
      "32767\n",
      "27279\n",
      "32767\n",
      "23536\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3875\n",
      "3\n",
      "3\n",
      "4247\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9984\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2970\n",
      "8332\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5703\n",
      "2064\n",
      "3\n",
      "3\n",
      "3\n",
      "4758\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3259\n",
      "3259\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "12939\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4449\n",
      "3\n",
      "3\n",
      "24122\n",
      "3\n",
      "3\n",
      "3\n",
      "2205\n",
      "3\n",
      "2306\n",
      "3\n",
      "2914\n",
      "3080\n",
      "10022\n",
      "32767\n",
      "2701\n",
      "3\n",
      "3\n",
      "3\n",
      "4641\n",
      "5403\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5320\n",
      "4498\n",
      "3\n",
      "3\n",
      "1612\n",
      "2116\n",
      "3\n",
      "3\n",
      "32767\n",
      "32767\n",
      "3010\n",
      "3010\n",
      "3010\n",
      "2707\n",
      "2707\n",
      "2707\n",
      "3\n",
      "1395\n",
      "5525\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4589\n",
      "32767\n",
      "8135\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1504\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3769\n",
      "3441\n",
      "3\n",
      "32767\n",
      "32767\n",
      "6593\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "8812\n",
      "3\n",
      "3\n",
      "24679\n",
      "4134\n",
      "3\n",
      "11496\n",
      "3\n",
      "5511\n",
      "12121\n",
      "3\n",
      "3\n",
      "1917\n",
      "3\n",
      "3\n",
      "3\n",
      "1914\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "2513\n",
      "3\n",
      "3\n",
      "3\n",
      "11082\n",
      "3\n",
      "3\n",
      "3\n",
      "4107\n",
      "3\n",
      "32767\n",
      "3\n",
      "3459\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "8755\n",
      "3\n",
      "2782\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "11101\n",
      "3\n",
      "2930\n",
      "3\n",
      "3\n",
      "3\n",
      "2824\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1196\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "16406\n",
      "3\n",
      "3\n",
      "3\n",
      "17421\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4201\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6116\n",
      "3\n",
      "3155\n",
      "3\n",
      "3\n",
      "3\n",
      "4521\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "16374\n",
      "3\n",
      "6886\n",
      "3\n",
      "3\n",
      "3\n",
      "5636\n",
      "3\n",
      "3\n",
      "3\n",
      "1083\n",
      "3978\n",
      "2013\n",
      "2556\n",
      "3\n",
      "3\n",
      "5148\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5102\n",
      "3\n",
      "3\n",
      "7857\n",
      "3\n",
      "3\n",
      "3\n",
      "2125\n",
      "24873\n",
      "3\n",
      "3\n",
      "4184\n",
      "3\n",
      "5879\n",
      "27674\n",
      "3767\n",
      "3\n",
      "32767\n",
      "5980\n",
      "3\n",
      "3\n",
      "3\n",
      "1982\n",
      "3\n",
      "31909\n",
      "3\n",
      "10675\n",
      "17785\n",
      "2149\n",
      "3143\n",
      "3\n",
      "3\n",
      "1767\n",
      "2409\n",
      "15231\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2090\n",
      "3\n",
      "7142\n",
      "5029\n",
      "3\n",
      "22069\n",
      "3\n",
      "3\n",
      "5431\n",
      "3\n",
      "4474\n",
      "16055\n",
      "11475\n",
      "3\n",
      "7959\n",
      "3798\n",
      "32767\n",
      "2495\n",
      "4160\n",
      "3266\n",
      "5809\n",
      "4555\n",
      "3470\n",
      "4606\n",
      "3008\n",
      "4139\n",
      "4292\n",
      "4571\n",
      "6236\n",
      "3\n",
      "5071\n",
      "5328\n",
      "4781\n",
      "5084\n",
      "4255\n",
      "3\n",
      "3815\n",
      "3727\n",
      "4267\n",
      "3354\n",
      "4381\n",
      "3674\n",
      "4881\n",
      "4824\n",
      "3466\n",
      "5858\n",
      "4644\n",
      "4644\n",
      "4434\n",
      "5608\n",
      "4277\n",
      "4277\n",
      "4277\n",
      "9468\n",
      "6102\n",
      "6102\n",
      "6102\n",
      "4741\n",
      "5364\n",
      "6250\n",
      "4296\n",
      "5605\n",
      "3\n",
      "3292\n",
      "1787\n",
      "5793\n",
      "3\n",
      "3599\n",
      "12948\n",
      "5497\n",
      "5497\n",
      "3\n",
      "8640\n",
      "7077\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3558\n",
      "10098\n",
      "10098\n",
      "10098\n",
      "3\n",
      "3\n",
      "4325\n",
      "5238\n",
      "5238\n",
      "5238\n",
      "5322\n",
      "5726\n",
      "5726\n",
      "5726\n",
      "11779\n",
      "5493\n",
      "5584\n",
      "5584\n",
      "5584\n",
      "4950\n",
      "3349\n",
      "18128\n",
      "5417\n",
      "6602\n",
      "2269\n",
      "14865\n",
      "10582\n",
      "8313\n",
      "2757\n",
      "4537\n",
      "3950\n",
      "4192\n",
      "5425\n",
      "7887\n",
      "3\n",
      "3\n",
      "4196\n",
      "4196\n",
      "4686\n",
      "3786\n",
      "6418\n",
      "2217\n",
      "5529\n",
      "3\n",
      "16195\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1756\n",
      "3\n",
      "6967\n",
      "3112\n",
      "3112\n",
      "27117\n",
      "3621\n",
      "3\n",
      "3\n",
      "3\n",
      "1846\n",
      "3\n",
      "3\n",
      "3\n",
      "2960\n",
      "2960\n",
      "2960\n",
      "31762\n",
      "3\n",
      "4931\n",
      "3\n",
      "9222\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5025\n",
      "5025\n",
      "5025\n",
      "3\n",
      "3\n",
      "3\n",
      "8491\n",
      "11452\n",
      "2065\n",
      "1492\n",
      "3\n",
      "9172\n",
      "3\n",
      "1858\n",
      "1738\n",
      "3\n",
      "15072\n",
      "2598\n",
      "4424\n",
      "27503\n",
      "3\n",
      "3\n",
      "3386\n",
      "3\n",
      "5470\n",
      "5470\n",
      "5470\n",
      "32767\n",
      "2279\n",
      "3\n",
      "3704\n",
      "3\n",
      "3\n",
      "17582\n",
      "31273\n",
      "3\n",
      "3\n",
      "3\n",
      "1908\n",
      "3\n",
      "3\n",
      "6563\n",
      "3\n",
      "4801\n",
      "26364\n",
      "3\n",
      "2628\n",
      "3\n",
      "3\n",
      "3\n",
      "1585\n",
      "7510\n",
      "16984\n",
      "3\n",
      "3\n",
      "3184\n",
      "3\n",
      "3\n",
      "1830\n",
      "3\n",
      "3\n",
      "7886\n",
      "1692\n",
      "3\n",
      "3\n",
      "1412\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4625\n",
      "3\n",
      "3\n",
      "3971\n",
      "3\n",
      "7916\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9235\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3742\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3624\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "20741\n",
      "3\n",
      "3\n",
      "1833\n",
      "3\n",
      "2926\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3487\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6115\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6281\n",
      "6281\n",
      "3\n",
      "3\n",
      "4521\n",
      "3\n",
      "3\n",
      "13852\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "7058\n",
      "3\n",
      "3\n",
      "3987\n",
      "3\n",
      "4828\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "12269\n",
      "3\n",
      "4322\n",
      "7204\n",
      "7204\n",
      "7204\n",
      "1797\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2121\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "11956\n",
      "3\n",
      "3\n",
      "6328\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2427\n",
      "3337\n",
      "3\n",
      "3\n",
      "3168\n",
      "3\n",
      "3\n",
      "3\n",
      "6439\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "8545\n",
      "3\n",
      "3\n",
      "6393\n",
      "5116\n",
      "5190\n",
      "11208\n",
      "10337\n",
      "3\n",
      "7581\n",
      "3\n",
      "4407\n",
      "3793\n",
      "3\n",
      "3\n",
      "3\n",
      "7065\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5254\n",
      "5254\n",
      "5254\n",
      "3\n",
      "3\n",
      "17094\n",
      "3469\n",
      "3\n",
      "18874\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4754\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4364\n",
      "17858\n",
      "9103\n",
      "32767\n",
      "32767\n",
      "5277\n",
      "5277\n",
      "5277\n",
      "4064\n",
      "4927\n",
      "32767\n",
      "4112\n",
      "4112\n",
      "4112\n",
      "3\n",
      "4849\n",
      "3432\n",
      "3432\n",
      "3432\n",
      "4475\n",
      "3\n",
      "10794\n",
      "5797\n",
      "5736\n",
      "7475\n",
      "4650\n",
      "10219\n",
      "5590\n",
      "4795\n",
      "3440\n",
      "5578\n",
      "6162\n",
      "2314\n",
      "7879\n",
      "3465\n",
      "6014\n",
      "6014\n",
      "6014\n",
      "5005\n",
      "3604\n",
      "3604\n",
      "3604\n",
      "7136\n",
      "5221\n",
      "4047\n",
      "4684\n",
      "4069\n",
      "4069\n",
      "4069\n",
      "8539\n",
      "4118\n",
      "5365\n",
      "3905\n",
      "2292\n",
      "8001\n",
      "3\n",
      "6034\n",
      "3\n",
      "3\n",
      "3\n",
      "4891\n",
      "3218\n",
      "4334\n",
      "4315\n",
      "5256\n",
      "5360\n",
      "5360\n",
      "8457\n",
      "11085\n",
      "1315\n",
      "4649\n",
      "4649\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3363\n",
      "5881\n",
      "5881\n",
      "5881\n",
      "6425\n",
      "6425\n",
      "5998\n",
      "4590\n",
      "4858\n",
      "4858\n",
      "5269\n",
      "5444\n",
      "6212\n",
      "4236\n",
      "5922\n",
      "4444\n",
      "4444\n",
      "5973\n",
      "5084\n",
      "4255\n",
      "3\n",
      "3815\n",
      "3727\n",
      "4267\n",
      "3354\n",
      "4381\n",
      "3674\n",
      "4881\n",
      "4824\n",
      "3466\n",
      "5858\n",
      "4644\n",
      "4644\n",
      "4434\n",
      "5608\n",
      "4277\n",
      "4277\n",
      "4277\n",
      "9468\n",
      "6102\n",
      "6102\n",
      "6102\n",
      "4741\n",
      "5364\n",
      "6250\n",
      "4296\n",
      "5605\n",
      "3\n",
      "3292\n",
      "1787\n",
      "5793\n",
      "3\n",
      "3599\n",
      "12948\n",
      "5497\n",
      "5497\n",
      "3\n",
      "8865\n",
      "7077\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3558\n",
      "10098\n",
      "10098\n",
      "10098\n",
      "3\n",
      "3\n",
      "4325\n",
      "5238\n",
      "5238\n",
      "5238\n",
      "5322\n",
      "5726\n",
      "5726\n",
      "5726\n",
      "11779\n",
      "5493\n",
      "5577\n",
      "5577\n",
      "5577\n",
      "4950\n",
      "3349\n",
      "18128\n",
      "5416\n",
      "6602\n",
      "2269\n",
      "14865\n",
      "10582\n",
      "8313\n",
      "2757\n",
      "3\n",
      "3\n",
      "22829\n",
      "3\n",
      "3\n",
      "22234\n",
      "3\n",
      "8185\n",
      "4727\n",
      "3604\n",
      "4007\n",
      "3\n",
      "11465\n",
      "3\n",
      "5574\n",
      "3\n",
      "5326\n",
      "3\n",
      "5915\n",
      "20004\n",
      "3\n",
      "3\n",
      "18660\n",
      "3561\n",
      "2507\n",
      "3\n",
      "3216\n",
      "2003\n",
      "3\n",
      "11202\n",
      "3\n",
      "3\n",
      "3\n",
      "2814\n",
      "3484\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6255\n",
      "3\n",
      "11070\n",
      "3\n",
      "3\n",
      "3\n",
      "5807\n",
      "5807\n",
      "5807\n",
      "2310\n",
      "3\n",
      "5914\n",
      "5914\n",
      "3174\n",
      "5024\n",
      "5024\n",
      "3191\n",
      "24223\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4736\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2919\n",
      "2919\n",
      "2919\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2939\n",
      "2939\n",
      "2939\n",
      "7057\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1656\n",
      "3\n",
      "1976\n",
      "3\n",
      "3417\n",
      "3\n",
      "3\n",
      "3960\n",
      "8261\n",
      "3\n",
      "1338\n",
      "4520\n",
      "3\n",
      "6144\n",
      "6262\n",
      "5794\n",
      "32767\n",
      "3\n",
      "3\n",
      "6847\n",
      "3740\n",
      "5157\n",
      "5778\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6386\n",
      "15313\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4790\n",
      "7755\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "7634\n",
      "11652\n",
      "29388\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5002\n",
      "3\n",
      "3\n",
      "2695\n",
      "3\n",
      "15426\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9303\n",
      "3\n",
      "14238\n",
      "3\n",
      "3\n",
      "3995\n",
      "15018\n",
      "3\n",
      "5306\n",
      "3\n",
      "3\n",
      "7137\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "19885\n",
      "3\n",
      "3\n",
      "3\n",
      "4439\n",
      "3\n",
      "1941\n",
      "3022\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3144\n",
      "3144\n",
      "3\n",
      "3\n",
      "3538\n",
      "32767\n",
      "1504\n",
      "3728\n",
      "7227\n",
      "3\n",
      "3\n",
      "3\n",
      "20196\n",
      "3\n",
      "3\n",
      "3\n",
      "12116\n",
      "3\n",
      "947\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "1343\n",
      "6042\n",
      "3\n",
      "3\n",
      "3\n",
      "20024\n",
      "3\n",
      "3\n",
      "1761\n",
      "3\n",
      "3\n",
      "3884\n",
      "3884\n",
      "3682\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1958\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3256\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1874\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6771\n",
      "6944\n",
      "3\n",
      "3\n",
      "3\n",
      "9840\n",
      "3\n",
      "5179\n",
      "3\n",
      "3768\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5436\n",
      "5499\n",
      "5499\n",
      "5499\n",
      "3\n",
      "3\n",
      "6947\n",
      "3\n",
      "3\n",
      "4038\n",
      "17820\n",
      "17820\n",
      "17820\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6168\n",
      "3\n",
      "12000\n",
      "3\n",
      "5749\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3832\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5409\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3029\n",
      "3029\n",
      "3029\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6294\n",
      "6294\n",
      "9981\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5049\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3602\n",
      "4782\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "13625\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "971\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3865\n",
      "3\n",
      "9045\n",
      "3\n",
      "3\n",
      "8167\n",
      "3\n",
      "3\n",
      "3\n",
      "5641\n",
      "5633\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4127\n",
      "4127\n",
      "3\n",
      "3\n",
      "3\n",
      "12197\n",
      "4759\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2350\n",
      "3\n",
      "11677\n",
      "3\n",
      "10402\n",
      "3\n",
      "3\n",
      "3440\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1517\n",
      "3\n",
      "5587\n",
      "18212\n",
      "2945\n",
      "1072\n",
      "2802\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4313\n",
      "3\n",
      "3\n",
      "4684\n",
      "3\n",
      "3\n",
      "8578\n",
      "9243\n",
      "9517\n",
      "17268\n",
      "5277\n",
      "5277\n",
      "5277\n",
      "4064\n",
      "4927\n",
      "32767\n",
      "4112\n",
      "4112\n",
      "4112\n",
      "3\n",
      "4849\n",
      "3432\n",
      "3432\n",
      "3432\n",
      "4475\n",
      "3\n",
      "10794\n",
      "5797\n",
      "5736\n",
      "7475\n",
      "4650\n",
      "10219\n",
      "5590\n",
      "4795\n",
      "3440\n",
      "5578\n",
      "6162\n",
      "2314\n",
      "8006\n",
      "3465\n",
      "6014\n",
      "6014\n",
      "6014\n",
      "5005\n",
      "3604\n",
      "3604\n",
      "3604\n",
      "7136\n",
      "5221\n",
      "4047\n",
      "4684\n",
      "4069\n",
      "4069\n",
      "4069\n",
      "8539\n",
      "4118\n",
      "5365\n",
      "3905\n",
      "2292\n",
      "8001\n",
      "3\n",
      "6034\n",
      "3\n",
      "3\n",
      "3\n",
      "4892\n",
      "3218\n",
      "4334\n",
      "4315\n",
      "5256\n",
      "5360\n",
      "5360\n",
      "8457\n",
      "11085\n",
      "1315\n",
      "4648\n",
      "4648\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3363\n",
      "5880\n",
      "5880\n",
      "5880\n",
      "6425\n",
      "6425\n",
      "5998\n",
      "4590\n",
      "4858\n",
      "4858\n",
      "5270\n",
      "5444\n",
      "6212\n",
      "4236\n",
      "5922\n",
      "4444\n",
      "4444\n",
      "5973\n",
      "32767\n",
      "32767\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4856\n",
      "4314\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4100\n",
      "24055\n",
      "3\n",
      "3\n",
      "3\n",
      "15120\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5804\n",
      "6126\n",
      "3853\n",
      "3002\n",
      "4940\n",
      "3\n",
      "4942\n",
      "15577\n",
      "16574\n",
      "3656\n",
      "3\n",
      "3\n",
      "6715\n",
      "6715\n",
      "6715\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "7935\n",
      "7935\n",
      "32767\n",
      "2972\n",
      "3\n",
      "4492\n",
      "4493\n",
      "3553\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4458\n",
      "3360\n",
      "3\n",
      "3\n",
      "3\n",
      "4225\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "4422\n",
      "32767\n",
      "3\n",
      "32767\n",
      "3\n",
      "2359\n",
      "3\n",
      "3\n",
      "1869\n",
      "32555\n",
      "3\n",
      "1840\n",
      "3680\n",
      "3\n",
      "3149\n",
      "4621\n",
      "3\n",
      "4042\n",
      "11386\n",
      "3252\n",
      "3\n",
      "2314\n",
      "13869\n",
      "3\n",
      "12333\n",
      "32767\n",
      "32767\n",
      "5932\n",
      "2944\n",
      "2011\n",
      "3\n",
      "3\n",
      "5826\n",
      "1622\n",
      "8143\n",
      "3\n",
      "26996\n",
      "2573\n",
      "7162\n",
      "12844\n",
      "3444\n",
      "5094\n",
      "4157\n",
      "3\n",
      "1889\n",
      "3\n",
      "4418\n",
      "4219\n",
      "5284\n",
      "3\n",
      "4245\n",
      "5447\n",
      "3771\n",
      "3\n",
      "1448\n",
      "3376\n",
      "3\n",
      "3\n",
      "3946\n",
      "3\n",
      "13153\n",
      "3\n",
      "2542\n",
      "32767\n",
      "6042\n",
      "3322\n",
      "3\n",
      "1924\n",
      "2182\n",
      "2127\n",
      "2131\n",
      "3\n",
      "3\n",
      "3\n",
      "1641\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6057\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6128\n",
      "4199\n",
      "2221\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1322\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2067\n",
      "3\n",
      "3\n",
      "3\n",
      "1917\n",
      "6548\n",
      "4749\n",
      "5851\n",
      "3\n",
      "4930\n",
      "3\n",
      "3\n",
      "2643\n",
      "3\n",
      "4554\n",
      "5537\n",
      "4963\n",
      "4640\n",
      "4916\n",
      "4997\n",
      "3440\n",
      "3\n",
      "3361\n",
      "9564\n",
      "2770\n",
      "3\n",
      "2729\n",
      "6987\n",
      "5270\n",
      "7215\n",
      "3\n",
      "5688\n",
      "3\n",
      "16509\n",
      "3\n",
      "32767\n",
      "2312\n",
      "28172\n",
      "11544\n",
      "3\n",
      "3\n",
      "22829\n",
      "3\n",
      "3\n",
      "22234\n",
      "3\n",
      "8185\n",
      "4727\n",
      "3604\n",
      "3873\n",
      "3\n",
      "11465\n",
      "3\n",
      "5574\n",
      "3\n",
      "5326\n",
      "3\n",
      "5915\n",
      "20004\n",
      "3\n",
      "3\n",
      "18660\n",
      "3561\n",
      "2507\n",
      "3\n",
      "3352\n",
      "2109\n",
      "3\n",
      "11202\n",
      "3\n",
      "3\n",
      "3\n",
      "2690\n",
      "3484\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6255\n",
      "3\n",
      "11070\n",
      "3\n",
      "3\n",
      "3\n",
      "5807\n",
      "5807\n",
      "5807\n",
      "2310\n",
      "3\n",
      "5914\n",
      "5914\n",
      "3174\n",
      "5023\n",
      "5023\n",
      "3191\n",
      "24223\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4610\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2919\n",
      "2919\n",
      "2919\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2939\n",
      "2939\n",
      "2939\n",
      "7057\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1656\n",
      "3\n",
      "1976\n",
      "3\n",
      "3417\n",
      "3\n",
      "3\n",
      "3960\n",
      "8261\n",
      "3\n",
      "1338\n",
      "4520\n",
      "3\n",
      "6144\n",
      "6262\n",
      "5794\n",
      "32767\n",
      "3\n",
      "3\n",
      "6847\n",
      "3740\n",
      "5157\n",
      "5778\n",
      "3\n",
      "3\n",
      "32767\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6386\n",
      "15313\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4790\n",
      "7755\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "7634\n",
      "11652\n",
      "29388\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5002\n",
      "3\n",
      "3\n",
      "2695\n",
      "3\n",
      "15426\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9303\n",
      "3\n",
      "14238\n",
      "3\n",
      "3\n",
      "3995\n",
      "15018\n",
      "3\n",
      "5306\n",
      "3\n",
      "3\n",
      "7137\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "19885\n",
      "3\n",
      "3\n",
      "3\n",
      "4297\n",
      "3\n",
      "1941\n",
      "3022\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3144\n",
      "3144\n",
      "3\n",
      "3\n",
      "3426\n",
      "32767\n",
      "1504\n",
      "3728\n",
      "7227\n",
      "3\n",
      "3\n",
      "3\n",
      "20196\n",
      "3\n",
      "3\n",
      "3\n",
      "12116\n",
      "3\n",
      "947\n",
      "3\n",
      "3\n",
      "3\n",
      "32767\n",
      "1343\n",
      "6042\n",
      "3\n",
      "3\n",
      "3\n",
      "20024\n",
      "3\n",
      "3\n",
      "1906\n",
      "1906\n",
      "3\n",
      "3884\n",
      "3884\n",
      "3682\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1958\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3256\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1874\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6771\n",
      "6944\n",
      "3\n",
      "3\n",
      "3\n",
      "9840\n",
      "3\n",
      "5179\n",
      "3\n",
      "3768\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5436\n",
      "5499\n",
      "5499\n",
      "5499\n",
      "3\n",
      "3\n",
      "6947\n",
      "3\n",
      "3\n",
      "4038\n",
      "17820\n",
      "17820\n",
      "17820\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6168\n",
      "3\n",
      "12000\n",
      "3\n",
      "5749\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3832\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5409\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3029\n",
      "3029\n",
      "3029\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6294\n",
      "6294\n",
      "9981\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5049\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3603\n",
      "4782\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "13625\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "971\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3865\n",
      "3\n",
      "9045\n",
      "3\n",
      "3\n",
      "8167\n",
      "3\n",
      "3\n",
      "3\n",
      "5641\n",
      "5633\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4127\n",
      "4127\n",
      "3\n",
      "3\n",
      "3\n",
      "12197\n",
      "4759\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2350\n",
      "3\n",
      "11677\n",
      "3\n",
      "10402\n",
      "3\n",
      "3\n",
      "3440\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1517\n",
      "3\n",
      "5587\n",
      "18212\n",
      "2886\n",
      "1072\n",
      "2802\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4313\n",
      "3\n",
      "3\n",
      "4684\n",
      "3\n",
      "3\n",
      "8578\n",
      "9243\n",
      "9517\n",
      "17268\n"
     ]
    }
   ],
   "source": [
    "nums = []\n",
    "for i in range(df2.shape[0]):\n",
    "   text = df2.iloc[i]['description']\n",
    "   nums.append(len(str(text)))\n",
    "\n",
    "for j in nums:\n",
    "   print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ee1dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_Test split ..\n",
    "train = data.head(15000)\n",
    "val = data.iloc[15000:15500]\n",
    "test = data.iloc[17000:]\n",
    "\n",
    "##  exporting to parquet files\n",
    "train.to_parquet('train.parquet')\n",
    "val.to_parquet('val.parquet')\n",
    "test.to_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01a56160-0703-4f7d-8bb7-a4cad90a6a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default to C:/Users/stude/.cache/huggingface/datasets/parquet/default-2010838a070b5f75/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5560a0cbd346478f9191af4608d59e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4eabbd127d4c45a3fe6bf5b1fddb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90aa2282ce174a9f840f8cf1f4fe3741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/stude/.cache/huggingface/datasets/parquet/default-2010838a070b5f75/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fac98afae864cafb99514f2c279d73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default to C:/Users/stude/.cache/huggingface/datasets/parquet/default-f43c6434c0cc99ee/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3e41ef6f44a4ca59406b078210dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e5b3a4768c484b8c26c759a894bd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf3d177ca284b8695ed618a734f4382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/stude/.cache/huggingface/datasets/parquet/default-f43c6434c0cc99ee/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235f7c9146e4463c99a2b290921f8686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default to C:/Users/stude/.cache/huggingface/datasets/parquet/default-8b3f83829e3804bd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea73dcaeac0f4257b70d7bf5580e7adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c4b226535749aebaf69cb13b301cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c2dc79aa154d1985dfc0d1a5d594ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/stude/.cache/huggingface/datasets/parquet/default-8b3f83829e3804bd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8069b34c05d1447e800f369f449dc352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##  Tokenization, Pre-procesing and Splitting of dataset  ..\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bart_lfqa\")\n",
    "\n",
    "#  Loading dataset  ..\n",
    "from datasets import load_dataset\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "# Loading Train, Validation and Test set ..\n",
    "train = load_dataset(\"parquet\", data_files=\"train.parquet\")\n",
    "val = load_dataset(\"parquet\", data_files=\"val.parquet\")\n",
    "test = load_dataset(\"parquet\", data_files=\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ba21214-f0aa-4b36-8efd-78cecc35770e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfcc4038fe54b46aa05f497b7986300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ca466f4b204f8f8d3bbcd674941cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf926ec2983f44578ef6beeb2f95a5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing function ..\n",
    "def preprocess_function(examples):\n",
    "    \n",
    "    inputs = examples['Input Text']\n",
    "    targets = examples[\"Target Text\"]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_input_length, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "# Driver code\n",
    "tokenized_train = train.map(preprocess_function, batched=True)\n",
    "tokenized_val = val.map(preprocess_function, batched=True)\n",
    "tokenized_test = test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49b92afa-64a8-42a9-bb2f-eecb868bdef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Model part ..\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('vblagoje/bart_lfqa').to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03a668bb-969e-44d8-8fa7-fe6d2375b558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a72778e25c420789d3d264e67ab8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 5.15 GiB already allocated; 0 bytes free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21704\\2931713809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m trainer = Seq2SeqTrainer(model, args, train_dataset = tokenized_train[\"train\"], eval_dataset = tokenized_val[\"train\"],\n\u001b[0;32m     12\u001b[0m                          data_collator=data_collator, tokenizer=tokenizer)\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         )\n\u001b[1;32m-> 1633\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                 if (\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_grad_scaling\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2655\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2656\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_apex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 5.15 GiB already allocated; 0 bytes free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#  Training arguments ..\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\"checkpoints\", overwrite_output_dir=True, evaluation_strategy = \"epoch\", learning_rate=2e-5, \n",
    "                                per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, save_total_limit=2, \n",
    "                                num_train_epochs=1, save_steps = 2500, predict_with_generate=True, fp16=True, push_to_hub=False,)  \n",
    "\n",
    "##  Data collator \n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Trainer API\n",
    "trainer = Seq2SeqTrainer(model, args, train_dataset = tokenized_train[\"train\"], eval_dataset = tokenized_val[\"train\"],\n",
    "                         data_collator=data_collator, tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313bb1f-6755-4ca4-a5f5-f468ab71ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from transformers import AutoModel, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bart_lfqa\")\n",
    "dataset = pd.read_parquet('data.parquet')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/BART_QA-finetuned-QA/checkpoint-7500/\").to(device)\n",
    "\n",
    "questions = []\n",
    "candidates = []\n",
    "references = []\n",
    "\n",
    "for i in range(17000,23000):\n",
    "    inputs = dataset.iloc[i]['Input Text']\n",
    "    orig_ans = dataset.iloc[i]['Target Text']\n",
    "    model_input = tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\") \n",
    "    generated_answers_encoded = model.generate(input_ids=model_input[\"input_ids\"].to(device),\n",
    "                                                 attention_mask=model_input[\"attention_mask\"].to(device),min_length=16, max_length=64,\n",
    "                                                 do_sample=False, early_stopping=True, num_beams=8, temperature=1.0, top_k=None,\n",
    "                                                 top_p=None, eos_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3,\n",
    "                                                 num_return_sequences=1)\n",
    "    ans = tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "  \n",
    "    \n",
    "    candidates.append(ans[0])\n",
    "    references.append(orig_ans)\n",
    "    questions.append(inputs)\n",
    "    print(i, end=', ')\n",
    "    \n",
    "# Saving the results in a dataframe .. \n",
    "indices = [i for i in range(len(questions))]\n",
    "data = pd.DataFrame(indices)\n",
    "data['Question'] = questions\n",
    "data['Reference'] = references\n",
    "data['Candidate'] = candidates\n",
    "data.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f497a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5335, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Loading scraped dataset ..\n",
    "import pandas as pd\n",
    "df2 = pd.read_excel('data.xlsx')\n",
    "df2.columns = ['news_index', 'image_url', 'title', 'text', 'source_name', 'date', 'topics', 'sentiment', 'type', 'description']\n",
    "df2 = df2[['title', 'text', 'description']]\n",
    "\n",
    "##  Generating final dataset ..\n",
    "def format_data(dataset):\n",
    "    inputs = []\n",
    "    #references = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        text = 'question: ' + dataset.iloc[i]['title'] + '. ' + 'context: ' + str(dataset.iloc[i]['description'])\n",
    "        #reference = dataset.iloc[i]['Reference']\n",
    "        inputs.append(text)\n",
    "        #references.append(reference)\n",
    "        \n",
    "    df = pd.DataFrame(inputs)\n",
    "    df.columns = ['Input Text']\n",
    "    return df\n",
    "\n",
    "# Driver code\n",
    "scraped_df = format_data(df2)\n",
    "scraped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1def2da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 'Top Healthcare Stocks' to Buy in March. \n",
      "I'm not sure if this is what you're looking for, but I'll give it a shot. I'm a pharmacist, and one of the things I love about my job is that I don't have to deal with a ton of paperwork. I can just go to the pharmacy and pick up whatever I need, and then I can go home and take care of the rest of my patients. This is a great way to save money, and I think it will be a good way to make money for a long time to come. If you want to learn more about it, I highly recommend checking out the company's website. They have a lot of great information on their website, and it's a great place to start if you're interested in learning more about pharma. They also have a great podcast called \"Pharmacy Insider\" that you can listen to while you wait for the stock to go up. It's a really good podcast, and if you have any questions, I'd be happy to answer them. I'd also like to point out that I'm not an expert on pharma, so if anyone has any questions or concerns, I'm happy to try and answer them as best as I can. I hope this answers your question, but if you don't want to read the whole thing, you might want to try /r/askmeafact or another stock recommendation service like Stock Advisor, which has a lot more information on the stock market, as well as how to find the best stocks to invest in. Good luck, and good luck investing in pharma! I'm sure you'll find some great stocks to buy in the next few months, so I'm looking forward to seeing what you come up with! I hope that helps! Edit: Here's a link to the podcast, if you'd like to listen to it, just click on the link and it will take you directly to it. The podcast is available on iTunes, Google Play, and Stitcher, but it's not available on any of the other services that I mentioned, so you'll have to wait a while for it to be available on all of those services. I'll try to find it when I get home, but hopefully it'll be available in a few days. If not, I'll look for it on Google Play or Stitcher. If I find it on my phone, I can try to track it down. If it's available on Amazon or something like that, you can also try searching for it.\n",
      "----------\n",
      "How Safe Are AMC Entertainment and Its Dividend?. \n",
      "AMC has a lot of debt. The company has a $5.4 billion debt load, and that debt is coming due in 2022. That means that the company will have to pay $1.2 billion in interest on that debt in order to pay it off. That's a lot, and it's going to take a long time for the company to pay off all of that debt, so it's safe to say that the dividend is safe as long as the company can pay off the debt in a reasonable amount of time. If the company doesn't pay off its debt in time, then the dividend will be paid out at a lower rate, and the stock will be worth less than it was before the dividend was paid out. This is why it's a good idea to wait until after the debt is paid off to buy the stock, so that you can get a better return on your investment than if you bought the stock before the debt was paid off. It's also important to note that the debt isn't going to be paid off in a year or two, but rather over the course of several years. So even if the company is able to pay all of its debt off in the next few years, it will take a lot longer than that for the stock to return to its pre-debt value. So if you want to buy a stock today, you should wait until the company has paid off all its debt, and then buy it at a higher price. If you don't want to wait that long, then you should sell the stock now and wait for the price to go back to where it was when you bought it in the first place. That way, you'll have more money to spend on other things, such as building new theaters, and Edit: Also, I forgot to mention that there is a $600 million convertible debt investment from Silver Lake Partners, which will take up most of the extra cash in the company's future. That money will be used to pay down some of the debt, but it won't be enough to cover the dividend in the short term, so the dividend isn't really safe. But it's still safe for the long term, because it's paying off the interest on the debt at a reasonable rate, so long as they don't pay it all off at the same time. Edit 2: I should have mentioned that they're paying off their debt in the future, but I didn't mention that they aren't paying off all their debt at once, so I forgot.\n",
      "----------\n",
      "eBay's $2.5 Billion Boondoggle. \n",
      "MercadoLibre was a huge part of eBay's business in Latin America. It was one of the largest e-commerce platforms in the world, and it was a big part of the reason why eBay was able to grow so fast in the first place. The reason why it was such a big deal is because it allowed eBay to compete with Amazon in Brazil, which was a major competitor to eBay in the early days of online shopping in the region. It also allowed Amazon to expand into other parts of the region, such as Argentina, Brazil, and Chile, which allowed them to grow much faster than they could in the rest of the world. As a result, it was very important for eBay to be able to compete against Amazon in the Latin American market, because it was the only way to make money in that region. Now that Amazon has gone out of business, it's no longer a big threat to eBay, but it's still an important part of their business, so it's important for them to sell off their stake in the company so that they can focus on growing their business elsewhere. This means that they're going to have a lot more money in the bank than they had before, and they can use that money to invest in other things that will help them grow their business in the future. It's also important to note that the money they're getting from selling their stake is going to be used to pay off some of the debt that they have on their balance sheet, which will allow them to pay down some of their debt, which is a very important thing for a company to do. They're also going to use a lot of the money that they get from the sale of their stake to pay for other things they want to do, like building new warehouses, Edit. I should clarify that I'm not saying that this is a bad thing, but I'm saying that it's a good thing that they've decided to sell their stake, and that they'll use the money to build new warehouses and expand their business overseas, so they can make more money than they would have if they hadn't sold their stake. That's why they're selling off their share of the company, and now they'll have more money to spend on other things like building warehouses and expanding their operations in other countries, and investing in other businesses, and so on and so forth. I don't know how much money they'll get out of the sale, but that's what they're planning to do with it, and I'm sure they'll do it anyway.\n",
      "----------\n",
      "Buy OrganiGram Holdings, Not Cronos Group. \n",
      "Cronos Group has a lot of money, but it doesn't have much in the way of assets. OrganiGram, on the other hand, has a ton of assets, and a lot more money in the form of debt. If you want to invest in a pot company, you're going to want to make sure that you have the money to do so, and that you know what you're doing. That means that you'll want to know what the company is doing, and what it's going to do in the future, so that you can make sure you're getting a good return on your investment. Cronos Group isn't going to be able to do that for a long time, because they don't have the assets to do it, and they're not going to have enough money to make it worth it in the long run. That's why it's not worth investing in Cronos, because it won't be worth it for a very long time. That said, there are plenty of other pot companies that are doing well right now, and you can buy them for a lot less than you're paying for them. They're just not worth buying at the moment, because you don't know what they'll be doing in a few years, or if they'll even be worth anything at all in a year or two. If they're doing well, then you'll probably want to buy them at a higher price than they're currently trading for, because there's a good chance that they'll continue to be doing well for a while, and if they aren't, you might want to sell them to someone else for a bit more than you paid for them, and then buy them back at a lower price. You might also want to look at other Pot stocks that you could consider adding in their place. I'm not sure if they're worth buying now, but they're definitely worth looking at if you're interested in investing in the pot industry, and I think they're probably worth a look in the near future. Source: I'm an investor in the marijuana industry, so I know a lot about the industry, but I don't really know enough about it to make an informed decision on whether to buy or not to buy pot stocks, because I haven't invested in any of the pot stocks I've seen in the last year or so. I've only seen a handful of pot stocks in the past year, and none of them have been worth much more than $1,000 or less. I think\n",
      "----------\n",
      "Tesla Unveils Model Y: What You Need to Know. \n",
      "Tesla Model Y: What You Need to Know The Model Y is a mid-sized SUV that will be available for $39,000. It will have a range of 230-300 miles, and a top speed of 5.9 seconds. The price will start at $47,000 for the standard model, and will go up to $60,000 if you upgrade to the performance version. The standard Model Y will start shipping in the spring of 2021, and the high-end versions will be delivered in the fall of 2021. The specs of the Model Y are pretty much the same as the Model 3, except for the extra $3,000 you'll have to pay to get the seven-seat configuration. The range will be the same, but the top speed will be a bit faster, and you'll need to pay a bit more to get it to go from 0-60 mph in a reasonable amount of time. The performance version will also have a higher top speed, but it won't be as fast as the regular Model Y, so it's not going to be as fun to drive as the standard Model S. The battery will be about the same size as the battery in the Model S, and it will have the same amount of power. The only thing that will change is the size of the battery pack. The Model S has a battery pack that's about 1.5 times as big as the average battery pack in the U.S., and the Model X has a 2.5-3 times larger battery pack than the average one. This means that if you want to drive your Model Y for a long time, you're going to have to spend a lot more money than you would if you wanted to drive it for a short time, and Tesla. If you're willing to spend more money, you'll be able to get more power out of your battery pack for a longer period of time, which is a big deal for a car that's only going to last a few years. It's also important to note that the battery packs will be different for each model, so there's no guarantee that you'll get the same power for the same length of time with the same battery pack, but you'll probably get better performance out of the same number of battery packs. The most important thing to know is that it's going to cost a lot of money to make it, and that's why it's so important to make sure that people are willing to pay that much money for it, because it's\n",
      "----------\n",
      "Expect 2 New Dow Stocks Before 2019 Ends. \n",
      "DowDuPont and United Technologies are two of the largest companies in the world, and they are both part of the Dow Jones Industrial Average. The Dow is an index of the 30 largest U.S. stocks, and it is based on how many of those stocks are in the S & P 500, which is another index based on the same 30 stocks. This means that there are 30 stocks that make up the Dow, and each of those 30 stocks is worth a certain amount of money. So if you want to buy a share of a company, you need to be able to sell it at a certain price, and if you can't sell it, you have to buy it from someone else. If you don't have enough money to buy the stock, you can sell it to someone else, who then sells it to you at a different price. This is how the Dow works, and this is how it's supposed to work. It's not perfect, but it's good enough for the purposes of the index. The reason why these two companies are going to leave the Dow is because they are too large to be part of it anymore. They're too big to be included in the index, but they're still worth a lot of money, so they're going to go out of business. If they don't go out, they'll be replaced by a new company, which will be smaller, but still worth more money than the old company, and will be added to the Dow. If the new company doesn't do well, it will be removed from the index and replaced by another company. If it does well enough, it'll be added back to the index as a new stock, but if it does poorly, it won't be replaced as a stock, Edit: I should clarify that I'm not saying that the Dow should be replaced, I'm saying that there should be two new stocks added to it. I'm just saying that if there are two new Dow stocks added, it's likely that one of them is going to be smaller than the other, and the other one will be larger than the one that's already in the Dow.\" Source: I'm a stockbroker, and I've been following the Dow for a while, so I know what I'm talking about, but I don't know how to explain it to a five-year-old, so here's what I'll do: I'll explain it like I'm five years old. I'll use the Dow as an example. I\n",
      "----------\n",
      "3 Growth Stocks to Buy and Hold for the Next 25 Years. \n",
      "Growth investing is all about focusing on the future. What is a company doing that is going to keep its top and bottom lines expanding over time? Here's why technology giants Alphabet (GOOG -1.92%) and Amazon (AMZN-1.27%) are the types of growth stocks that investors will want to take to their graves. Google is the largest search engine in the world, and has a huge market share. It's also one of the most profitable companies on the planet. Amazon is the second largest online retailer, but it's not the only one. It also has a massive market share in the cloud computing space, which is where most of the world's computing is done. Google has a lot of other things going for it as well, but these are the three things that I think are the most likely to keep it going for the next 25 years or more. The other two are probably going to be a bit more difficult to pin down, but I think they're the ones that are going to make the most sense for you to buy and hold for a long time. **Growth Stocks to Buy and Hold for the Next 25 Years** 1. Alphabet: Google is a huge search engine, and it has a large market share, so it's going to continue to grow. 2. Amazon: Amazon has an enormous market share and a huge competitive advantage. 3. Hormel Foods: I'm not sure if this is the one you're looking for, but you might want to look into it. 4. Berkshire Hathaway: I don't know if this qualifies as a growth stock, but if it does, I think it's worth a look. 5. Facebook: Facebook is the biggest social network on the internet, and 7. Apple: Apple is the most popular company on the web, and will continue to be for a very long time to come. 6. Microsoft: Microsoft is a great company, but they're going to go out of business in a few years. 7. Netflix: Netflix is the best streaming service on the net, and I think that will be the next big thing. 8. Tesla: Tesla will be a big company in the next 20 years or so. 9. Microsoft will be worth a lot more than it is now. 10. Apple is a lot less than it was 10 years ago. 11. Microsoft is worth about $100 billion right now. 12. Apple will probably be worth more in the near future. 13. Facebook is worth more\n",
      "----------\n",
      "XPO Off-Loads Chief Operating Officer as Focus Shifts. \n",
      "XPO Logistics (XPO -1.54%) is cutting ties with its Chief Operating Officer after less than a year, a fresh indication that the transport and logistics operator is altering its strategy after hitting a rough patch in recent quarters. In a statement to MarketWatch, XPO said the decision to let Wagers go was made after the company moved away from pursuing acquisitions in favor of deploying capital for stock repurchases. Wagers was hired in April 2018 in part to help the acquisitive company plot the next stage of its merger and acquisition (M&A) strategy. However, the company has not been able to find a suitable M&A target in the past few months, and Wagers' departure is a sign that the company is shifting its focus away from M & A in order to focus on its core business, which is delivering packages to retailers and e-commerce companies. The CEO of XPO Logistics, Bradley Jacobs, is expected to take the helm of the company in the next few months. In the meantime, the board of directors has authorized a new $1.5 billion stock repurchase program to be funded with cash and debt. The company's stock price has fallen 17.2% in February after reporting fourth-quarter earnings that fell considerably short of estimates, and have lost more than half their value over the past six months. The stock price is likely to continue to fall in the coming months as the company continues to repurchase its own stock, but it's likely that the stock price will continue to rise in the near future as the board continues to look for a suitable candidate for the company's COO position. Source: I'm an investor in XPO, and I've been following the company for a while, so I'm Edit: I forgot to mention that Wagers is the COO of XPO Direct, which provides Amazon with integrated logistics, transportation, and delivery services. This is not the same as Amazon's Fulfillment by Amazon operation and should help non-Amazon retailers to better compete against the online giant. It's also worth noting that Amazon has not pulled its business from XPO because of Wagers, but rather because of Amazon's decision to pull business from the company because it saw Wagers as a distraction from Amazon's efforts to grow its own logistics business. I'm not an expert in the industry, but I think it's important to note that the CEO's departure is not a sign of a shift in strategy, but an indication of a change in strategy\n",
      "----------\n",
      "The Retail Apocalypse Is Not as Bad as It Seems. \n",
      "It's only mid-March, and U.S. retailers have already announced close to 5,000 store closures since the beginning of 2018, according to Coresight Research. It's not as bad as it seems, but the pace of store closures in 2019 is very manageable compared to the demand for new space. In fact, the biggest source of new space on the market this year is likely to be Sears (again). In conjunction with its bankruptcy proceedings, the retail icon announced more than 250 stores in late 2018 that are being implemented in the first three months of 2019. J.C. Penney plans to close another 27 stores this year, although some of those are smaller locations. In addition, the vast majority of the stores closing in 2019 are \"in-line\" retailers that operate much smaller stores. For example, the off-price retail sector has taken over lots of space previously occupied by department stores and big-box retailers in recent years. The roughly 1,000 stores that got the axe accounted for well over 100 million square feet of space. That's not necessarily the best metric for tracking the impact of the retail apocalypse, but it's a good place to start if you're interested in understanding what's going on. Source: I'm a retail analyst, and I've been involved in the retail industry for over 20 years. I'm not a retail expert, but I've done a fair amount of research on the industry and I think I can give you a good idea of how the industry is doing right now. The retail apocalypse is not a bad thing, it's just that there are a lot of things that are going on right now that make it hard to predict what will happen in the future. If you want to know more, I'd recommend reading The Business of Retail: A Short History of the Business of the Retail Industry by Michael Shermer. Shermer's book is a great read if you want a more in-depth look at the industry as a whole. I'd also recommend checking out the book \"The Retail Apocalypse\" by Mark Kurlansky. Sherwin's is a very well-regarded book on the subject, and it's one of the best books I've ever read on the topic. I highly recommend checking it out if you have any follow-up questions about the industry, or if you'd like to learn more about what's happening in the business of retail right now, or how it's going to change in the next few years, or any other aspect of the industry.\n",
      "----------\n",
      "Tread Lightly With Momo Stock Despite Its 50% Jump to Start 2019. \n",
      "Momo's stock is up 50% since the start of 2019. That's a pretty big jump. However, it's not going to last forever. It's going to hit a wall later on this year. The stock is going to have a hard time keeping up with the growth of the company. The company has been growing at a very high rate for the past few years, but it's starting to slow down. This means that the company's growth will slow down in the next few years. If that trend continues, investors looking to pick up shares for the first time should exercise some caution. If the company continues to grow at a high rate, the stock will continue to go up. If Momo's growth slows down, investors should expect the stock to go down. If it doesn't slow down, they should expect it to go back up. I'm not saying that Momo stock is a bad investment, I'm just saying that it's a good investment if you're willing to wait a few years for it to get back to where it was. If you're not, I'd advise you to take a look at some of the other companies that are doing well right now. If they're doing well, you might want to look at Facebook, Twitter, Google, Amazon, etc. They're all doing well at the moment, and they're all going to be doing well for a long time to come. But if they don't do well, they might not be able to keep up with their growth for a while, and the stock might go down a lot. I don't think that's a bad thing. I think it's just a sign that the stock isn't going to do well in the long run, and if it does, it might A. Facebook is doing well. B. Twitter is doing poorly. C. Amazon is doing okay. D. Apple is doing OK. Facebook isn't doing so well. They are all doing okay right now, but they're not doing great. So I'd say that they're going to need to do a lot of things to make up for their slow growth, and I think that they'll need to start doing a lot more things to get their growth going faster than they've been doing in the last couple of years. They need to figure out how to get more people to use their services, and that's what they need to spend more money on their services. They also need to find ways to make more people using their services faster, and make more money faster.\n",
      "----------\n",
      "Amazon's 2nd headquarters faces new blocks in Virginia funding vote. \n",
      "I'm not sure if this is what you're looking for, but I'll give it a shot. I live in Arlington, VA, and I've been here for a few years now. I've never had a problem with Amazon, but there are a lot of people here who don't like the idea of them coming here. They don't think they'll be able to live in the area long enough to support themselves, and they don't want to have to deal with the influx of high-paying jobs that would come with it. They're also worried that they'll have to pay a lot more taxes than they already do, and that they won't have enough money to pay for things like roads, schools, etc. I don't know how much of an impact this will have on the local economy, but it's definitely going to have a big impact on the tax base of the area. It's also important to note that this isn't just about Amazon, it's also about the city of Alexandria, which is the largest city in the region. I'm sure there are other factors, but this is the one I'm most familiar with, so I'll try to explain it as best as I can. First off, I'm going to assume that you're talking about the Amazon HQ2 project, because that's what I've heard the most about it, and it's the only thing I can think of off the top of my head. Second, I think it's important to keep in mind that this is just one of many proposals that have been put forth by the city and county of Arlington. There are many others, but these are the ones that I've seen the most. I think the biggest issue is the fact that these proposals are being put forward The City of Alexandria has said that they will not allow Amazon to build on the site unless they pay a certain amount of taxes to the city. The city has also said that if they do not pay the taxes, then the city will not be allowed to use the land for the HQ2. This means that the city won't get any money from Amazon, and the county will not get any tax revenue from Amazon. This is a huge deal for the city, because it means that Amazon will be paying a huge amount of money into the city's coffers, which will be used to build roads, bridges, and schools, and other infrastructure. It also means that there will be a huge influx of people who will be living in the city for a long time.\n",
      "----------\n",
      "5 Sizzling Biotech Stocks Under $10 With Gigantic Upside Potential. \n",
      "Here's a list of 5 Sizzling Biotech Stocks Under $10 With Gigantic Upside Potential. If you want to know more about them, I'd recommend checking out /r/explainlikeimfive. They're all under $10, and they're all in the biotech space, so there's a lot of room for growth. Here's the list: 1. Biotech Stock 2. Stock 3. Stock 4. Stock 5. Stock 6. Stock 7. Stock 8. Stock 9. Stock 10. Stock 11. Stock 12. Stock 13. Stock 14. Stock 15. Stock 16. Stock 17. Stock 18. Stock 19. Stock 20. Stock 21. Stock 22. Stock 23. Stock 24. Stock 25. Stock 26. Stock 27. Stock 28. Stock 29. Stock 30. Stock 31. Stock 32. Stock 33. Stock 34. Stock 35. Stock 36. Stock 37. Stock 40. Stock 41. Stock 43. Stock 44. Stock 45. Stock 46. Stock 47. Stock 51. Stock 52. Stock 53. Stock 54. Stock 55. Stock 60. Stock 61. Stock 62. Stock 65. Stock 67. Stock 72. Stock 80. Stock 81. Stock 83. Stock 84. Stock 89. Stock 90. Stock 101. Stock 100. Stock 103. Stock 120. Stock 104. Stock 99. Stock 130. Stock 01. Stock 00. Stock 111. Stock 110. Stock 115. Stock 123. Stock 0. Stock 1. Stock 102. Stock 105. Stock 03. Stock 04. Stock 98. Stock 95. Stock 02. Stock 49. Stock 50. 100. 10. 100% Stock 00, 00. 00. 100, 100. 00, 100,000 Stock 100, 0. 0. 1. 10, 100% 100. 100%. 100. 50. 50% 100, 10. 10% 100% of 100. 1, 00, 1. 100. 100, 1, 100 and 100, etc. 1% of 1. 0, 0, 10, and 1. 1 100, 50. 1 of 100, or 100, 2. 1 1. 50, etc, 1 of 1, and 0. 100 and 1, or 1, 50, and the 1, 2, and 10, 1 1, 3, and 2, 1: 100, and a 1, 1 and the stock, or 10, or the 1. 20, etc of the stock. 100 of the number. 1 than 1.\n",
      "----------\n",
      "Ford CEO Made 276 Times Median Worker in 2018. \n",
      "Ford CEO Made 276 Times Median Worker in 2018. The median worker is the person who makes the most money. Ford CEO made 276 times the median worker. That's a lot of money. It's not enough to make Ford CEO rich. Ford's CEO makes 276 times as much as Ford's average worker. Ford makes 276 Times more than Ford CEO. Ford is making 276 times more. Ford made 276 Times less than Ford. Ford was making 276.5 times less. Ford lost 276.6 times more than that. Ford earned 276.7 times less than $276.5. Ford paid 276.8 times less of that than $275.5 more than $277.5 less than it. Ford did not make 276.1 times more of that. It made 276.0 times more money than $27.1 more than it did. Ford had 276.2 times less money than it made of that number. Ford spent 276.9 times more dollars than it spent of that amount of money than that number of dollars. Ford has 276.3 times less dollars than that amount. Ford. That number of people. Ford said 276.000 more than they made of the number of times. Ford, Ford made more than the amount of dollars than they paid of that company. Ford and Ford. They made more of a company. They paid more than a number of other companies. Ford than a company than they had more of the company than the company. The number of companies than a car. Ford of a car than a person. Ford as a company, they made less of a person than a corporation. They had a company and a car, and a company that company, and the company, a company of a corporation, and they made a car and a. company. a company in a company or a company with a company's company. company, Ford, a person, a country, a car that company and company, or a car or company, the company and the car, a corporation and a country. a person of a country and a person and company. the company of company, it's company, another company, that company than company.company, and another company. other company. it's a company making a company on a company; a company as a corporation of a number. company and one of a state, a number, a state.company.company and a corporation that company or company. person. company of the country.company of company.self, company, company.a company.\n",
      "----------\n",
      "Amazon May End Up With Only One Headquarters — Its Current One. \n",
      "Amazon may end up with Only One Headquarters — Its Current One. It's not clear if that's a good thing or a bad thing, but it's a possibility. Amazon's HQ is located in Seattle, Washington, so it's not like it's the only HQ in the world, it's just the one that's closest to Seattle. The other HQ is in Portland, Oregon. Seattle. Portland. Oregon. Portland, OR. Seattle, WA. Vancouver, Washington. Washington. Seattle Washington. Portland Oregon. Washington Washington. Vancouver Washington. New York. New Jersey. New England. Seattle Oregon. New Hampshire. New Mexico. Portland Maine. Washington Oregon. Massachusetts. Washington Virginia. Washington, Oregon, New Jersey, New York, New Hampshire, New England, New Mexico, Washington State, Washington and Oregon. Oregon, Washington state, Oregon and Washington, or Oregon, or Washington, Idaho, Oregon or Washington. Oregon and Oregon, California, or California. Washington and California, Washington or California, and California. Oregon or California or Oregon. California, California. California and California or California and Washington. California or Washington State. Washington State or Oregon and California and Oregon or Oregon State, or Canada. Washington or Oregon or Canada, or Virginia, or Texas, or the United States, and Canada, and the United State of California, Canada, California and Canada. or the U. State of America, or some of these states, and some of those states, or a lot of others, or other states, some of them, and others, and a few states, etc. and some states. or some states and states and some other states. States, some states, a few of states. states, states and other states and others. States and states, others, states, of states, other states of states and a state, or states, cities, or others. states of the states, state, and states. state of a state of states or states and companies. states and cities. states that states of a country. states. some states of companies, states of other states or a state and states of some states' states, the states of others of states that state of the state, states. of states of state. states with states, companies, and companies, of a states of many states, many states of countries. states' state of companies of states than states of cities, states or companies, or cities, and of states with a states, countries, states in states.states of states states.\n",
      "----------\n",
      "Medical Cannabis Cultivator Green Peak Innovations Raises $30 Million In Mezzanine Debt Round. \n",
      "Green Peak Innovations Raises $30 Million In Mezzanine Debt Round. That's a lot of money for a company that doesn't have much in the way of cash. It's not like they're going to be able to do anything with it. They're not going to make any money off of it. It doesn't matter what they do, they can't do anything. They can't make money off it, because they don't have any cash. They have no money. They don't even have any assets. They need to make money. And they need to do something. And that's what they're doing. They've got no cash. No money. No assets. No stocks. No stock. Nothing. No investments. No debt. No credit. No interest. No loans. No equity. No shares. No bonds. No mortgages. No real estate. None of that. No food. No cars. No houses. No buildings. No banks. No businesses. No people. No jobs. No taxes. No medical marijuana. No weed. No alcohol. No drugs. No cigarettes. No doctors. No hospitals. No booze. No guns. No pot. No insurance. No marijuana. Marijuana. No life. No health care. No water. No sex. No electricity. No kids. No plants. No cash. Marijuana, no drugs. Marijuana and alcohol. Weed. Weed, weed. weed. money. weed, weed, money. money, money, drugs. weed and money. marijuana. money and money, marijuana, money and weed. pot. money; money. pot, weed and weed, marijuana. weed; money, pot, money; weed. marijuana, weed; weed, drugs, money: weed, pot. drugs. money: money. water. weed: money, people. weed (money. money (money, weed), weed, and money; marijuana. marijuana and weed; marijuana, marijuana; weed and marijuana. pot; weed; pot, marijuana and marijuana, water, weed: weed. cannabis. weed) money, and weed and pot. weed? weed, tobacco, weed (pot, weed) weed, cannabis, weed-pot, and marijuana; money and marijuana and money and pot, pot and weed: marijuana, and the weed, or weed, the weed. people, weed of weed, people, money of weed. me, weed to weed, me, and me. weed of marijuana, the money, the marijuana, pot\n",
      "----------\n",
      "The Week Ahead In Biotech: Conferences, PDUFA Dates, Clinical Trials And Earnings. \n",
      "The Week Ahead In Biotech: Conferences, PDUFA Dates, Clinical Trials And Earnings The Week Ahead in Clinical Trials: Clinical Trials and Earnings In Biopharmaceuticals: The week ahead in clinical trials: Clinical trials and earnings The week Ahead in biopharmacology: Earnings Earnings: Biopharma Earnings Overview: Biotech Earnings Announcement: Biotechnology Earnings Announcement: BioPharmaceutical Earnings Conference: Clinical Trial Announcements: Research Announcements Biopharmacological Earnings Anouncements: Regulatory Announcements BioPharmacology Earnings Exclusives: Immuno-Medicine Earnings (Pharmacy Earnings): Clinical Trials/Earnings Announcements/Regulatory Announcements/Patrols/Clinical Trials/Drugs/Data Announcements (Biopharma, Biotech, Pharmaceuticals, Medical Devices, Biotechnology, Medical Sciences, Medical Technology, Biomedical Research, Biophysics, Biotherapeutics, Biomedicine/Biotech/Medicine/Medicines/Biotechnology/Medical Devices/Biopharmacine/Pharmacaceuticals/Medical Sciences/Biomedical Research/Medical Technology/Medical Science/Pharmistry/Biotherapeuticals/Bioopharmacines/Medical Therapeutics/Microbiotech/BioPharmacine & Biotech/Medical Research/Biotherapy/Pharma/Physics/Phology/Medical Biotechnology/Bio-Pharmine/Medical Medicine/Medical-Pharmacines. Biotech and Biotech (Biotech: The Biotech Week Ahead of Clinical Trials & Earnings Week Ahead Of Clinical Trials; Clinical Trials, Earnings, Research, Research Dates, Data Announcements, Data, Data Dates, Biotherapy, Data and Clinical Trials. Biotherapy/Data, Data Annunciations, Research Weeks, Data Weeks, Research Announceings, Data Days, Data-Earnings, etc. The Week of the Week of August 27th, September 28th, October 29th, November 30th, December 31st, October 31, 2014, November 31, 2015, December 30th and December 31, 2016, February 2, 2017, February 3, 2014. Year of 2018, February 31, 2018, and November 30, 2017: The Year of the week of the year of the Biotech Review, February of the month of the Year of Biotech Study, and February of February, etc, and October of the rest of the day, February, and\n",
      "----------\n",
      "RigNet, Inc. (RNET) CEO Steven Pickett on Q4 2018 Results - Earnings Call Transcript. \n",
      "RigNet, Inc. (NASDAQ:RNET) CEO Steven Pickett on Q4 2018 Results - Earnings Call Transcript: March 15, 2019 11:00 AM ET Company Participants Lee Ahlstrom - SVP & CFO Steve Pickett - CEO, President & Director Conference Call Participants Allen Klee - Maxim Group Josh Goldberg - G2 Investment Partners Management Walter Chancellor - Macquarie Research I'm going to assume you're talking about RigNet's fourth quarter and full year 2018 results. I'm assuming you're referring to the company's financial results for the quarter and the year. If you're not, I'll ask you to clarify what you mean by \"Q4 2018 results\" and I'll be happy to answer any follow-up questions you may have. Thank you for joining us today, and I look forward to seeing you at the end of the day. I hope that answers your question, and if you have any follow up questions, I'm happy to be able to answer them. If not, please feel free to ask them in the Q & A portion of the conference call. Thanks again, and have a great day! [Operator Instructions] Thank you, and good morning, everyone. My name is Victor and I will be your coordinator for today. I'll try to answer your questions as best as I can, but I'm not an expert in this field, so please bear with me. First, I'd like to thank everyone for taking the time to participate in this call. It's been a while since I've done this, so I'm looking forward to doing it again in the future. I also want to thank all of you for your hard work and dedication to RigNet, and thank you again for your continued support RIG. I appreciate the time and effort that you put in to make this happen. Thanks for your patience and understanding, and thanks for your interest in the company. I will try to do my best to keep this call as informative as possible, but please keep in mind that this is a conference call, and we don't want to give you any information that isn't in-depth, so if you don't have access to it, please don't ask questions. Thanks, and keep it short and sweet, and don't worry about it, we'll do our best to make it as simple as possible. [End of Q&A] Thanks, Victor, thanks for doing this, and sorry for any followup questions, and appreciate your patience.\n",
      "----------\n",
      "Is Diageo (DEO) a Buy?. \n",
      "I don't think Diageo's stock is a \"buy\" per se, but I think it's a good place to start if you're looking to invest in a company that's doing well. The company is doing well because it's making a lot of money, and it's doing so in a way that makes it seem like it's going to continue to do so for a long time to come. If you want to buy the stock, you're going to have to be willing to pay a lot more for it than you would for, say, Coca Cola, which has been doing well for a few years now. But it's not going to go away anytime soon, so you're probably going to want to wait for it to do well before you start investing in it. It's also worth noting that the company's stock price has been going up for a while now, so it's probably not a good idea to buy it right now unless you're willing to wait a while for the stock price to go up a lot, which it probably isn't going to do. The stock price is going up because people are buying it because they think it will do well in the future, which is a good thing, but it's also a bad thing because it means that the stock is going to keep going up, which means that you'll have to wait until it's worth more than what you paid for it in the first place, which will probably take a while. But if you think the stock will be worth more in the long run, then you'll probably want to give it a shot, and I think that's probably the best thing you can do right now. I'm not sure if it's the right thing to do at the moment, though, Edit: I should clarify that I'm talking about the company that makes the world's best-selling Scotch whisky, not the one that makes all the money in the world, so I'm going to assume that's what you're thinking of when you say \"is it a buy\". I'm just saying that it's an interesting company, and that you might want to look into it a bit more before you make a decision on whether or not to invest. I don't know if you'll be able to make any money off of it, but if you do, I'd be happy to recommend it to someone who knows more about it than I do, and if you have any questions about it, I'll try to explain it to you in more detail.\n",
      "----------\n",
      "The Tesla Network Is Dead. \n",
      "The Tesla Network isn't dead. It's just not going to happen anytime soon. There are a lot of reasons why the Tesla Network is dead, but one of the main reasons is that it's not a viable business model. The Tesla Model S is the best car in the world, and it's the only one that can be used for self-driving cars. The Model X is a better car than the Model S, but it can't be used as a self driving car because it doesn't have the technology to do so. So, if you want to use your Model S for self driving cars, you're going to have to buy a new car, and that's going to be a lot more expensive than buying a Model S. If you're willing to pay for the car, then you'll be able to use the car as a ride-hailing service. The problem is that there's no way to make a car that can do that. The only way to do that is to have a fleet of cars that you can rent out to people who want to drive around in their cars. That's not really feasible, because there's not enough space for all the cars that need to be rented out, and there's only so much money you can make from renting out your car to people that want to rent it out to other people. There's also no way for you to make money from renting your car out to someone who wants to rent out their car to someone else, because you can't charge them for the privilege of using your car. So the only way that Tesla can make money is by charging people for using their cars as a way to share their car with other people, and they can't do that because they don't have any way to charge The Model S isn't going anywhere anytime soon, and the Model X won't be around for a long time, so it won't make any money in the near future. Tesla's valuation is based on future mobility-as-a-service revenues. These are unlikely to materialize anytime soon (or perhaps ever), they will have to revise their price targets to reflect the fact that they're not making any money at all. They'll have to lower their expectations for future revenue, which means that they'll need to raise their expectations about future revenue. This means that the company will need to lower its expectations about how much they'll make in the future, which will mean that they have to raise the price of the company. This will mean they need to increase\n",
      "----------\n",
      "The Clock Is Ticking On DISH Network. \n",
      "The Clock is Ticking On DISH Network. The stock has been on a downward spiral since the summer of 2017. The company has borrowed over the years, pushing the total debt to $15 billion as it currently stands. This is a very large amount of debt, and the company has been unable to pay it back. This means that the company is in a very precarious position, and it is unlikely that it will be able to pay off its debt in the foreseeable future. It is also worth noting that the stock is trading at a very low valuation compared to the rest of the S & P 500. This makes the stock highly speculative and a potentially volatile investment. TL;DR: The stock is going to go down, but it is not likely to go back to where it was before the recent drop in the share price. If the stock does go back up, it will likely be at a much higher valuation than it is now. If it doesn't go up, the stock will likely stay at a low valuation for a very long time. EDIT: I should add that I am not an expert on the stock, but I have been following the company for a while, and I think it is a good idea to keep an eye on the company's financials. If you want to learn more about the company, I highly recommend checking out their website. They have a lot of great information on their website, and they have a great podcast on their YouTube channel called \"The Dish Network Podcast\" that you can listen to if you are interested in investing in the company. The podcast is available on iTunes, Google Play, and other streaming services, and you can also listen to it on Spotify, Pandora, and Stitcher. They also have a podcast called \"Sling TV The Edit: Here is a link to the podcast that explains the whole thing better than I could ever explain it myself. I hope this helps. I'm sure someone else can give a better explanation than I can, but this is the best explanation I could come up with at the moment. If anyone has any questions or concerns, I would be happy to answer them. I just wanted to point out that I'm not a professional stock analyst, so I don't know what I'm talking about, so please feel free to correct me if I'm wrong. Thank you for your patience and I'm sorry if I've made any mistakes, I'm just a layman, I just don't have the expertise to give you a good explanation of what I think.\n",
      "----------\n",
      "Ambarella Turning The Corner. \n",
      "Ambarella is in the process of transitioning from a consumer video processing company to a full-blown CV (computer vision) solutions provider. The transition will take time, so there is no immediate hurry to rush into the shares, in our view. However, there are considerable opportunities here, not in the least because there are so many applications for the company's video processing technology. For example, there is a growing market for self-driving cars, and the company is well positioned to take advantage of that market. There is also a market for autonomous vehicles, where the company has a significant market share. The company is also well positioned in the security camera market, where it has a large market share, and where it is expected to make a lot of money in the future. In short, the transition is taking time, but it will take the company a long time to make the transition, and it is likely that the company will make a profit in the long run. If you want to invest in the company, you should wait until the company makes the transition to a more profitable business model, and then you should buy the stock. The stock is currently trading at a discount to the market, so it is unlikely that the stock will be worth much in the near future, but if it is worth much, you might want to wait until it becomes a profitable business, and if it isn't, you may want to sell the stock to get a better return on your investment. If the stock is going to be worth a lot in the short term, it should be worth buying the stock when it becomes profitable. If it is not profitable, it might be worth holding onto the stock for a longer period of time, and you should consider selling it to someone who is willing to pay more Edit: I forgot to mention that the CV (Computer Vision) market is growing, and that it is becoming more and more important for self driving cars. This is a good thing, because it means that there are more applications for their video processing technologies, and they are making a lot more money, which means that they will be able to invest more in the business side of the business, which will allow them to make more money. Edit 2: I should mention that they are also making more money than they were in the consumer market, which is why they are doing so well. Edit 3: They are still making money, but not as much as they used to be. Edit 4: I'm not sure why they were making so much money. EDIT 5\n",
      "----------\n",
      "Chegg's Scaling Efforts In A Fragmented Industry Reveal Long-Term Potential. \n",
      "Chegg's Scaling Efforts In A Fragmented Industry Reveal Long-Term Potential. Chegg is a student resource and services provider that offers a variety of services for students at the high school and college levels. These services include online tutoring, test prep, and math/writing help - all offered from an integrated source. As a result, Chegg's revenue is growing at a CAGR of 45% since 2012. This rapid growth is helping Chegg scale its operations, and operating margin has drastically improved over the past few years. As the company continues to grow and acquisition spending slows down, the company's profitability will follow suit. However, the stock is overvalued at 60X 2019 earnings. This doesn't even factor in the eventual tapping of international markets. The company faces a massively underutilized customer base, just 3.1 million of 36 million students in the US use Chegg. The service helps itself by appealing through accessibility, and we see strong growth ahead. We like Chegg as a long-term growth play, but investors should be aware of valuation at all times - even when it comes to a pure growth stock such as Chegg, it should be taken with a grain of salt, as the company has only penetrated only a fraction of the market (students), and the long term growth potential of the company is not evident. This is a great question, and I hope someone with more knowledge on the subject can chime in with a more in-depth answer, but in the meantime, you may be interested in these previous posts: * Why is Chegg so hot right now? * Why does Chegg stock have such a high share price? * What are the best ways to invest in Chegg? * Chegg Stock: How do you think Chegg will grow in the future? * How long will it take for Chegg to become profitable? * When will Chegg be worth more than $10 a share? * Should you buy Chegg right now or wait until it's worth more? * Is Chegg a good investment? * Do you think it's a good idea to wait for it to grow? * Does it make sense to buy it now, or should you wait for a better time? * Are there other companies that are doing the same thing? * If so, why are they doing it now? Why are they not doing it right now, and why aren't they growing at the same rate? Why aren't other companies doing it as fast as they are\n",
      "----------\n",
      "Ascena Retail: Investment Thesis Intact Despite Recent Sell-Off. \n",
      "Ascena Retail: Investment Thesis Intact Despite Recent Sell-Off The main thesis on Ascena Retail is that the company has a long runway to complete its turnaround. The company has been struggling with GAAP profitability for the past couple of years. However, when I'm paying 4% of sales for the whole company on the stock market, I don't have to care about revenue growth. I can look at the company's cash flow statements and see that it generates cash flow from operations and is able to pay interest on its debt. It's important to note here that most of the expenses that have been a drag on the expenses are non-cash expenses, not just cash expenses. These are decreasing each year, pointing towards profitability over the next two years. I expect Justice to continue with the wonderful growth, as it continues its journey on becoming a household brand name for Kids Fashion. I also tend to look for opportunities in the most hated sectors in the market. I believe in cheap stocks outperforming exuberant ones in the long run, I still firmly believe that trying to have an edge on the counterparty by using the mere fact that a stock is cheap is foolish. The stock has been nothing short of a roller-coaster ride, rallying more than 150% to $5/share before crashing back to around $2/share on Wednesday. This is because investors had magnified the negatives and discounted them unfairly into the stock price as I referred to them previously. I still believe there is huge upside in the stock, I just don't think it's going to be as high as it has been in the past. I think it will be closer to $1/share in the future, but I'm not sure if that will be enough to buy the stock At this point in time, I think there is a lot of room for the stock to go up, but it will take a while to do so. If you want to learn more about the company, I highly recommend checking out their website. They are a great company with a great management team, and I think they will be able to turn the company around in a very short amount of time. They have a ton of growth potential, but they have a long way to go before they can turn it into a profitable company. They also have a huge cash flow statement, which is why I think the stock will be worth a lot more than what the market thinks it is worth in the short term. They will continue to grow in the next few years, and\n",
      "----------\n",
      "Is Aurora Cannabis the Next DryShips?. \n",
      "Aurora isn't the next DryShips, but it is the next big thing in the marijuana industry. It's been a while since I've been in the industry, so I don't have a lot of experience with it, but I'll give it a shot. A lot of people are saying that Aurora is going to be a big player in the cannabis industry in the near future, and that it's going to make a ton of money. I think that's a good thing, because it means that a lot more people will be able to buy and sell shares of the company, which means that they'll have more money to invest in the company. However, I think it's also important to note that Aurora isn't going to become a huge company in the short term. It will be a small company, and it will have a very small share of the market in the long term. So, it's not like it will be the next Dryships, or even the next Cannabidiol, or anything like that. But it's definitely going to have a big impact on the future of the industry. If you want to know more about it, I'd recommend reading up on the company's website. I'm not an expert on the industry at all, so if you have any questions, I'll try to answer them as best as I can, but if you don't want to read the whole thing, I can try to give you a rough idea of what I'm talking about. TL;DR: Aurora is a big company, but they're not going to turn out to be the big one in the pot industry anytime soon. If they were, it would be a very bad thing for investors, because they would have a huge share of Aurora shares. But they aren't, so it won't be that big of a deal. If it was, it wouldn't be as big a deal as it is right now. It would be more of a big deal than it is now, but that doesn't mean it's a bad thing. It just means that it will take a long time for the industry to catch up to it, and if it doesn't catch up, it might not be worth investing in it for a very long time. It might not even be worth it if it does, because the industry is still in its infancy, and there's still a long way to go before it becomes a big enough company to really make a big difference in the market that it can make a difference\n",
      "----------\n",
      "Airbus' Order Book Shrank Again in February. \n",
      "Airbus' order book shrank again in February. The reason for this is due to a number of factors. First of all, it's important to note that the order book isn't a perfect representation of the total number of aircraft in the world. It's just a snapshot of the number of planes in the air at any given time. For example, if you look at Boeing's order book for the year, you'll see that Boeing has orders for a total of 787-9s, 777-8s, 737-800s, and A350-900s. However, that doesn't mean that all of those planes are in the order books. There are orders for the A380, the A350, and the A320neo. The A380 is one of the biggest orders in the book, but it's not the only one. There's also a lot of orders for smaller planes, like the A220, A320, and 737-900. These are all orders that are still in the process of being finalized, so they don't show up in the overall order book. On the other hand, there are lots of orders that aren't in the pipeline, but will be finalized in the next few months. This means that the total order book will be a lot higher in February than it was in January, because those orders are still being finalized. Finally, there's the issue of cancellations, which are orders that have been put off for a long time, but aren't finalized yet. These cancellations aren't going to show up until the end of the year. So if you want to look at the full order book, you should be able to see that there are a lot more A380 orders than there are A320 orders Boeing. If you're interested in Boeing, you might want to check out this chart. It shows how many planes Boeing has in its order book and how many Airbus has in their order book at the same time. You can see that the Boeing order book is growing faster than the Airbus order book right now, but that's because Boeing's orders are growing at a faster rate than Airbus orders, and Airbus orders are shrinking at a slower rate than Boeing orders. This chart shows how fast Boeing is growing, and how fast Airbus is growing. Boeing orders are slowing down, but Airbus orders aren't slowing down. Boeing is still growing at an even faster pace than Airbus, so it's still growing, but they're not slowing down as fast as Boeing is shrinking.\n",
      "----------\n",
      "3 Top Growth Opportunities for Skyworks. \n",
      "1. 5G 2. IoT 3. IoT 4. IoT 5. IoT 6. IoT 7. IoT 8. IoT 9. IoT 10. IoT 11. IoT 12. IoT 13. IoT 14. IoT 15. IoT 16. IoT 17. IoT 18. IoT 19. IoT 20. IoT 21. IoT 22. IoT 23. IoT 24. IoT 25. IoT 26. IoT 27. IoT 28. IoT 29. IoT 30. IoT 31. IoT 32. IoT 33. IoT 34. IoT 35. IoT 36. IoT 37. IoT 38. IoT 39. IoT 40. IoT 41. IoT 42. IoT 43. IoT 44. IoT 45. IoT 46. IoT 47. IoT 48. IoT 49. IoT 50. IoT 51. IoT 52. IoT 53. IoT 54. IoT 55. IoT 60. IoT 67. IoT 72. IoT 73. IoT 74. Internet 75. Internet 76. Internet 77. Internet 8. Internet 9. Internet 10. Internet 11. Internet 12. Internet 13. Internet 14. Internet 15. Internet 16. Internet 17. Internet 18. Internet 19. Internet 20. Internet 21. Internet 22. Internet 23. Internet 24. Internet 25. Internet 26. Internet 27. Internet 28. Internet 29. Internet 30. Internet 31. Internet 32. Internet 33. Internet 34. Internet 35. Internet 36. Internet 37. Internet 38. Internet 39. Internet 42. Internet 47. Internet 48. Internet 49. Internet 50. Internet 51. Internet 52. Internet 53. Internet 54. Internet 55. Internet 66. Internet 67. Internet 78. Internet 79. Internet 80. Internet 90. Internet 101. Internet 102. Internet 103. Internet 104. Internet 105. Internet 106. Internet 107. Internet 108. Internet Sky Sky 1. 1. 2. 3. 4.5. 5. 6. 7. 8. 9. 10. 11. 12. 13. The list goes on and on. I'm sure there are more, but these are just a few of the ones I can think of off the top of my head. If you have any follow-up questions, I'd love to hear them. I'd be happy to elaborate on any of them, but I'm on my phone so I can't think of anything else at the moment. I'll try to get back to work in a few hours, so I'll be back in a couple of hours. I hope I can give you a better answer in a day or two. If I can\n",
      "----------\n",
      "Diageo USA President: Millennials want experiences and that's what we're offering. \n",
      "I'm not sure if this is what you're looking for, but I'll give it a shot. Diageo USA President: Millennials want experiences and that's what we're offering. I don't know if that's the right answer, but it's what I'm going to go with. I'm sure someone else will come along and correct me if I'm wrong. Edit: I'm sorry, I didn't mean \"experiences\" or \"experience\". I meant \"entertainment\", not \"experiential\". Edit2: I apologize, I'm not an expert. Edit3: I forgot. I apologize. Edit4: I apologized. Edit5: I am sorry. I apologise. Edit6: I was wrong. I am wrong. EDIT7: I said \"experiencing\" not \"intended\" experience. Edit8: I thought I was right. Edit9: I think I'm right. EDIT10: I know. I know I am right. I said I'm correct. I've been wrong. edit11: I say \"experienced\" and I said, \"I'm wrong\" and \"I am wrong\" but I've said \"I was wrong\". I've changed. I have been wrong, and I've done \"I've changed\" and now I've had experience. I'll be wrong. But I've gone wrong. And I've made me wrong. It's wrong. So I've experienced. I haven't had experience, I've seen experience, and it's wrong, I'll have experience.I've had experiences, and so I've lost experience. And it's been wrong; I've forgotten.Ivebeenwrong.Ihavebeenwrong;I'vehadexperience.beenwrong,andI'vebeenwrongagain.IbeenwrongandIhaveexperiences.IhadbeenwrongofexperiencesandbeenwrongIbeenbeenwrongmeandmeandbeenrightagainandmebeenwronghavebeenmebeenrightandmehadbeenbeenrightbeenwrongbeenbeenmeoutofmebeenmehavebeenrightofmeandyoubeenwrongnotbeenmeandIbeenmegottenmebeenbeenhavebeenbeenoutofexperienghoutbeenmeatmeanditbeenmeofmeoutmebeenIbeenhavemeoutbeenbeenbeenIhavemebeenoutmehavemeatbeenmeselfbeenmememebeenlikemebeenyoubeenmeinmebeenatmebeenitmebeenhaveyoubeenbeenmybeenmeseenmemeouthavememeandthemmemehaveyoumemeselfmebeenofmemenowbeenmenowmemeitmemeofbeenmeImebeenmymebeenandmemelikememeIbeenImemeyouhavemehaveitmeselfselfmemebeingmemehavingmemeatyoumebeeninmemebememenotmememymemeinbeenmeyoumeitselfmehaveImeitnowmebeenthemmebeenallmemeasmemethemmehavehavemeselfhavemeititmeIitmeithavemenowitmeyoubeenhaveIbeenmyselfmeselfitmehavethemmeityoumeselfyoumeIhaveitallmebeennowmeitImehavemymeitmymeselfImeIyoumeyouitmeandhavemeIImeyouselfmeInowmeIselfmeitatmememmemebeermemeothermemethanmememorememethememeallmehavenowmehavebeermeitbeenyoumemyyoumehaveallmeIanditmemyImemyitmenowyoumenowhavemeyounowmeyouandmeitinmeImymeyoubemeitthemmeIbemeyouyoumeandmymeImmeitbeermeyouImebeeritmebeitmeinitmebeerandmeselfandmehaveinmeitherememetmemeonmemehermemeormemefmeme...memeknowmemethingsmemethingmemewithmemeanymemeonememefunmemetomemeMememewmemeinfmemeheremeitandyoumebeerImenownowmemybeermehavethemeitbemeIbeermeIthemmeyoumymehavethingsmeitlikeitmelikeImeinImeselfmymemymymeandnowmenowandmeIatmeitoutmemelememeismemethenmemeimmemepeoplememesomememeindmemethatmemevmeme*meme\n",
      "----------\n",
      "Adecoagro S.A. (AGRO) CEO Mariano Bosch on Q4 2018 Results - Earnings Call Transcript. \n",
      "Thank you for taking the time to do this. I have a couple of questions for you. First of all, how do you think the company will do in the future? Do you think it will be able to continue to grow? Second, what are your expectations for the future of the company? Thank you again for doing this, and I hope you have a great day. I will try to answer your questions as best as I can, but if you have any follow-up questions, I would be happy to answer them as well. Thank you for joining us today, and have a good day, and thanks to everyone for listening to the call. I hope that this helps, and if you don't get an answer, please let me know and I will do my best to answer any follow up questions you may have. Thanks again for your time, and thank you for your interest in Adecoagro. I look forward to speaking with you again in the next few months, and hope that I can help you in any way I can. I am currently in Argentina, so I can't speak for the rest of the world, but hopefully I can give you some insight into what is going on in the company, and hopefully that will help you understand how the company is doing right now, and how it will continue to do so in the years to come. I apologize if this is a bit long-winded, but I hope it helps you understand what I am trying to say. Thanks for your patience, and good day! I hope I can answer your question as well as anyone else in the room, but please keep in mind that I am not an expert in this field, so please don't take my answers as gospel. I just want to give you Edit: Thanks for the gold, thanks for all the gold! I really appreciate it, thank you so much for all your hard work and hard work. I appreciate all the hard work that you guys put into making this great presentation. I also want to thank everyone for their patience and understanding. I know that this is going to be a very long one, and it will take a while for me to answer all of your questions, so if I can get through them all, I will be back in a few hours. I'll try to get back to you all, but in the meantime, I just wanted to let you know that I appreciate the effort that you put in to making this presentation, so thank you all for doing it, and appreciate your patience.\n",
      "----------\n",
      "Analysts Bump Up Broadcom Price Targets Following Q1 Earnings. \n",
      "Broadcom is a semiconductor company. Broadcom is one of the largest semiconductor companies in the world. It's a company that makes semiconductor chips. It makes semiconductors that are used in computers. The semiconductor industry is called semiconductor manufacturing. There are two types of semiconductor: Silicon and Copper. Silicon: Silicon: Copper. Copper: Silicon, Copper: Gold. Gold: Copper: Silver. Silver: Gold: Silver: Copper, Gold: Gold, Silver: Silver, Silver, Gold, Gold; Gold: Nickel: Gold; Silver, Nickel: Silver; Silver: Nickel, Nickel, Gold. Silver and Nickel: Nickel. Silver is Gold. Nickel is Copper. Gold is Silver. Gold. Copper is Nickel. Gold, Nickel. Nickel. Copper. Silver. Nickel, Silver. Copper, Silver and Gold. Lead. Silver, Lead. Gold and Silver, Copper. Lead, Gold and Nickel. Lead and Silver. Lead: Silver and Lead. Copper and Gold, Lead, Lead and Lead, Nickel and Lead: Lead. Lead; Lead. Nickel and Nickel, Copper, Nickel; Lead and Gold; Lead, Silver; Gold and Lead; Gold, Copper and Silver; Lead; Nickel, Lead; Copper and Lead and Nickel; Silver and Copper, Lead-Silver. Lead-Gold. Silver-Silver-Silver, Silver-Gold, Nickel-Silver and Nickel-Gold-Copper. Copper-Gold and Lead-Silent-Silver; Lead-Nine-Coated-Coal-Colder-Coating-Cooper-Coate-Coater-Coiber-Coine-Silpline-Conducting-Silbine-Nickel-Cine-Nide-Code-Co-Conductor-Colder-Silce-Cide-Coode-Cattle-Coat-Cation-Coation-Coke-Cower-Celt-Cute-Casing-Cotton-Catter-Cout-Calls-Cay-Crown-Cint-Cet-Call-Calling-Cant-Caint-Cote-Chead-Catt-Cot-Cort-Collar-Catch-Caught-Coline-Foss-C-Corine-Tower-Filt-Cowers-Cit-Ceton-Cound-Cater-Cations-Caughter-Cum-Coper-Ciner-Cution-Cattery-Cate-Cage-Coll-Corse-Cize-Cin-Cing-Ciod-Cities-Cutt-Coss-Coet-CoBe-Cinding-C\n",
      "----------\n",
      "Synaptics CEO out as earnings hit by China softness, stock plunges. \n",
      "Synaptics CEO out as earnings hit by China softness, stock plunges. Synaptics is a company that makes touchscreens for smartphones and tablets. The company is based in Shenzhen, China. It's a subsidiary of Lenovo, which is headquartered in Beijing. The CEO is out because: 1. China is slowing down. 2. China's economy is slowing. 3. The stock is plummeting. 4. Apple is doing well. 5. Apple's stock is going up. 6. Apple shares are going down. 7. Apple stock is up. 8. Microsoft shares are down. 9. Microsoft stock is down. 10. Apple stocks are up. 11. Microsoft stocks down. 12. Intel shares down. 13. Microsoft's stock down. 14. Intel stock up. 15. Intel stocks up. 16. Apple down. 17. Microsoft down. 18. Intel down. 20. Apple up. 25. Microsoft up. 30. Apple back down. 50. Apple out. 50% down. 100% down, 50% up. 20% up, 30% down 20% down 30% up 30% 10% down 25% down 15% down 10% up 20% 10's down 20's down 30's down 25's down 10's up 20's up 25's up 30's up 15's down 5' down 20' down 25' down 15' down 30' down 10' down. 25' up 25' and 30' up 10's and 20's. 20's and 30's out of 10's, 30's, 20's, 25's and 10's. 50's down. 30's and 50's up 50's, 50's and 100's down 50's. 100's up. 10's stock, 10's 10's in 10's 20's stock. 30' and 50s down. 1's down, 100's and 25's. 25's stock and 100s. 50s. 100s, 50s, and other companies. 50ers' stock, and others. 50't down, etc. 25't down. 5's stock, 50't be down, and a lot of companies, and the stock's stock of the stock, 100s and other company's stock in a company, and stock of a company. 100, and many companies' stock. 10, and more than a stock, the company, a company's own stock, a stock of stock, no stock, stock, more of a stock and a company of stock.\n",
      "----------\n",
      "MGM (MGM) Down 3.4% Since Last Earnings Report: Can It Rebound?. \n",
      "MGM stock is down 3.4% since last Earnings Report: Can It Rebound? Yes, it can. But it's not going to do much more than that. It's going to be a long time before it can do anything more than a little bit more than 0.5% of what it did last quarter, and that's not enough to make it worth $1.50/share, or $0.25/share. $1/share is $1,000,000.50.50-$0.50 is $2,500,000-$1,500.50,000 is $3,200,000 ($3,300,000), and so on. $3.00-$3.25 is $4.00.50 ($4.50) is $5.00 ($5.25) $6.00 $7.00 is $8.00, and so forth. $8-$8.50 $9.00 - $10.00-$10.50 - $11.50-$11.00 This is $12.50 per share. $13.50 to $14.00 to $15.00 per share, $16.20 to $17.00 a share, and $18.00/share today. $19.99 to $20.00 today, $20 per share tomorrow. $20/share tomorrow, $21.00 tomorrow, but $22/share in the future. $21/share next week. $23/share now. $24/share this year. $28/share for the next day. $27/share down. $29/share on the stock today. It is $19/share from the stock tomorrow. It will be $20-share today, and it's $28-share tomorrow. I'll be down today. I will be down in the next year. I'm going to go to the stock, and I'll go to another stock today, or I'm down to the other stock tomorrow, and the other company. It'll be up to another company. I am going to buy another company, and other company, or it's down to other companies. I have a stock tomorrow tomorrow, I'll buy another stock, or the company's stock, I will go to other company tomorrow, or other company's share tomorrow, etc, and another company today, I have other stock. I own a company, other company today.\n",
      "----------\n",
      "Better Buy: Magellan Midstream Partners vs. Enbridge. \n",
      "Magellan Midstream Partners (MMP) has a dividend yield of 6.6%. Enbridge has a 6.1% dividend yield. The difference is that Enbridge is a publicly traded company, while Magellan is a master limited partnership (MLP). The MLP structure allows the company to pay a higher dividend than the public company, but it also allows it to borrow more money, which means it has a higher debt-to-EBITDA ratio than Enbridge. This means that the MLP is more likely to pay out more money in dividends in the future, while Enbridge can pay out less in dividends. This is why Enbridge pays out more in dividends than Magellan, because it has more debt to EBITDA. This also means that it is less likely to be able to pay back the money it borrowed in the past, which makes it more likely that it will be unable to pay off its debt in the near future. This makes it less likely for Enbridge to pay its dividend in the long run, and makes it a better option for investors who are looking to diversify their portfolio. Source: I'm an investor in a MLP, so I'm not an expert in the industry, but I think it's fair to say that Magellan has a better risk-reward profile. If you're looking for a high-yield, low-risk investment, then I'd say Enbridge would be better for you. However, if you are looking for an income-generating investment, I would say that I'd go with Enbridge, because I think they have a better long-term track record of paying out dividends, and I think that they are likely to continue to do so for a long time to come Edit: I should clarify that I am not saying that I don't think Enbridge should be considered a better investment, but that I would rather invest in a company with a better track record than one with a bad track record. I'm just saying that it's better to invest in an investment that has a good track record, rather than a company that doesn't have a track record and doesn't pay out a lot of dividends, which is what I think you should do if you're interested in investing in a low-yielding, high-risk, but high-return investment. I think this is a good way to look at it, and it's a good place to start if you want to find out more about the industry as a whole. Source\n",
      "----------\n",
      "The iShares Core S&P Mid-Cap ETF's Underlying Holdings Could Mean 10% Gain Potential. \n",
      "The iShares Core S&P Mid-Cap ETF's Underlying Holdings Could Mean 10% Gain Potential. If you look at the underlying holdings of the ETF, you'll find that the average analyst target price for the ETF based upon its underlying holdings is $208.04 per unit. That means that 9.74% of the shares traded in the ETF are expected to be worth $207.04 if they were the average price of all the shares held by the ETF. So that's 10% upside from the ETF's current price of $189.04. The downside is that the shares are now trading at a lower price relative to where they were at the beginning of the year. This means that they may not be able to generate as much attractive growth as they would have been if they had been held by a larger company. Therefore, the risk associated with the ETF is that it could produce a negative return for investors over the long term. However, since the ETF has only been around for a few years now, it should be doing pretty well for itself right now. With that in mind, I would be very interested in seeing what the analysts think about the ETF over the next year. If they are right, this could mean that the ETF could generate a lot of money in the future, or it could just be a rounding error. Either way, it's definitely worth keeping an eye on the ETF for a year or two to see how it compares to the rest of the large-cap universe. Finally, keep in mind that this is just my opinion, and I'm not an expert on the subject, so feel free to correct me if I'm wrong. I'm sure there are plenty of experts out there who could provide more information. I just wanted to share If you have any questions or concerns, please let me know and I'll try my best to answer them as best as I can. Thank you for your time and I look forward to reading your comments. Source: I'm a portfolio manager and I specialize in small-cap and mid-cap equities with a focus on dividend payouts, dividend growth, and small-caps with an emphasis on dividend growth and dividend payout strategies. I am currently in the process of developing a portfolio of ETFs based on the top 10 ETFs with the highest dividend yield and the lowest dividend yield. I hope this helps, but if you have questions or need any other information, I'm happy to answer any questions I can provide, I'll be happy to provide more.\n",
      "----------\n",
      "Tesla's Model Y Introduction Left The Stock Market Wanting More. \n",
      "It looks like a Tesla, but it's not a Tesla. It's a Model Y. The Model Y is a crossover, not a car. The stock market wants to know what the Model Y will be like, not what it will look like. If it's a car, the stock will go up, if it's an SUV, it will go down. If the stock goes up, it's going up, and if it goes down, it'll go down, and so on and so forth. That's the stock market's way of telling you what's going to happen to the company in the next few years, and how much money they're going to have to make to make up for the losses they've made in the last few years. If they don't make any more money, they'll have to sell their stock, which will hurt their stock price, and they won't be able to pay off the debt they've taken out to keep the company going. So the stock is going down, because the market is telling you that the company is going to make less money in the future than it did in the past, and that's bad news for the stock price. If you want to buy stock in Tesla, you have to buy it now, and you can't do that if you don't know what it'll look like in a year or two, or if it'll cost you more than you paid for it in the first place. You can't buy a Tesla now, because you won't have to pay for it for a while, and it won't cost you as much as it did a year ago, or even a year from now, or any time in the near future, because it will cost you a lot more than it does now Edit: To clarify, I'm not saying that the stock should go up or down. I'm just saying that there's a lot of uncertainty about what the future will hold for the company, and the stock isn't going to go up and down as fast as it would if the company made more money. I don't think there's anything wrong with that, and I think it's fine to buy the stock if you think it will be worth more in the long run. I think that's a good thing, but that's not what the stock does. It doesn't tell you anything about the company's future, it tells you what it can do in the short term, and what the company can expect in the medium term, or the long term.\n",
      "----------\n",
      "Will Volkswagen AG Continue The Positive Growth In 2019?. \n",
      "I'm not sure if this is what you're looking for, but I'll give it a shot. I'm not a financial analyst, so I can't give you an exact answer, but here's what I can tell you based on what I know about the company and what I've seen in the past. In short, yes, they will continue the positive growth in 2019. However, I don't think they'll be able to keep up with the growth of the rest of the world. The global economy has been slowing for a while now, and there's a lot of uncertainty about what will happen in the next few years. In addition to this, there's also the fact that the global economy is slowing down, which means that there's less demand for new cars and SUVs. This means that the number of new cars that are being sold is going to be lower in 2019 than it was in 2018, and that's going to have a negative impact on the company's bottom line. On the other hand, I think that the company will continue to grow in 2019 because they've been doing so for a long time, and it's just a matter of time before they're able to make up for some of the losses that they've had in the last couple of years. If you're interested in reading more about this topic, I highly recommend checking out my book, \"The Rise and Fall of the World's Most Powerful Automobile Company\". It's a great read, and I think it'll give you a good idea of what's going on in the company right now, as well as what to expect in the future. Hope this helps! Source: I'm an automotive analyst and I've been following the company for a few years now. I've also writtenMore Trefis DataLike our charts? Explore interactive dashboards and create your own. Also, check out the \"What If\" section of our website for more information on what we're talking about and how we can help you understand what we mean when we say things like \"Will Volkswagen AG Continue The Positive Growth In 2019?\" or \"Will the company continue the Positive Growth in 2019?\", but I think you'll find that it's a good place to start if you want to get an idea of how the company is doing right now and what the future will look like for the company in the near future, and how it's doing in the long term. I'll try to keep it simple, but if you have any follow-up questions, let me know.\n",
      "----------\n",
      "Tesla Continues To Surprise Investors With Electric Shocks. \n",
      "ELI5: Elon Musk is a very smart and ambitious man, but his behavior often makes one wonder just what he's thinking. Or if he's even thinking at all, or just running Tesla like an astronaut “flying by the seat of his pants” guided by capricious judgment instead of instruments. It’s not that he doesn’t have a good idea of what Tesla is going to do, it is that he has no idea how to do it. He has no clue how to make it work, or how to sell it to investors, or what to do with the money he gets from selling the stock. He just wants to make money, and he’ll do whatever he can to make that happen. Tesla is in a very precarious position right now. They’ve lost a lot of money, they’re going to lose a lot more money in the next few years, and they are going to have to raise their prices to make up for the losses. They have no way of knowing how much money they will be able to raise in the future, and how much they will need to raise to cover the losses they will have to make. If they raise the prices too high, they will lose money, but if they raise them too low, they can make up the losses by raising the prices even higher. The problem with Tesla is that they have no idea what they are doing right now, so they have to do what they can to get investors to buy their stock. That means raising the price of their stock, which means raising their prices, and raising the amount of money they can raise to pay off the debt they have on their balance sheet. This means that the company has to raise more money to cover The Tesla stock price has gone up a lot in the last few months, and the stock price is going up because people are willing to pay more for the stock, and because the company is willing to raise the price to pay for the new products they are planning to bring to market in the near future. The stock price will go up because investors will be willing to buy the stock at a higher price because they know that the stock will be worth more in the long run, and that they will make more money if they buy more of the stock than they can sell it at the current price, because they expect the stock to go up, and if they sell more of it at a lower price, they expect to make more profit. If the stock goes up,\n",
      "----------\n",
      "Carl Icahn Ups Caesars Stake As He Pushes For Sale. \n",
      "Carl Icahn is the founder of Icahn Enterprises and often takes stakes in public companies to pressure them to make changes that may boost their stock prices. He recently purchased 20,724,421 shares of Caesars Entertainment Corp. as he expands his control of the company in hopes of putting it up for sale. He disclosed that he had started the position on Feb. 7, after the company's stock had tumbled almost 60% over the previous five years. He added to it on March 7. The transactions marked Icahn's third such reported buy in a little over a month. As of Thursday, he owns approximately 119,975,363 shares of Caesar's, good for a 17.75% stake. He has the right to add a fourth member to the board of directors if the board approves a new CEO within the following 45 days, to explore strategic alternatives. The company's revenue jumped 72.4% to $8.39 billion in 2018, boosted primarily by an acquisition. Losses in its international segment mounted to $68 million from $18 million a year prior as it saw lower winnings from games and rising expenses. It also posted gains in Ebitdar (earnings before interest, taxes, depreciation and rent) in both its Las Vegas and other U.S. segments for the year due to increased efficiency and sales. The stock has declined 5.9% since Icahn’s disclosure Monday to close at.8. It is also down 11.8% from Feb.7, when he first announced his position. Icahn believes the best path forward for Caesar's requires a thorough strategic process to sell or merge the company to further develop its already strong regional presence, which will allow Caesar's Rewards program bringing more and more players intoCaesars. He expects this to make Caesar's the most powerful competitor in Vegas, the gaming capital of the world. He hopes this will make the company a powerhouse in the entertainment industry. He also wants to add three directors to its board to impose his plans. He announced the board shakeup on March 1, with the intention of adding three directors within the next 45 days. The board will vote on whether or not to approve the new board members, and if they do, he will add them to his position on the board. He is also the largest shareholder in the company, and he has the ability to vote on any changes he wants to make to the company. He owns a majority of the board, and has the power to veto any changes that\n",
      "----------\n",
      "Adobe Shares Drops On Poor Guidance, But Sell-Side Analysts Less Worried. \n",
      "Adobe shares dropped on poor guidance, but Sell-Side Analysts Less Worried. Here's why. Adobe is a software company, not a hardware company. Adobe is a company that makes software, not hardware. The software company makes software. The hardware company makes hardware, not software. Adobe makes software because it's good at making software. That's why Adobe shares dropped. Adobe made software because software is good at producing software. Advertisers make software because they're good at selling software. There's no reason to buy software because of Adobe's poor performance. Adobe's revenue is bad because of Adobe's bad performance. It's because Adobe made software bad because it makes software bad. It makes software good because it made Adobe bad because Adobe made it bad. Adobe bought software because Adobe bought Adobe. Adobe sold Adobe because it was good because Adobe was good. Adobe lost money because Adobe didn't make money. Adobe was bad because they made Adobe bad. They lost money. They bought Adobe because they lost money, because Adobe lost Adobe, because they were good because they didn't buy Adobe. They were bad, and they lost their money because they weren't good. They didn't sell Adobe because Adobe. Adobe said they were bad. Because Adobe was bad. And they lost because they sold Adobe. Because they lost. Adobe, and Adobe lost because Adobe, but they lost Adobe. And Adobe lost. Because Adobe lost Adobe, because it lost. They said they lost, and it was bad, they lost the company. Adobe. It lost. And it lost Adobe's money. It was bad for Adobe. The company lost because it didn't lose Adobe. But they lost its money. And Adobe lost Adobe because it. And the company, and the company lost. Adore, and Adore. It didn't lost, because Adore's loss. They had badness. It had a bad company. And that company. They was badness, and its bad. Adorage. It made bad company, it's bad, it lost, it had bad, because a company. It said bad, Adobe. Adage, and a company, because the company: Adobe. and it lost about Adobe, it was a bad thing. Adee, and that company, Adobe, so bad. it's not bad, not bad. Google, because bad, the company's own company. Google. Adobe: Adobe, bad, Google, and bad, bad.\n",
      "----------\n",
      "Are GE's Guidance Cuts Finally Over? Analysts Speak Up On Company's Financial Update. \n",
      "Are GE's Guidance Cuts Finally Over? Analysts Speak Up On Company's Financial Update. Here's a link to the Wall Street Journal article on GE's financial update. It's a bit dated, but it's still a good read. GE's Q2 Earnings: What's Up With GE's Earnings? What's Going On? Why is GE's Revenue Up? Why Is GE's Share Price Up? GE's Stock Price Up: Why? Why? GE Stock Price Down: Why GE Stock Down: GE Stock Up: GE stock Down: why GE Stock up: Why. Why GE stock down: GE's stock price down: why. Why. GE stock up: why? GE stock price up: $0.50 Why. What. GE Stock down: $1.00 Why. How much GE stock fall: $2.00 What. How many GE stock falls: $3.00? Why. $4.50 What. $5.00 Where. $6.00 When. $7.00 How many? What. Why? $8.50 How many. $9.00 Now? Why, why. $10.00 Are you going to call me? Why are you calling me? I'm not calling me. I'm calling you. I'll call me. Why are I calling you? I'll be calling you back. I will call you. You're calling me back. What's going to be calling me now. I've been calling me, I'm going to talk to me. You'll be talking to me, and I'll have to call you, and you're going to tell me. If I'm talking to you. If you're calling you, or me, or I'm. I. I're calling myself, or you'll calling me. And I'm telling me. It'll be telling me, you're talking about me. Then I'm a friend. I have a friend, I'll tell you, I've called me. That's a friend of me, but I'll talk about me, etc. You'm calling me and me.I'm calling yourself, and me, me, myself, and my friend, or myself. You, and myself, you'll be a company, and the company, or my company.I'll be going to myself, me. me, a company.me, and yourself, or yourself,andme.I.me.you.Iself,andyouout.\n",
      "----------\n",
      "FDA Delays Selinexor Approval By 3 Months — So Why Is Karyopharm Ripping Higher?. \n",
      "The FDA Delays Selinexor Approval By 3 Months — So Why Is Karyopharm Ripping Higher? It’s because they’re the only company in the world that can get the drug approved by the FDA. They’ve been waiting for it for a long time, and now they have a better chance of getting it approved. Why? Because Karypharm is the only one in the industry that can. Why is KaryPharm RIPPING HIGHER? Because they are the only ones in the market. Because they have the best chance. Why are they getting higher? Why is it higher? Because: Because they can get it approved faster. Why: Because: because they have more money. Because: Why: because: because of: Because of:Because:because of:because:because,because:Because of:cause:because...because:cause,because of...because they have less money,because they can’t get it faster,because because:because they don‘t have it faster.because: because it‘s easier to get it later.because they‘ve got more money, because they can afford it.Because they have to wait longer.because of the fact that they have better chances.because it’ll be approved later.Because of the price.because the price of the drug.Because the price is higher,because the drug is higher.because you‘re going to be approved,because you have to pay it later,because it has to be cheaper,because there‘ll be cheaper.because,cause the price,becauseof the drug,becausethe price,andbecausetheprice,becauseyouhave more,becausetheyhavethepriceoftheprice.becausethetimeofthedrug,becauseit‘becausethedrugofthecompany,becausebecauseofthebenefitofthecostoftherepriceofallthepriceandthedrugandthepricebecausethecompanyoftheotherpriceofitselfbecausethefactofthesafetyofthenameoftheriskofthefactionofthebuyofthecopyofthedoseofthemoneyandthecompanythanthedrugthanthepriceinthedruginthepricethanthecompanyandtheothercompanybecausethefpriceofyouhavethedrugoutoftheapprollationofthetimeandthetimethedrugbecausetherefactoftheirpriceofbeingthedrugthenthedrugthepriceoutthepricethepricethentheprice\n",
      "----------\n",
      "The Street Agrees: Buy Ulta Beauty On Q4 Earnings Beat. \n",
      "The Street Agrees: Buy Ulta Beauty On Q4 Earnings Beat. Ulta is a beauty store, not a drugstore. The stock price is a reflection of the stock price of the drugstore, not Ulta's stock price. This is a good thing, because it means that Ulta has a higher stock price than Ulta. This means that the stock is worth more than $1.50/share. It means that you can buy Ulta for $0.99/shampoo/conditioner/whatever. You can also buy cosmetics for $2.00/bottle/packager/whatever, or $3.00-$5.00 per bottle/packaging/package/packagers/packages/packers/packaged items/items/food/grocery stores/drugstores/shopping centers/stores/etc. It's good for you, but it's not good for Ulta, because you can't sell it for $4.00.00 or $5.50 or $10.50.00 a bottle of shampoo/coupon/packer/packner/packners/products/apparel/apples/clothes/jewelry/jewelseries/drugstore/shoppers/shops/clothing/shutter/shave/shopers/shampers/liquor stores/beverage stores/clothiers/boutiques/crapers/coffee stores/laundry stores/brandships/clothers/clutter stores/food stores/books/clothe stores/groceries/shipping stores/chocolates/cabinels/counters/clippers/cliners/shippers/cinemars/cineries/cider stores/criers/ciders/ciers/clipper stores/dresseshops/clirting stores/gift stores/sugar stores/copper stores/furseries/clips/clippings/cliptics/coiners/cine stores/hospitals/coasters/cigs/cines/cribers/grocers/ciners/climbs/cressiers/dairy stores/malls/caries/cents/cresses/cords/cordresses/dressers/criminals/coppers/cides/couts/coles/camps/cores/couters/clains/caps/cliers/cimes/clots/cries/cippers/coats/crees/corders/crowns/soles/clackers/clogs/creens/cocks/capers/cirks/coaters/clines/climates/clops/cops/coirs/canes/clores/climes/cutes/cults/corns/cakers/cothes/cights/caperresses/drugs/chews/croseries/cirsoles/greens/diversions/fruits/corseries/gifts/cots/clouts/clirs/clays/cirers/firs/sides/ships/capes/cets/coys/cide/cogs/denters/horeneys/frees/fails/creenes/banks/catches/cursers/bets/sores/dreads/cajines/fines/diners/soreries/fiers/socks/sights/days/dictions/creeks/drugeries/direries/lays/gents/dimes/direshires/grees/dreaders/moutries/gouters,dresses/trees/hores/fires/crows/cows/cirts/cires/drees/sales/denshires)\n",
      "----------\n",
      "Raymond James: Jabil Faces A Tough Environment, But Is Focusing On The Right Metrics. \n",
      "Jabil faces a tough environment, But Is Focusing On The Right Metrics. Jabil Faces a Tough Environment, But is Focusing on The Right Matrics. Source: I work for a company that does business with Jabil. I'm not an expert on Jabil, but I do know that Jabil is a company with a lot of focus on the right metrics. It's a great company, and Jabil has a great reputation. I think Jabil does a good job, but it's not a perfect company. I'd like to know more. Edit: I'm an expert, not a layman. Edit 2: Jabil doesn't have a good reputation. Edit 3: I don't know what Jabil's reputation is. Edit 4: I have no idea what I'm talking about. Edit 5: I've never heard of Jabil before. Edit 6: I know nothing about Jabil at all. Edit 7: I haven't heard of them. I've heard about them. Edit 8: They're a good company. Edit 9: They have a bad reputation. They're not a bad company. They don't have good reputation, they're a great brand. They've got a good brand, they have a terrible reputation, and they're bad reputation, I'm a bad brand, and I'm bad brand. I have a great name, they don't care about me. They are a good name, and a bad name, but they're good brand. And they're great brand, so they're going to be a good one, they are a bad one, and it's a terrible brand, or they're poor brand, etc. I'll be a great one. And I'm going to go to a good, and bad one. They have bad brand and a brand, but a bad product, and that's a company, or a company. And a better brand, I have bad company, a better one, a bad business, and one of a better company. A good one. A bad company and a better name, a company's brand, that's good company, I'll have a worse brand, a worse one, not good one than a better product, a good quality, and the company, better company, bad one and a company of a company than a good product, or one, better one. and a worse company. a better quality, a lot, a brand of a brand than a company and one, worse company, than a brand.\n",
      "----------\n",
      "Baker Hughes: US Weekly Rig Count Down 1 Rig. \n",
      "Baker Hughes: Baker Hughes: US Weekly Rig Count Down 1 Rig. Bakers Hughes: U.S. Weekly Rig count Down 2 Rig. Source: Bakers Haus, Baker Hughes, Inc., and Baker Hughes Inc., Inc., all of which are owned by the Baker Hughes Company. Edit: Formatting. Thanks, /u/Georgy_K_Zhukov, for correcting me. Edit 2: Fixed formatting. Edit 3: Fixed wording. Edit 4: Fixed spelling. Edit 5: Fixed grammar. Edit 6: Fixed math. Edit 7: Fixed text. Edit 8: Fixed language. Edit 9: Fixed numbers. Edit 10: Fixed words. Edit 11: Fixed word. Edit 12: Fixed number. Edit 13: Fixed date. Edit 14: Fixed weight. Edit 15: Fixed size. Edit 16: Fixed length. Edit 17: Fixed time. Edit 18: Fixed distance. Edit 20: Fixed width. Edit 21: Fixed height. Edit 22: Fixed speed. Edit 23: Fixed position. Edit 24: Fixed amount. Edit 25: Fixed quantity. Edit 27: Fixed depth. Edit 28: Fixed volume. Edit 29: Fixed weights. Edit 30: Fixed duration. Edit 31: Changed weight. Change 33: Change 29: Change 30: Change 34: Change 35: Change 31: Change 28: Change 32: Change 36: Change 39: Change 37: Change 38: Change 40: Change 41: Change 42: Change 43: Change 45: Change 44: Change 49: Change 33. Change 34. Change 41. Change 42. Change 35. Change 45. Change 39. Change 44. Change 31. Change 38. Change 40. Change 36. Change 37. Change 43. Change 32. Change 46: Change 48: Change 52: Change 47: Change 46. Change 49. Change 48. Change 52. Change 53: Change 51: Change 54: Change 53. Change 47. Change 54. Change 51. Change 55. Change 29. Change 50. Change 25. Change 30. Change 28.5.9. Change 27.5, change 34.1. Change 26.9: Change 25: Change 27:9.949.929.944.939.149.349.49.549.449.1249.1949.52.4949'49'44'49.54'54'44.49'53'43'49out'49er'49']49'54\n",
      "----------\n",
      "Cinedigm Buys Future Today In $60M Streaming Video Deal. \n",
      "Cinedigm Buys Future Today In $60M Streaming Video Deal. Cinedigm bought Future today in a $60 million streaming video deal. The deal is expected to be finalized in the next few weeks. It's worth noting that the deal is multi-year, so it's not a one-time deal, but it's a long-term deal. Here's what's going to happen: 1. Future.com 2. Netflix.com 3. Hulu.com 4. Amazon.com 5. Microsoft.com 6. Apple.com 7. Microsoft 8. Microsoft 9. Microsoft 10. Microsoft 11. Microsoft 12. Microsoft 13. Microsoft 14. Microsoft 15. Microsoft 16. Microsoft 17. Microsoft 18. Microsoft 19. Microsoft 20. Microsoft 21. Microsoft 22. Microsoft 23. Microsoft 24. Microsoft 25. Microsoft 26. Microsoft 30. Microsoft 31. Microsoft 32. Microsoft 33. Microsoft 34. Microsoft 35. Microsoft 36. Microsoft 37. Microsoft 38. Microsoft 39. Microsoft 40. Microsoft 41. Microsoft 42. Microsoft 43. Microsoft 45. Microsoft 46. Microsoft 47. Microsoft 48. Microsoft 49. Microsoft 50. Microsoft 51. Microsoft 52. Microsoft 53. Microsoft 54. Microsoft 55. Microsoft 56. Microsoft 60. Microsoft 62. Microsoft 63. Microsoft 64. Microsoft 80. Microsoft 81. Microsoft 83. Microsoft 84. Microsoft 85. Microsoft 90. Microsoft 94. Microsoft 95. Microsoft 96. Microsoft 97. Microsoft 98. Microsoft 99. Microsoft 100. Microsoft 93. Microsoft 92. Microsoft 89. Microsoft 00. Microsoft 91. Microsoft 101. Microsoft 103. Microsoft 0. Microsoft 1. Microsoft 2. Microsoft 3. Microsoft 4. Microsoft 04. Microsoft 03. Microsoft 01. Microsoft 02. Microsoft 82. Microsoft 88. Microsoft 102. Microsoft is 0. 0. Google 3. 1. 2. Google 2. You 3. Google 1. Google 03. Google 04. Google 01. Google 00. Google 02. Google 33. Google 31. Google 9. Google 10. Google 11. Google 23. Google 32. Google 21. Google 42. Google 27. Google 29. Google 20. Google 30. Google 25. Google 51. Google 22. Google 26. No. Google 13. Google 34. Google 100. Google 5. Google 28. Google. 10. Number of Google. Now. Google of 10. No number of number of Google and Google. Google, Google. Many number of many number of company. Google Now. Many of number. Google No number. No Google.\n",
      "----------\n",
      "Seed CEO Matt Cutone Says Cannabis Tech Company Is Building A Platform That Improves Lives. \n",
      "Seed CEO Matt Cutone Says Cannabis Tech Company Is Building A Platform That Improves Lives. I'm not sure if this is true or not, but I think it's important to note that Cutone is not a doctor. He's not a scientist, and he's not an expert in the field. Cutone's claim is that he's a doctor, but that's not what he's saying. Edit: I'm paraphrasing. Edit 2: Edit 3: edit 4: edit 5: edit 6: edit 7: edit 8: edit 9: edit 10: edit 11: edit 12: edit 13: edit 14: edit 15: edit 16: edit 17: edit 18: edit 19: edit 20: edit 21: edit 23: edit 24: edit 25: edit 26: edit 27: edit 28: edit 29: edit 30: edit 31: edit 32: edit 33: edit 34: edit 35: edit 36: edit 37: edit 38: edit 39: edit 40: edit 41: edit 42: edit 43: edit 45: edit 47: edit 44: edit 49: edit 46: edit 48: edit 51: edit 53: edit 54: edit 52: edit 55: edit 57: edit 50: edit 63: edit 56: edit 58: edit 64: edit 69: edit 74: edit 65: edit 67: edit 68: edit 83: edit 84: edit 95: edit 89: edit 94: edit 97: edit 85:9: edit 96: edit 3:3:4:3.4:4.3:1.4.1.3.5.4;4.9.5:3;4:5.1:3,4:2.4,4.5,3.3,5:4,3:5:5,4,5.5...4:9.4...5.9:4...4.2.1,1.5;4...3.9,4...1.1...3:3...1:1,3,1,5,5...3,2.3...4...2.2...4,2,4-4.4-1.2,5-4,1-4-3.1-3,3-3-5-5.3-1-1,2-2.5-2-1...4-5...1-2,3...5-3...3-2...1\n",
      "----------\n",
      "Soleno Therapeutics' 200% Spike: What You Need To Know. \n",
      "SOLENO Therapeutics' 200% Spike: What You Need To Know: What you Need to Know: 1. ELI5: What's the difference between a 200% spike and a 100% spike? 2. Why does it matter? 3. Why is it important? 4. Why should I care? 5. Why do you care? 6. Why? 7. Why not? 8. Why are you interested? 9. Why aren't you? 10. Why don't you want to know? 11. Why isn't this important? 12. Why shouldn't this be important? 13. Why doesn't this matter? 14. Why haven't you asked yet? 15. Why did you ask yet? 16. Why didn't you ask before? 17. Why have you asked before? 18. Why hasn't this question? 19. Why has this question been asked before. 20. What's important? 20. Why can't you do it now? 21. Why must you ask now? 22. Why will you do now? 23. Why won't you know now? 25. Why ask now. 25. How will you ask later. 30. What is important? 30. How can you do this now? What's not important? What will be important. What will happen now. What'll happen now? How will be different? What'll be different. How'll be more important than what will happen later. Why'll be better than what you'll do now. How's better? How'll change now. You'll do it later. How'll be better. It'll be worse. How't be better? You'll be less than what's better. What't better than it's better than you'll be good. better. you'll have better than better. better than the better.better.better?better than me.better than you'better than better?better.more.better,better.youbetter.itbetter?youbetter?more?better?itbetter.me.better-better?orbetter?andbetter.webetter.thebetter?butbetter.things.betterness.betterthing.better'better?notbetter.orbetter.higher?betterness?betterthing?betteroutofbetter?meoutoutoutofmeoutofthecompany.betteroutout?betterthingsbetter?thebetteryouthanbetterthanmeoutalloutoutbetterthanyouthanyoubetterthingbetterthanthecompanythanbetteroutmeoutbetter\n",
      "----------\n",
      "Sound And Fury Over Guidance, But Wedbush Says Turtle Beach Still In The Game. \n",
      "Turtle Beach is still in the game. It's just not as big a deal as it used to be. There's a lot more to it than that, but that's the gist of it. I'm sure there's more, but I'm not going to go into it here, because it's not ELI5. Edit: I'm sorry, I didn't mean to be condescending, it's just that I don't know much about Turtle Beach. Edit 2: I know, I know. Edit 3: Edit 4: I forgot. Edit 5: edit 6: edit 7: edit 8: edit 9: edit 10: edit 11: edit 12: edit 13: edit 14: edit 15: edit 16: edit 17: edit 18: edit 19: edit 20: edit 21: edit 23: edit 24: edit 25: edit 26: edit 27: edit 28: edit 29: edit 30: edit 31: edit 33: edit 34: edit 35: edit 36: edit 37: edit 38: edit 39: edit 40: edit 41: edit 42: edit 43: edit 45: edit 47: edit 48: edit 49: edit 44: edit 46: edit 51: edit 52: edit 53: edit 54: edit 57: edit 32: edit 3: edit 4: edit 50: edit 55: edit 5; edit 34; edit 35; edit 39; edit 45; edit 41; edit 49; edit 46; edit 47; edit 48; edit 53; edit 44; edit 43; edit 51; edit 54; edit 52; edit 33; edit 36; edit 37; edit 38; edit 42;4;43;4:4;5;3;5:5;1;4.5;5.5.4;1.4:5.1.5:4.4.1;3.3;3:3;4,5;54;5,4;4...5;6;5...4;6.4...4.3.5...5.6;3,4.6.5,3.4,4...1.3,5.3...4...3.1...5,5...3...5...6.3:5...1...4,3...3;1...1,5,1.1,4,1...3,3,2...3:4......4:3...etc...4....4...2...4all.........5\n",
      "----------\n",
      "Boeing Trades In The Green On Report Of Coming Software Fix. \n",
      "Boeing Trades In The Green On Report Of Coming Software Fix. I'm not sure if this is a good thing or a bad thing, but I think it's a sign that Boeing is taking steps to reduce their carbon footprint. Here's a link to an article about Boeing's plan to reduce CO2 emissions. It's a bit long, but you get the idea. Edit: Here's another link. I don't know if it's good or bad, but it's better than nothing. Edit 2: I'm sure it's bad. Edit 3: I know it's not good. Edit 4: I've seen it. I've heard it before. Edit 5: I haven't seen it yet. Edit 6: I have seen it before, I've read it. Edit 7: I read it again. I have not seen it, I have read it, and I have heard it again, etc. Edit 8: I'll see it later. I see it now. Edit 9: It's good. I read 9: I heard it. It was good. It is bad. I hear it, it was bad, it's worse, I heard bad, I read bad, and it was good, it is good, I hear bad, then I heard good, then it is better, I think bad, or it is worse, and then I hear good, and now it is bad, so it's great, I'm bad. Then I'm better, and so I'm worse, then good. Then it's all good. And then I'm good, so I think better, then bad. And I'm sorry, I see bad, now it's evil, and worse. I think worse. And it's so bad, than bad, etc, then worse. Then bad. then bad, better, so bad. and it's more bad, bad, worse, than better. I am worse, but worse, worse. and worse, it better, than worse, so worse, better. then worse, more, and me, and better. worse. worse, me, then better, it worse, less, and more, I better, worse than worse. better, more. I better. better. it's less bad, me worse. than better, me better. me, better than me, I worse. it worse. me worse, not worse, bad. worse than me. I worse, the worse. more, me. me better, less.\n",
      "----------\n",
      "KeyBanc Ups DocuSign Price Target On Q4 Beat, Customer Scale. \n",
      "KeyBanc Ups DocuSign Price Target On Q4 Beat, Customer Scale. I'm not sure if this is what you're looking for, but I think it's important to note that KeyBanc isn't just buying DocUSign. They're buying the entire company. And they're doing so on a large scale. I mean, they're not just buying the company. They are buying the hardware. And the software. The hardware. The software. All of it. And all of the services. And everything. It's all software. And it's all hardware. Everything. And nothing else. And that's why it's so important. And why it matters. And how important it is. And what it is, and why it is important, and what it's worth, and how much it matters, and so on. And so on and so forth. And then they're buying it, and they're selling it, because they're making money, because it's a company, and because they want to. And now they're going to make money, and the company is making money. And you're making a profit, and you're paying them, because you're doing something, and it's going to pay them, and then you're getting paid, and that's because you want to make more money. You're making more money, so they're paying you, so you're not going to buy it, you're a company. Because they're being paid, so it's making more. You want to pay you, and now you're going out of money. Because you're buying you, because of the company, you want you. You have to make less money, you don't want to be paid. You don't. You making more, you have more, because. You's making less, you's paying you. and you have less, and more. you want more, and I'm making less money. I don't have more money and you want less. I want more. I have less. You've got more money? You're paying me, and me, you've got less. you're better. you have better, you got better. and me. you better. I've better, I'm better, and better. me, me, I have more. me. me more, me less, me more.you, you, you better, me. I, you. you more.me, you more, more me, more, I more.\n",
      "----------\n",
      "Contractors Claim \"Great Victory\" In $100 Million Knight-Swift Settlement. \n",
      "Contractors Claim \"Great Victory\" In $100 Million Knight-Swift Settlement. It's not a great victory, but it's a good victory. Contractors claim \"great victory\" in a $100 million settlement because they won't have to deal with a lawsuit. They'll be able to hire lawyers, and they'll have a lot of money. That's great. But they'll also have to pay lawyers. They can't afford lawyers. And they can't hire judges. And lawyers can't pay judges. But lawyers can. And judges can't do judges. So they have to hire contractors. And contractors can't. And so on and so forth. And that's good. And then there's a lot more. And it's bad. And there's lots of lawyers and judges and lawyers and lawyers. But there's no lawyers. There's no judges, and no judges. No lawyers. No judges. None of them. And none of them have lawyers. Nothing. And nothing. And nobody has lawyers. So there's nothing to do with lawyers. Nobody has lawyers, no lawyers, nobody has cops. No cops. Nothing has cops, and nobody has judges, no cops, nothing has judges. Nothing's lawyers, nothing's cops, nobody's lawyers. You don't have cops. You've got lawyers, you've got cops, you have cops, cops, police, police. You have lawyers, cops. And you've had cops, people, and police, and you've been sued, and cops, they've got money, and the police, you're cops, the cops, etc. You're not going to have a lawyer, you haven't got a lawyer. You haven't had a judge. You got a judge, you, you got a cop, you had a police, or you got you, and a police. you'll got a court, you'll had a cop. You had a jury, you seen a police and you got an officer, you been arrested, you't a cop and you been a police? You've had a person, you's got a law, you done a lawyer and you had sex, a person. you've seen a law. You been a drug, you heard a cop of a person and you seen you, a guy, you a drug. you had an police, a lot, a police now, a man, you said a law of a law and you a person of a drug and a lot. you a guy. a\n",
      "----------\n",
      "I.D. Systems To Acquire Pointer Telocation. \n",
      "I.D. Systems To Acquire Pointer Telocation. I'm not sure if this is what you're looking for, but I'll give it a shot. 1. ELI5: What is a Pointer? 2. How does it work? 3. Why is it important? 4. Why do I need it? 5. Why does it matter? 6. Why should I care? 7. Why? 8. Why not? 9. Why don't you care? 10. Why are you interested? 11. Why aren't you? 12. Why can't you be bothered? 13. Why isn't this important? 14. Why am I interested? 15. Why shouldn't I be interested? 16. Why haven't you been interested? 17. Why must I be concerned? 18. Why, why are you concerned? 19. Why have you been concerned? 20. What are you not interested? 21. What is important? 23. What's important? What's not important? Why are we interested? What is not important. What isn't important? How are you worried? How is important. How are we concerned? What are important? I'm interested. How is this important. 20. Is it important. Is this important, what's important. It's not so important. You're not important, how are I concerned. How's important, and what's so important? Is it so important, but what's not relevant, what is important, I'm concerned, and I'm worried. Is not. How important. Why. How much? How much. What't so important than me. How many? What't not so much. Is I concerned, how important. I. Is important. And I'm important.. How. What.I. What? I.is.I'm not important than I'm more than me, and you're not so, and me, what't so much, etc.me, and so much? What.it.me.I't.me? What'me. What'not.Im.Ime.it't.I'm not so.Iself.Ihave.itself.it'me?I.me'me'not?Imnot.me'tso.Inot.isnot.you.Iis.it?Ime?me.me',me.you'me',it'tme'Ime'andme'orme.oh.itme.\n",
      "----------\n",
      "Apple vs. Spotify: Apple brings a gun to a knife fight. \n",
      "Apple brings a gun to the knife fight. Spotify brings a knife fight to the sword fight. Apple has a gun. Spotify has a knife. Apple's gun is Spotify. Spotify's knife is Apple's knife. It's not a knife, it's a sword. Apple is the knife. Spotify is the gun. Apple vs. Spotify: Apple brings a weapon. Spotify vs. Apple: The knife. The gun is Apple. The knife is Google. Google vs. Google: the knife is Microsoft. Microsoft vs. Microsoft: the gun is Amazon. Google versus Microsoft: Apple vs Microsoft: Microsoft vs Microsoft. Google against Microsoft: Google vs Google. Microsoft versus Apple: Microsoft versus Microsoft. Apple versus Google: Microsoft against Microsoft. And Microsoft vs Apple: Google versus Google. And Apple vs Google: The gun. And Google vs Apple. And so on and so forth. And then it's the knife, and so on. And the gun, and the gun and the knife and the sword, and then the sword and the music, and all the music. And all the other things. And it's gun and all of the music and the song and the internet, and it's music and everything. And they're all of all the songs, and they all of them. And their music and their music. They're all music and music and they're music and its music and all their songs and their songs. And everything and the songs and all those songs and everything and everything, and their own music. and they are all of their music, all of those songs. They all of these songs and music. The music and them all of it. And them, and them and the people, and you and the other music and themselves. And you, and me. and them. and all all of themselves, and themselves and all. and the others. And me, and others. and me and them, the music of all of that music and it. and it, and those music and others, and I and me, they all, and its own music and you. and you, they, and yourself, and your own music, you and it and the company, and, me and the things, and myself. and themselves, me, me. me and me... and the me and it all. me, you, the others, the me, it, the all of me and all, the other, and of me, themselves. and I, and that. me. I and the myself.\n",
      "----------\n",
      "Boeing Stock Rallies On Hopes For Quick 737 Max Software Fix. \n",
      "Boeing Stock Rallies On Hopes For Quick 737 Max Software Fix. Here's a link to Boeing's Q2 earnings report. It's a little dated, but it's still a good read. Boeing stock is up about 2% today, and it's up about 3% for the year. Boeing's stock is down about 1% today. Boeing shares are down about 2%. Boeing's shares are up about 1.5%. Boeing is up 1.4%. Boeing stocks are down 1.3%. Boeing stock rises 1.2%. Boeing shares rise 1.1%. Boeing Stock rises 1%. Boeing sells 1.0%. Boeing falls 1.6%. Boeing drops 1.7%. Boeing rises 0.8%. Boeing goes up 0.9%. Boeing loses 1.8% and 2.7% and 3.2% and 4.3% and 5.4% and 6.5% and 7.6% and 8.9% and 10% and 9% and 11% and 12% and 15% and 20% and 16% and 30% and 25% and so forth. Boeing is down 1% and 13% and 14% and 17% and 19% and 40% and 50% and 60% and 100% and 80% and 70% and 0.1% and 1,000% and 75% and 90% and 99% and 200% and 500% and 300% and 1000% and other companies. And so on. And 100% is down 2.1. And 1.9. And 2.2. And 0.3. And 5.5. And 3.4. And 10.5,000. And 50% And 100. And 20% And 10,000 and 1. and 100. and 20,000, 1.000 and 20. and 50. and 10.000. 5.000, and 100,000 & 100, and 50,000 And 50.3, and 5,000... and 10,500, and 20%, and 50%, and 5% and others, and so many, and 3,000) and 5'000 and 50'000, etc. and so, and 2,000-5. Many, and more, and 30, and the other, and many, 5'and 10, and others. 5, and a lot, and 25, and 4, and 40, and 9, and other, etc, and 8, and 6, and 7, and 1's and\n",
      "----------\n",
      "It's What Tesla Didn't Say At The Model Y Unveiling That Matters Most. \n",
      "It's what Tesla didn't say At The Model Y Unveiling That Matters Most. It's not the Model Y itself that matters, it's the fact that they didn't talk about it at all. They didn't even mention it in the press release. They just didn't mention it. That's all that matters. It doesn't matter what they say. It matters what they don't say, because it matters what Tesla doesn't say. They don't have to say anything. They can say whatever they want, but they can't say anything about it. And that's all they need to say. And they can say anything they want. And it matters. But it doesn't. Because it's what they want to say, not what they're saying. Because they're not saying. And because it's not what Tesla wants to say it is. And Tesla wants it to be. And you don't want to hear it. So they say nothing about it, because they're afraid of it. They say nothing. And then they say it, and they say something, and you say nothing, and then they're like, and it's like, they say, \"I don't care. And I don't like it. You don't know anything. But you're wrong. You're wrong, and I'm wrong, you're right, you know. You are wrong, but you're not wrong. I'm not wrong, I'm right, and me, you are wrong. And me, I am wrong. and you are right. and me. I am right. I're wrong and you're me, and we're not right. You. and I am not right, but I'm me. You, you, you. I, and, you and you. you. You and you, me. you, I, me, me and you and I. I.me, and all of you. It is wrong. you are, and the other, and that's wrong. it's wrong, it is wrong, so, I.you.me. me.you, I're me. me, it, you all, you's not me.andme, you;me.me;me, me;you.youself, you...me,me,you,me.self.me?me.youme,andme.I.me...me.it.me-me.all.meself,youself.self, andme.\n",
      "----------\n",
      "Adobe Stock Drops After 'Noisy' Quarter, Disappointing Guidance. \n",
      "Adobe Stock Drops After 'Noisy' Quarter, Disappointing Guidance. I'm not sure what you mean by 'noisy' quarter, or 'disappointing guidance'. If you're referring to Adobe's financial performance, that's a different story. Adobe's stock is dropping because: 1. Adobe is losing money. 2. Adobe isn't making money. 3. Adobe doesn't make money. 4. Adobe makes money. 5. Adobe loses money. 6. Adobe lost money. 7. Adobe shares are falling. 8. Adobe stock is falling because: 9. Adobe made money. 10. Adobe didn't make any money. 11. Adobe has no money. 12. Adobe had no revenue. 13. Adobe wasn't profitable. 14. Adobe was not profitable. 15. Adobe did not make any profit. 16. Adobe failed to make any revenue. 17. Adobe missed no profit. 18. Adobe went bankrupt. 19. Adobe fell. 20. Adobe reported a loss. 21. Adobe declined. 22. Adobe said no loss. 23. 25. Adobe dropped a profit. 26. 30. Adobe declared a loss of profit. 27. Adobe says no loss of revenue. 28. Adobe announced no profit, Adobe lost a loss on revenue. 29. Adobe declares no loss on earnings. 31. Adobe loss of earnings. 30: Adobe lost no profit on quarter. 31: Adobe stock dropped on quarter of quarter. 32. Adobe earnings. 33. Adobe Stock drops on quarter, Adobe stock drops on loss of quarter of year. 33: Adobe's loss on quarter and quarter. 35. Adobe. 35: Adobe loss on loss. 36. Adobe losing on quarter about quarter. 37. Adobe share of quarter, loss of loss. quarter of loss about quarter of earnings on quarter on quarter than quarter. 25: Adobe. 33, loss. Adobe, loss about loss. 25% of quarter about loss of year of quarter than loss. loss of number of quarter on loss, loss on number of loss of stock. loss on profit. loss. number of company, loss, quarter of company. loss, stock of quarter: loss of share of loss, share of stock, loss in quarter. loss about company, stock, stock. stock of loss on stock. share of company about loss, number of stock about loss about stock. company. stock. number. loss than loss, same loss. stock, share, stock about stock, same stock. same stock, company, same company.\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22360\\2527559121.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#orig_ans = dataset.iloc[i]['Target Text']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     generated_answers_encoded = model.generate(input_ids=model_input[\"input_ids\"].to(device),\n\u001b[0m\u001b[0;32m     19\u001b[0m                                                  \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attention_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                                  \u001b[0mdo_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1488\u001b[0m             )\n\u001b[0;32m   1489\u001b[0m             \u001b[1;31m# 13. run beam search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1490\u001b[1;33m             return self.beam_search(\n\u001b[0m\u001b[0;32m   1491\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2747\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2749\u001b[1;33m             outputs = self(\n\u001b[0m\u001b[0;32m   2750\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2751\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1371\u001b[0m                 )\n\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1373\u001b[1;33m         outputs = self.model(\n\u001b[0m\u001b[0;32m   1374\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \u001b[1;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[0;32m   1256\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1111\u001b[0m                 )\n\u001b[0;32m   1112\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[0;32m   1114\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;31m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[0;32m    427\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stude\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[0mvalue_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;31m# self_attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "##  Getting predictions from pre-trained model ..\n",
    "import torch, nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from transformers import AutoModel, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bart_lfqa\")\n",
    "dataset = pd.read_parquet('data.parquet')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"vblagoje/bart_lfqa\").to(device)\n",
    "\n",
    "for i in range(100):\n",
    "    inputs = scraped_df.iloc[i]['Input Text']\n",
    "    #orig_ans = dataset.iloc[i]['Target Text']\n",
    "    model_input = tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\") \n",
    "    generated_answers_encoded = model.generate(input_ids=model_input[\"input_ids\"].to(device),\n",
    "                                                 attention_mask=model_input[\"attention_mask\"].to(device),min_length=512, max_length=3200,\n",
    "                                                 do_sample=False, early_stopping=True, num_beams=8, temperature=1.0, top_k=None,\n",
    "                                                 top_p=None, eos_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3,\n",
    "                                                 num_return_sequences=1)\n",
    "    ans = tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "  \n",
    "    text = scraped_df.iloc[i]['Input Text']\n",
    "    print((text.split('question: ')[1]).split('context: ')[0])\n",
    "    print (ans[0])\n",
    "    print ('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97fc6ed1-2a48-477f-9723-404d7c191bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrices in NLP .. \n",
      "(0, 'lvwerra/test')\n",
      "(1, 'precision')\n",
      "(2, 'code_eval')\n",
      "(3, 'roc_auc')\n",
      "(4, 'cuad')\n",
      "(5, 'xnli')\n",
      "(6, 'rouge')\n",
      "(7, 'pearsonr')\n",
      "(8, 'mse')\n",
      "(9, 'super_glue')\n",
      "(10, 'comet')\n",
      "(11, 'cer')\n",
      "(12, 'sacrebleu')\n",
      "(13, 'mahalanobis')\n",
      "(14, 'wer')\n",
      "(15, 'competition_math')\n",
      "(16, 'f1')\n",
      "(17, 'recall')\n",
      "(18, 'coval')\n",
      "(19, 'mauve')\n",
      "(20, 'xtreme_s')\n",
      "(21, 'bleurt')\n",
      "(22, 'ter')\n",
      "(23, 'accuracy')\n",
      "(24, 'exact_match')\n",
      "(25, 'indic_glue')\n",
      "(26, 'spearmanr')\n",
      "(27, 'mae')\n",
      "(28, 'squad')\n",
      "(29, 'chrf')\n",
      "(30, 'glue')\n",
      "(31, 'perplexity')\n",
      "(32, 'mean_iou')\n",
      "(33, 'squad_v2')\n",
      "(34, 'meteor')\n",
      "(35, 'bleu')\n",
      "(36, 'wiki_split')\n",
      "(37, 'sari')\n",
      "(38, 'frugalscore')\n",
      "(39, 'google_bleu')\n",
      "(40, 'bertscore')\n",
      "(41, 'matthews_correlation')\n",
      "(42, 'seqeval')\n",
      "(43, 'trec_eval')\n",
      "(44, 'rl_reliability')\n",
      "(45, 'jordyvl/ece')\n",
      "(46, 'angelina-wang/directional_bias_amplification')\n",
      "(47, 'cpllab/syntaxgym')\n",
      "(48, 'lvwerra/bary_score')\n",
      "(49, 'kaggle/amex')\n",
      "(50, 'kaggle/ai4code')\n",
      "(51, 'hack/test_metric')\n",
      "(52, 'yzha/ctc_eval')\n",
      "(53, 'codeparrot/apps_metric')\n",
      "(54, 'mfumanelli/geometric_mean')\n",
      "(55, 'daiyizheng/valid')\n",
      "(56, 'poseval')\n",
      "(57, 'erntkn/dice_coefficient')\n",
      "(58, 'mgfrantz/roc_auc_macro')\n",
      "(59, 'Vlasta/pr_auc')\n",
      "(60, 'gorkaartola/metric_for_tp_fp_samples')\n",
      "(61, 'idsedykh/metric')\n",
      "(62, 'idsedykh/codebleu2')\n",
      "(63, 'idsedykh/codebleu')\n",
      "(64, 'idsedykh/megaglue')\n",
      "(65, 'kasmith/woodscore')\n",
      "(66, 'cakiki/ndcg')\n",
      "(67, 'brier_score')\n",
      "(68, 'Vertaix/vendiscore')\n",
      "(69, 'GMFTBY/dailydialogevaluate')\n",
      "(70, 'GMFTBY/dailydialog_evaluate')\n",
      "(71, 'jzm-mailchimp/joshs_second_test_metric')\n",
      "(72, 'ola13/precision_at_k')\n",
      "(73, 'yulong-me/yl_metric')\n",
      "(74, 'abidlabs/mean_iou')\n",
      "(75, 'abidlabs/mean_iou2')\n",
      "(76, 'KevinSpaghetti/accuracyk')\n",
      "(77, 'Felipehonorato/my_metric')\n",
      "(78, 'NimaBoscarino/weat')\n",
      "(79, 'ronaldahmed/nwentfaithfulness')\n",
      "(80, 'Viona/infolm')\n",
      "(81, 'kyokote/my_metric2')\n",
      "(82, 'kashif/mape')\n",
      "(83, 'Ochiroo/rouge_mn')\n",
      "(84, 'giulio98/code_eval_outputs')\n",
      "(85, 'leslyarun/fbeta_score')\n",
      "(86, 'giulio98/codebleu')\n",
      "(87, 'anz2/iliauniiccocrevaluation')\n",
      "(88, 'zbeloki/m2')\n",
      "(89, 'xu1998hz/sescore')\n",
      "(90, 'mase')\n",
      "(91, 'mape')\n",
      "(92, 'smape')\n",
      "(93, 'dvitel/codebleu')\n",
      "(94, 'NCSOFT/harim_plus')\n",
      "(95, 'JP-SystemsX/nDCG')\n",
      "(96, 'sportlosos/sescore')\n",
      "(97, 'Drunper/metrica_tesi')\n",
      "(98, 'jpxkqx/peak_signal_to_noise_ratio')\n",
      "(99, 'jpxkqx/signal_to_reconstrution_error')\n",
      "(100, 'hpi-dhc/FairEval')\n",
      "(101, 'nist_mt')\n",
      "(102, 'lvwerra/accuracy_score')\n",
      "(103, 'character')\n",
      "(104, 'charcut_mt')\n",
      "(105, 'ybelkada/cocoevaluate')\n",
      "(106, 'harshhpareek/bertscore')\n",
      "(107, 'posicube/mean_reciprocal_rank')\n",
      "(108, 'bstrai/classification_report')\n",
      "(109, 'omidf/squad_precision_recall')\n",
      "(110, 'Josh98/nl2bash_m')\n",
      "(111, 'BucketHeadP65/confusion_matrix')\n",
      "(112, 'BucketHeadP65/roc_curve')\n",
      "(113, 'yonting/average_precision_score')\n",
      "(114, 'transZ/test_parascore')\n",
      "(115, 'transZ/sbert_cosine')\n",
      "(116, 'hynky/sklearn_proxy')\n",
      "(117, 'mcnemar')\n",
      "(118, 'exact_match')\n",
      "(119, 'wilcoxon')\n",
      "(120, 'ncoop57/levenshtein_distance')\n",
      "(121, 'kaleidophon/almost_stochastic_order')\n",
      "(122, 'word_length')\n",
      "(123, 'lvwerra/element_count')\n",
      "(124, 'word_count')\n",
      "(125, 'text_duplicates')\n",
      "(126, 'perplexity')\n",
      "(127, 'label_distribution')\n",
      "(128, 'toxicity')\n",
      "(129, 'prb977/cooccurrence_count')\n",
      "(130, 'regard')\n",
      "(131, 'honest')\n",
      "(132, 'NimaBoscarino/pseudo_perplexity')\n",
      "(133, 'ybelkada/toxicity')\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "print ('Evaluation metrices in NLP .. ')\n",
    "for metrices in enumerate(evaluate.list_evaluation_modules()):\n",
    "    print (metrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cacb956-7b58-48ff-940c-48d35c1c6972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.2910223541851805,\n",
       " 'rouge2': 0.14529131843778137,\n",
       " 'rougeL': 0.24518024564755925,\n",
       " 'rougeLsum': 0.24511279751095388}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data = pd.read_csv('data.csv')\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "for i in range(pred_data.shape[0]):\n",
    "    candidate = pred_data.iloc[i]['Candidate']\n",
    "    reference = pred_data.iloc[i]['Reference']\n",
    "    candidates.append(candidate)\n",
    "    references.append(reference)\n",
    "    \n",
    "# Rouge scores .. \n",
    "rouge = evaluate.load('rouge')\n",
    "rouge.compute(predictions=candidates, references = references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31e115c9-e184-4a17-bf75-f03e52d76ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22aa958dbb345559921999d38232f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599010da487f421c8930c066e789ad36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17955aab75204400a39db54946cd0a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd64fb5420fb4f0194e55001a3d81b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee8a5b06cfd41f2a27d9f21bd42d70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.8168895244598389,\n",
       "  0.8019647002220154,\n",
       "  0.8553283214569092,\n",
       "  0.8143807649612427,\n",
       "  0.8631734848022461,\n",
       "  0.7982041239738464,\n",
       "  0.9067254662513733,\n",
       "  0.8278956413269043,\n",
       "  0.8609578013420105,\n",
       "  0.8544194102287292,\n",
       "  0.8585938215255737,\n",
       "  0.87358558177948,\n",
       "  0.8630079627037048,\n",
       "  0.8343862295150757,\n",
       "  0.9040990471839905,\n",
       "  0.7990198731422424,\n",
       "  0.8621712923049927,\n",
       "  0.8312155604362488,\n",
       "  0.8632560968399048,\n",
       "  0.8648049831390381,\n",
       "  0.8106958866119385,\n",
       "  0.8513157963752747,\n",
       "  0.9130407571792603,\n",
       "  0.9051608443260193,\n",
       "  0.7944158911705017,\n",
       "  0.8533000946044922,\n",
       "  0.9218778610229492,\n",
       "  0.8472998142242432,\n",
       "  0.8361377120018005,\n",
       "  0.8415578603744507,\n",
       "  0.9029667377471924,\n",
       "  0.833875834941864,\n",
       "  0.8519759178161621,\n",
       "  0.9179714918136597,\n",
       "  0.811511754989624,\n",
       "  0.8617004156112671,\n",
       "  0.8532947897911072,\n",
       "  0.7770687341690063,\n",
       "  0.8556592464447021,\n",
       "  0.865023136138916,\n",
       "  0.8625094890594482,\n",
       "  0.8158740997314453,\n",
       "  0.8196545243263245,\n",
       "  0.8481621146202087,\n",
       "  0.8410518765449524,\n",
       "  0.8785098195075989,\n",
       "  0.8076927661895752,\n",
       "  0.9373558759689331,\n",
       "  0.8133595585823059,\n",
       "  0.8952068090438843,\n",
       "  0.8205430507659912,\n",
       "  0.8237491250038147,\n",
       "  0.8468120098114014,\n",
       "  0.8596718907356262,\n",
       "  0.8156405091285706,\n",
       "  0.8313325047492981,\n",
       "  0.823479413986206,\n",
       "  0.8169834017753601,\n",
       "  0.798799991607666,\n",
       "  0.8503011465072632,\n",
       "  0.8449333310127258,\n",
       "  0.8552035689353943,\n",
       "  0.823064923286438,\n",
       "  0.8399831056594849,\n",
       "  0.8390403985977173,\n",
       "  0.9006175398826599,\n",
       "  0.8560118079185486,\n",
       "  0.8399600982666016,\n",
       "  0.9136227965354919,\n",
       "  0.8346573114395142,\n",
       "  0.8436022996902466,\n",
       "  0.8842211365699768,\n",
       "  0.8896768093109131,\n",
       "  0.8262585401535034,\n",
       "  0.873744785785675,\n",
       "  0.8491083383560181,\n",
       "  0.8743191361427307,\n",
       "  0.8412367105484009,\n",
       "  0.8252299427986145,\n",
       "  0.8699148893356323,\n",
       "  0.9658868312835693,\n",
       "  0.8073245286941528,\n",
       "  0.8046866655349731,\n",
       "  0.848943829536438,\n",
       "  0.7980422377586365,\n",
       "  0.8233263492584229,\n",
       "  0.8000746965408325,\n",
       "  0.9999999403953552,\n",
       "  0.8736447691917419,\n",
       "  0.8493481278419495,\n",
       "  0.8378271460533142,\n",
       "  0.8697676062583923,\n",
       "  0.8153162002563477,\n",
       "  0.8138262033462524,\n",
       "  0.882783055305481,\n",
       "  0.8459495306015015,\n",
       "  0.7999508380889893,\n",
       "  0.7875345349311829,\n",
       "  0.8833869099617004,\n",
       "  0.8626053333282471,\n",
       "  0.9181439280509949,\n",
       "  0.8179258704185486,\n",
       "  0.8945740461349487,\n",
       "  0.8206347823143005,\n",
       "  0.8897116780281067,\n",
       "  0.8506430387496948,\n",
       "  0.8142831921577454,\n",
       "  0.9365649819374084,\n",
       "  0.7902882099151611,\n",
       "  0.9663023352622986,\n",
       "  0.8405187726020813,\n",
       "  0.9022724032402039,\n",
       "  0.7926401495933533,\n",
       "  0.8544831275939941,\n",
       "  0.8215804696083069,\n",
       "  0.9782195687294006,\n",
       "  0.8598840832710266,\n",
       "  0.97209233045578,\n",
       "  0.8784201145172119,\n",
       "  0.9157640933990479,\n",
       "  0.8623061180114746,\n",
       "  0.816614031791687,\n",
       "  0.8655651807785034,\n",
       "  0.8700920939445496,\n",
       "  0.8631269335746765,\n",
       "  0.8686367273330688,\n",
       "  0.826317548751831,\n",
       "  0.8420516848564148,\n",
       "  0.8485382795333862,\n",
       "  0.799752414226532,\n",
       "  0.8991382122039795,\n",
       "  0.9005206227302551,\n",
       "  0.8828011155128479,\n",
       "  0.8624149560928345,\n",
       "  0.8706614971160889,\n",
       "  0.865578830242157,\n",
       "  0.8331320285797119,\n",
       "  0.8863795399665833,\n",
       "  0.7880687713623047,\n",
       "  0.8575595021247864,\n",
       "  0.8791473507881165,\n",
       "  0.8270008563995361,\n",
       "  0.8203415274620056,\n",
       "  0.8436731696128845,\n",
       "  0.8381245136260986,\n",
       "  0.7966206073760986,\n",
       "  0.897299587726593,\n",
       "  0.8394902944564819,\n",
       "  0.8036373257637024,\n",
       "  0.8553556203842163,\n",
       "  0.8282667398452759,\n",
       "  0.853807806968689,\n",
       "  0.8284898400306702,\n",
       "  0.8021643161773682,\n",
       "  0.8667582869529724,\n",
       "  0.8682271242141724,\n",
       "  0.8410242199897766,\n",
       "  0.8485532999038696,\n",
       "  0.9050861597061157,\n",
       "  0.81266850233078,\n",
       "  0.8688077926635742,\n",
       "  0.7980635762214661,\n",
       "  0.8480088710784912,\n",
       "  0.906940221786499,\n",
       "  0.8914679884910583,\n",
       "  0.7931961417198181,\n",
       "  0.8575382828712463,\n",
       "  0.8480644226074219,\n",
       "  0.8260679841041565,\n",
       "  0.8924654722213745,\n",
       "  0.846768856048584,\n",
       "  0.8773106336593628,\n",
       "  0.8367905020713806,\n",
       "  0.8816591501235962,\n",
       "  0.8876282572746277,\n",
       "  0.8197789192199707,\n",
       "  0.8336011171340942,\n",
       "  0.8874266743659973,\n",
       "  0.8217582702636719,\n",
       "  0.8462197780609131,\n",
       "  0.9648644924163818,\n",
       "  0.993846595287323,\n",
       "  0.841018557548523,\n",
       "  0.9441213011741638,\n",
       "  0.7969198822975159,\n",
       "  0.8934682607650757,\n",
       "  0.8339227437973022,\n",
       "  0.9534602165222168,\n",
       "  0.8440268039703369,\n",
       "  0.8887717723846436,\n",
       "  0.9734496474266052,\n",
       "  0.8733178377151489,\n",
       "  0.9936745166778564,\n",
       "  0.8850425481796265,\n",
       "  0.9051705002784729,\n",
       "  0.7899861335754395,\n",
       "  0.8863376975059509,\n",
       "  0.8358772397041321,\n",
       "  0.9525018930435181,\n",
       "  0.8183590173721313,\n",
       "  0.8570138216018677,\n",
       "  0.8620575666427612,\n",
       "  0.8087201714515686,\n",
       "  0.8273029327392578,\n",
       "  0.81361323595047,\n",
       "  0.7823556065559387,\n",
       "  0.8135594129562378,\n",
       "  0.7909958362579346,\n",
       "  0.8803432583808899,\n",
       "  0.8342089056968689,\n",
       "  0.7880022525787354,\n",
       "  0.8781358599662781,\n",
       "  0.8220081329345703,\n",
       "  0.8456079959869385,\n",
       "  0.8746397495269775,\n",
       "  0.8557173013687134,\n",
       "  0.8864267468452454,\n",
       "  0.8282325863838196,\n",
       "  0.9935621619224548,\n",
       "  0.8266026973724365,\n",
       "  0.9803318977355957,\n",
       "  0.7956003546714783,\n",
       "  0.8678789734840393,\n",
       "  0.8406918048858643,\n",
       "  0.8610033988952637,\n",
       "  0.8610034584999084,\n",
       "  0.7689916491508484,\n",
       "  0.8593044281005859,\n",
       "  0.8176716566085815,\n",
       "  0.8973932862281799,\n",
       "  0.8080836534500122,\n",
       "  0.8754680156707764,\n",
       "  0.7973126173019409,\n",
       "  0.9819200038909912,\n",
       "  0.9461697340011597,\n",
       "  0.7994086742401123,\n",
       "  0.7994226217269897,\n",
       "  0.9216071367263794,\n",
       "  0.9860222339630127,\n",
       "  0.8345595002174377,\n",
       "  0.851902961730957,\n",
       "  0.8484084606170654,\n",
       "  0.8464251160621643,\n",
       "  0.8266483545303345,\n",
       "  0.8426841497421265,\n",
       "  0.8354078531265259,\n",
       "  0.809876561164856,\n",
       "  0.9817372560501099,\n",
       "  0.8386623859405518,\n",
       "  0.8875032663345337,\n",
       "  0.907721221446991,\n",
       "  0.8434880375862122,\n",
       "  0.8444940447807312,\n",
       "  0.8491325378417969,\n",
       "  0.7626921534538269,\n",
       "  0.8352291584014893,\n",
       "  0.8205790519714355,\n",
       "  0.8363540172576904,\n",
       "  1.0,\n",
       "  0.8115675449371338,\n",
       "  0.8253688216209412,\n",
       "  0.8247827291488647,\n",
       "  0.853441596031189,\n",
       "  0.838908851146698,\n",
       "  0.8972271680831909,\n",
       "  0.8325629830360413,\n",
       "  0.8732136487960815,\n",
       "  0.8897249698638916,\n",
       "  0.8259281516075134,\n",
       "  0.8817345499992371,\n",
       "  0.8654376268386841,\n",
       "  0.7886086702346802,\n",
       "  0.8286593556404114,\n",
       "  0.8519821166992188,\n",
       "  0.9061959981918335,\n",
       "  0.8296847343444824,\n",
       "  0.8471542000770569,\n",
       "  0.849597692489624,\n",
       "  0.8651214838027954,\n",
       "  0.804693341255188,\n",
       "  0.8994712829589844,\n",
       "  0.8158667087554932,\n",
       "  0.8547839522361755,\n",
       "  0.7969654202461243,\n",
       "  0.865016520023346,\n",
       "  0.898643434047699,\n",
       "  0.8442763686180115,\n",
       "  0.8819686770439148,\n",
       "  0.8285864591598511,\n",
       "  0.8856604099273682,\n",
       "  0.9189720153808594,\n",
       "  0.8529126048088074,\n",
       "  0.8303808569908142,\n",
       "  0.9999999403953552,\n",
       "  0.7722229361534119,\n",
       "  0.8647345900535583,\n",
       "  0.8402795791625977,\n",
       "  0.8692430257797241,\n",
       "  0.8550673127174377,\n",
       "  0.8586527109146118,\n",
       "  0.896953821182251,\n",
       "  0.8194648623466492,\n",
       "  0.9542698860168457,\n",
       "  0.9932988882064819,\n",
       "  0.7820035219192505,\n",
       "  0.8272724747657776,\n",
       "  0.833105206489563,\n",
       "  0.7806922197341919,\n",
       "  0.8257865309715271,\n",
       "  0.8721776008605957,\n",
       "  0.8310539126396179,\n",
       "  0.9999999403953552,\n",
       "  0.9883651733398438,\n",
       "  0.7667674422264099,\n",
       "  0.7851166129112244,\n",
       "  0.8462358713150024,\n",
       "  0.8341578245162964,\n",
       "  0.8535211086273193,\n",
       "  0.835982620716095,\n",
       "  0.7705861926078796,\n",
       "  0.8387344479560852,\n",
       "  0.8940114378929138,\n",
       "  0.8542049527168274,\n",
       "  0.8940653204917908,\n",
       "  0.807338297367096,\n",
       "  0.8330615162849426,\n",
       "  0.8144005537033081,\n",
       "  0.8769422173500061,\n",
       "  0.8386358618736267,\n",
       "  0.8483321666717529,\n",
       "  0.7952945232391357,\n",
       "  0.8267538547515869,\n",
       "  0.9186980724334717,\n",
       "  0.8841789960861206,\n",
       "  0.8683608770370483,\n",
       "  0.930200457572937,\n",
       "  0.7789062261581421,\n",
       "  0.7787532210350037,\n",
       "  0.853103518486023,\n",
       "  0.8229150772094727,\n",
       "  0.8800874948501587,\n",
       "  0.8782168626785278,\n",
       "  0.7945203185081482,\n",
       "  0.8416433334350586,\n",
       "  0.8125879168510437,\n",
       "  0.900031566619873,\n",
       "  0.82166588306427,\n",
       "  0.7883912920951843,\n",
       "  0.8208604454994202,\n",
       "  0.8399079442024231,\n",
       "  0.7999122738838196,\n",
       "  0.8340191841125488,\n",
       "  0.8497874736785889,\n",
       "  0.8657835721969604,\n",
       "  0.8004971742630005,\n",
       "  0.8426607847213745,\n",
       "  0.7878623008728027,\n",
       "  0.9212279319763184,\n",
       "  1.0,\n",
       "  0.8636185526847839,\n",
       "  0.8701310157775879,\n",
       "  0.7755826711654663,\n",
       "  0.825979471206665,\n",
       "  0.9342309236526489,\n",
       "  0.813634991645813,\n",
       "  0.8468517661094666,\n",
       "  0.8260258436203003,\n",
       "  0.9176294803619385,\n",
       "  0.8421785235404968,\n",
       "  0.8667964935302734,\n",
       "  0.8683132529258728,\n",
       "  0.7878836989402771,\n",
       "  0.9084166884422302,\n",
       "  0.8547649383544922,\n",
       "  0.8316981792449951,\n",
       "  0.8705646991729736,\n",
       "  0.8597808480262756,\n",
       "  0.835731029510498,\n",
       "  0.8338669538497925,\n",
       "  0.8958495855331421,\n",
       "  0.9284050464630127,\n",
       "  0.8801041841506958,\n",
       "  0.8551317453384399,\n",
       "  0.8771610260009766,\n",
       "  0.8113945126533508,\n",
       "  0.8630316257476807,\n",
       "  0.834782600402832,\n",
       "  0.8601710796356201,\n",
       "  0.8144115805625916,\n",
       "  0.8131594657897949,\n",
       "  0.8145104646682739,\n",
       "  0.8587814569473267,\n",
       "  0.8757843971252441,\n",
       "  0.8313226103782654,\n",
       "  0.8547001481056213,\n",
       "  0.8441083431243896,\n",
       "  0.8978655338287354,\n",
       "  0.814104437828064,\n",
       "  0.8624294996261597,\n",
       "  0.8389402031898499,\n",
       "  0.8419221639633179,\n",
       "  0.7525742650032043,\n",
       "  0.857800304889679,\n",
       "  0.8240735530853271,\n",
       "  0.8368508815765381,\n",
       "  0.8372223973274231,\n",
       "  0.7962686419487,\n",
       "  0.8575415015220642,\n",
       "  0.9293155074119568,\n",
       "  0.811089038848877,\n",
       "  0.8638033270835876,\n",
       "  0.8532209992408752,\n",
       "  0.8090806603431702,\n",
       "  0.8450816869735718,\n",
       "  0.8798899054527283,\n",
       "  0.8252538442611694,\n",
       "  0.8590815663337708,\n",
       "  0.7918915748596191,\n",
       "  0.8334986567497253,\n",
       "  0.8798596262931824,\n",
       "  0.8412653207778931,\n",
       "  0.8552492260932922,\n",
       "  0.766360878944397,\n",
       "  0.860722541809082,\n",
       "  0.8423747420310974,\n",
       "  0.8296036720275879,\n",
       "  1.0,\n",
       "  0.8750836253166199,\n",
       "  0.929360032081604,\n",
       "  0.8386814594268799,\n",
       "  0.8188442587852478,\n",
       "  0.8699319362640381,\n",
       "  0.8423279523849487,\n",
       "  0.843501091003418,\n",
       "  0.8992267847061157,\n",
       "  0.8662563562393188,\n",
       "  0.8211989402770996,\n",
       "  0.8041476607322693,\n",
       "  0.8287574052810669,\n",
       "  0.8381829261779785,\n",
       "  0.8463216423988342,\n",
       "  0.8915795683860779,\n",
       "  0.8748198747634888,\n",
       "  0.8034809827804565,\n",
       "  0.8430531024932861,\n",
       "  0.8842417001724243,\n",
       "  0.8470984101295471,\n",
       "  0.915260910987854,\n",
       "  0.8886171579360962,\n",
       "  0.8150038123130798,\n",
       "  0.8514983654022217,\n",
       "  0.8791236877441406,\n",
       "  0.8865331411361694,\n",
       "  0.8637728691101074,\n",
       "  0.9014432430267334,\n",
       "  0.8771367073059082,\n",
       "  0.8358006477355957,\n",
       "  0.8125298023223877,\n",
       "  0.8399760723114014,\n",
       "  0.8428090214729309,\n",
       "  0.8352928757667542,\n",
       "  0.8320460319519043,\n",
       "  0.8536003232002258,\n",
       "  0.9624559879302979,\n",
       "  0.8287120461463928,\n",
       "  0.9059878587722778,\n",
       "  0.7835603952407837,\n",
       "  0.8877618312835693,\n",
       "  0.8659955859184265,\n",
       "  0.8800401091575623,\n",
       "  0.8419204950332642,\n",
       "  0.9146302342414856,\n",
       "  0.8217824697494507,\n",
       "  0.9613246917724609,\n",
       "  0.8974550366401672,\n",
       "  0.8388198614120483,\n",
       "  0.8441893458366394,\n",
       "  0.8796386122703552,\n",
       "  0.8528470396995544,\n",
       "  0.8687860369682312,\n",
       "  0.8210709691047668,\n",
       "  0.8715583682060242,\n",
       "  0.8073656558990479,\n",
       "  0.818816065788269,\n",
       "  0.8796798586845398,\n",
       "  0.8191537261009216,\n",
       "  0.8653206825256348,\n",
       "  0.8218911290168762,\n",
       "  0.7778759598731995,\n",
       "  0.979341983795166,\n",
       "  0.8793962001800537,\n",
       "  0.8550763726234436,\n",
       "  0.8359089493751526,\n",
       "  0.8749119639396667,\n",
       "  0.8311895728111267,\n",
       "  0.8962109684944153,\n",
       "  0.888271689414978,\n",
       "  0.8158060312271118,\n",
       "  0.8402373194694519,\n",
       "  0.8847679495811462,\n",
       "  0.8063694834709167,\n",
       "  0.8360788226127625,\n",
       "  0.8358613848686218,\n",
       "  0.9999998807907104,\n",
       "  0.8679613471031189,\n",
       "  0.837654709815979,\n",
       "  0.8518450260162354,\n",
       "  0.8119091391563416,\n",
       "  0.8393262624740601,\n",
       "  0.9183008074760437,\n",
       "  0.8155439496040344,\n",
       "  0.8083206415176392,\n",
       "  0.8683822154998779,\n",
       "  0.8615548014640808,\n",
       "  0.8811880946159363,\n",
       "  0.7849732637405396,\n",
       "  0.836055338382721,\n",
       "  0.8291847109794617,\n",
       "  1.0000001192092896,\n",
       "  0.851235032081604,\n",
       "  0.8215876817703247,\n",
       "  0.8261538147926331,\n",
       "  0.8604801297187805,\n",
       "  0.840779721736908,\n",
       "  0.8605029582977295,\n",
       "  0.8392848968505859,\n",
       "  0.835010826587677,\n",
       "  0.8263719081878662,\n",
       "  0.7897619009017944,\n",
       "  0.840306282043457,\n",
       "  0.8152124881744385,\n",
       "  0.8597927093505859,\n",
       "  0.8709235191345215,\n",
       "  0.8169071078300476,\n",
       "  0.8547611832618713,\n",
       "  0.9049431681632996,\n",
       "  0.8090122938156128,\n",
       "  0.8325332403182983,\n",
       "  0.836273193359375,\n",
       "  0.8092098236083984,\n",
       "  0.8288265466690063,\n",
       "  0.8612836003303528,\n",
       "  0.8987011313438416,\n",
       "  0.8588818311691284,\n",
       "  0.8841039538383484,\n",
       "  0.986943244934082,\n",
       "  0.812242865562439,\n",
       "  0.8566903471946716,\n",
       "  0.8083227872848511,\n",
       "  0.8271613717079163,\n",
       "  0.8808634281158447,\n",
       "  0.9071585536003113,\n",
       "  0.8272976279258728,\n",
       "  0.8546386361122131,\n",
       "  0.8020420074462891,\n",
       "  0.8935510516166687,\n",
       "  0.8729904890060425,\n",
       "  0.8275235891342163,\n",
       "  0.8759183883666992,\n",
       "  0.8190904259681702,\n",
       "  0.8339591026306152,\n",
       "  0.9013983011245728,\n",
       "  0.8387666940689087,\n",
       "  0.8380861282348633,\n",
       "  0.9890755414962769,\n",
       "  0.8808725476264954,\n",
       "  0.9035356044769287,\n",
       "  0.8149120807647705,\n",
       "  0.864463210105896,\n",
       "  0.8878263831138611,\n",
       "  0.8514724373817444,\n",
       "  0.8291720151901245,\n",
       "  0.8879809379577637,\n",
       "  0.7229053974151611,\n",
       "  0.8383185267448425,\n",
       "  0.8767263889312744,\n",
       "  0.8173342943191528,\n",
       "  0.8367860317230225,\n",
       "  0.8500732183456421,\n",
       "  0.8201488256454468,\n",
       "  0.8684288263320923,\n",
       "  0.8684372901916504,\n",
       "  0.8906667232513428,\n",
       "  0.8912402987480164,\n",
       "  0.8992329239845276,\n",
       "  0.8976742625236511,\n",
       "  0.9326133728027344,\n",
       "  0.8264382481575012,\n",
       "  0.8514453768730164,\n",
       "  0.8909825086593628,\n",
       "  0.8374335169792175,\n",
       "  0.8721256256103516,\n",
       "  0.9122278690338135,\n",
       "  0.8235973119735718,\n",
       "  0.8520007133483887,\n",
       "  0.8075955510139465,\n",
       "  0.8127202987670898,\n",
       "  0.8775038719177246,\n",
       "  0.8080453872680664,\n",
       "  0.8578823208808899,\n",
       "  0.8731640577316284,\n",
       "  0.8906859755516052,\n",
       "  0.8383275270462036,\n",
       "  0.807727575302124,\n",
       "  0.8862886428833008,\n",
       "  0.9358862638473511,\n",
       "  0.8456220626831055,\n",
       "  0.809037446975708,\n",
       "  0.8976148366928101,\n",
       "  0.8596683144569397,\n",
       "  0.840735912322998,\n",
       "  0.834138810634613,\n",
       "  0.866899847984314,\n",
       "  0.9329168796539307,\n",
       "  0.9999999403953552,\n",
       "  0.8121906518936157,\n",
       "  0.8153085708618164,\n",
       "  0.8477329015731812,\n",
       "  0.8584405183792114,\n",
       "  0.7746235132217407,\n",
       "  0.8126255869865417,\n",
       "  0.8873953223228455,\n",
       "  0.7873538136482239,\n",
       "  0.7774078249931335,\n",
       "  0.8561898469924927,\n",
       "  0.997416079044342,\n",
       "  0.8211771249771118,\n",
       "  0.8475674390792847,\n",
       "  0.8475085496902466,\n",
       "  0.8612033128738403,\n",
       "  0.8411684036254883,\n",
       "  0.8048195242881775,\n",
       "  0.8186710476875305,\n",
       "  0.8704357147216797,\n",
       "  1.0,\n",
       "  0.8492681980133057,\n",
       "  0.8287378549575806,\n",
       "  0.8014315366744995,\n",
       "  0.8775004148483276,\n",
       "  0.8822748064994812,\n",
       "  0.8253507018089294,\n",
       "  0.8145065903663635,\n",
       "  0.853269636631012,\n",
       "  0.7938454747200012,\n",
       "  0.8336151242256165,\n",
       "  0.8989198207855225,\n",
       "  0.8267922401428223,\n",
       "  0.8416689038276672,\n",
       "  0.900744616985321,\n",
       "  0.8222770690917969,\n",
       "  0.8382699489593506,\n",
       "  0.8690460324287415,\n",
       "  0.985227644443512,\n",
       "  0.8437349796295166,\n",
       "  0.8374765515327454,\n",
       "  0.8658615946769714,\n",
       "  0.8615731000900269,\n",
       "  0.8227400779724121,\n",
       "  0.9776397347450256,\n",
       "  0.8316153883934021,\n",
       "  0.8577132821083069,\n",
       "  0.9658995866775513,\n",
       "  0.785868227481842,\n",
       "  0.8613393902778625,\n",
       "  0.9824023246765137,\n",
       "  0.775701642036438,\n",
       "  0.8820206522941589,\n",
       "  0.8740351796150208,\n",
       "  0.8317671418190002,\n",
       "  0.8727688789367676,\n",
       "  0.8345943093299866,\n",
       "  0.7853578329086304,\n",
       "  0.9009689688682556,\n",
       "  0.8471279740333557,\n",
       "  0.8587766289710999,\n",
       "  0.8554733395576477,\n",
       "  0.8145288825035095,\n",
       "  0.8831660747528076,\n",
       "  0.8767507076263428,\n",
       "  0.8806585073471069,\n",
       "  0.9127898812294006,\n",
       "  1.0,\n",
       "  0.8550348877906799,\n",
       "  0.8228922486305237,\n",
       "  0.8422412872314453,\n",
       "  0.8367186188697815,\n",
       "  0.8930637240409851,\n",
       "  0.7981497049331665,\n",
       "  0.8382734060287476,\n",
       "  0.8290578722953796,\n",
       "  0.8857256174087524,\n",
       "  0.8529178500175476,\n",
       "  0.8576605916023254,\n",
       "  0.8323830962181091,\n",
       "  0.8487017154693604,\n",
       "  0.8792490363121033,\n",
       "  0.8720558881759644,\n",
       "  0.8771716952323914,\n",
       "  0.8439682126045227,\n",
       "  0.8444111943244934,\n",
       "  0.8426415920257568,\n",
       "  0.854590892791748,\n",
       "  0.7847833037376404,\n",
       "  0.965724527835846,\n",
       "  0.7949066758155823,\n",
       "  0.8181893825531006,\n",
       "  0.8403756022453308,\n",
       "  0.8665558099746704,\n",
       "  0.831110417842865,\n",
       "  0.8501197695732117,\n",
       "  0.9046780467033386,\n",
       "  0.8345373272895813,\n",
       "  0.9189465641975403,\n",
       "  0.9007930755615234,\n",
       "  0.8289925456047058,\n",
       "  1.0000001192092896,\n",
       "  0.8569580316543579,\n",
       "  0.8970338106155396,\n",
       "  0.9256972074508667,\n",
       "  0.8486872315406799,\n",
       "  0.8919126987457275,\n",
       "  0.8349452018737793,\n",
       "  0.841187596321106,\n",
       "  0.8692650198936462,\n",
       "  0.863418698310852,\n",
       "  0.8432427048683167,\n",
       "  1.0,\n",
       "  0.8965476155281067,\n",
       "  0.8660216331481934,\n",
       "  0.8131223320960999,\n",
       "  0.8766704201698303,\n",
       "  0.8421363830566406,\n",
       "  0.9070956707000732,\n",
       "  0.8487846851348877,\n",
       "  0.8534148931503296,\n",
       "  0.8748643398284912,\n",
       "  0.8165846467018127,\n",
       "  0.8582914471626282,\n",
       "  1.0,\n",
       "  0.8355923891067505,\n",
       "  0.8228768110275269,\n",
       "  0.9105766415596008,\n",
       "  0.811026394367218,\n",
       "  0.8780438899993896,\n",
       "  0.8334909081459045,\n",
       "  0.8886705636978149,\n",
       "  0.8732478618621826,\n",
       "  0.8143326044082642,\n",
       "  0.9169788956642151,\n",
       "  0.8378579020500183,\n",
       "  0.8689053058624268,\n",
       "  0.8512266874313354,\n",
       "  0.8980823755264282,\n",
       "  0.8752287030220032,\n",
       "  0.8304548263549805,\n",
       "  0.836479663848877,\n",
       "  0.8439193367958069,\n",
       "  0.8098425269126892,\n",
       "  0.8611745834350586,\n",
       "  0.8534207940101624,\n",
       "  0.8212040662765503,\n",
       "  0.8764553070068359,\n",
       "  0.8057917952537537,\n",
       "  0.8157431483268738,\n",
       "  0.8308189511299133,\n",
       "  0.82569819688797,\n",
       "  0.8439183831214905,\n",
       "  0.8401871919631958,\n",
       "  0.848796546459198,\n",
       "  0.827951192855835,\n",
       "  0.8242117166519165,\n",
       "  0.8386021256446838,\n",
       "  0.8478038311004639,\n",
       "  0.857979416847229,\n",
       "  0.8261750340461731,\n",
       "  0.7951989769935608,\n",
       "  0.8783385753631592,\n",
       "  0.826557457447052,\n",
       "  0.8541353940963745,\n",
       "  0.849847674369812,\n",
       "  0.7939414978027344,\n",
       "  0.9090077877044678,\n",
       "  0.9686133861541748,\n",
       "  0.9526352882385254,\n",
       "  0.8214235305786133,\n",
       "  0.822765588760376,\n",
       "  0.8546665906906128,\n",
       "  0.867048978805542,\n",
       "  0.8922525644302368,\n",
       "  0.8680754899978638,\n",
       "  0.8946712017059326,\n",
       "  0.9850558638572693,\n",
       "  0.8197267651557922,\n",
       "  0.9042361974716187,\n",
       "  0.8320050239562988,\n",
       "  0.8892378807067871,\n",
       "  0.876883327960968,\n",
       "  0.9709376096725464,\n",
       "  0.8068169355392456,\n",
       "  0.8745880722999573,\n",
       "  0.8932532072067261,\n",
       "  1.0,\n",
       "  0.807163417339325,\n",
       "  0.9873034954071045,\n",
       "  0.9999999403953552,\n",
       "  0.8604268431663513,\n",
       "  0.8290771245956421,\n",
       "  1.0,\n",
       "  0.8517138957977295,\n",
       "  0.8742432594299316,\n",
       "  0.8441351652145386,\n",
       "  0.8045679330825806,\n",
       "  0.8085777759552002,\n",
       "  0.8557013273239136,\n",
       "  0.8175679445266724,\n",
       "  0.8266522884368896,\n",
       "  0.8291947841644287,\n",
       "  0.8136647939682007,\n",
       "  0.8157783150672913,\n",
       "  0.8492376208305359,\n",
       "  0.8510894179344177,\n",
       "  0.8226914405822754,\n",
       "  0.8300879597663879,\n",
       "  0.8278945684432983,\n",
       "  0.8725736141204834,\n",
       "  0.8853504061698914,\n",
       "  0.8317465782165527,\n",
       "  0.8336992859840393,\n",
       "  0.8561223745346069,\n",
       "  0.9999999403953552,\n",
       "  0.8768839240074158,\n",
       "  0.7780327796936035,\n",
       "  0.8537847995758057,\n",
       "  0.8234719038009644,\n",
       "  0.8609693646430969,\n",
       "  0.819843053817749,\n",
       "  0.8943827748298645,\n",
       "  0.8667385578155518,\n",
       "  0.8558728694915771,\n",
       "  0.8310984373092651,\n",
       "  0.9607626795768738,\n",
       "  0.9271950721740723,\n",
       "  0.8909659385681152,\n",
       "  0.8491020798683167,\n",
       "  0.8904870748519897,\n",
       "  0.8046674728393555,\n",
       "  0.8528059124946594,\n",
       "  0.8583398461341858,\n",
       "  0.8128894567489624,\n",
       "  0.8120117783546448,\n",
       "  0.8294256925582886,\n",
       "  0.7959977388381958,\n",
       "  0.8609970808029175,\n",
       "  0.7946743369102478,\n",
       "  0.8027285933494568,\n",
       "  0.8311744332313538,\n",
       "  0.8192607760429382,\n",
       "  0.9011086821556091,\n",
       "  0.876828670501709,\n",
       "  0.8711692094802856,\n",
       "  0.7956855297088623,\n",
       "  0.8411847949028015,\n",
       "  0.800072193145752,\n",
       "  0.8465588688850403,\n",
       "  0.8766303658485413,\n",
       "  0.8374291062355042,\n",
       "  0.8182363510131836,\n",
       "  0.8616371154785156,\n",
       "  0.8772798180580139,\n",
       "  0.8834795951843262,\n",
       "  0.8059959411621094,\n",
       "  0.8488792181015015,\n",
       "  0.8023189902305603,\n",
       "  0.8368504047393799,\n",
       "  0.888045072555542,\n",
       "  0.8361740112304688,\n",
       "  0.884830892086029,\n",
       "  0.8220818638801575,\n",
       "  0.7903332710266113,\n",
       "  0.8392497301101685,\n",
       "  0.8592631816864014,\n",
       "  0.8533052802085876,\n",
       "  0.8147701025009155,\n",
       "  0.8031954765319824,\n",
       "  0.8186203837394714,\n",
       "  0.8203953504562378,\n",
       "  0.970073938369751,\n",
       "  0.8725842237472534,\n",
       "  0.8164974451065063,\n",
       "  0.8144553303718567,\n",
       "  0.8697044849395752,\n",
       "  0.8045626878738403,\n",
       "  0.8415200114250183,\n",
       "  0.8367668390274048,\n",
       "  0.8464717864990234,\n",
       "  0.926444411277771,\n",
       "  0.8888814449310303,\n",
       "  0.8189254403114319,\n",
       "  0.8175670504570007,\n",
       "  0.8717337250709534,\n",
       "  0.8289942741394043,\n",
       "  0.873540461063385,\n",
       "  0.7910815477371216,\n",
       "  0.8116432428359985,\n",
       "  0.8659883737564087,\n",
       "  0.8665942549705505,\n",
       "  0.8114984631538391,\n",
       "  0.923154890537262,\n",
       "  0.8605619668960571,\n",
       "  0.8240863084793091,\n",
       "  0.8550605773925781,\n",
       "  0.8317745923995972,\n",
       "  0.9026974439620972,\n",
       "  0.7992187142372131,\n",
       "  0.8448352813720703,\n",
       "  0.8205402493476868,\n",
       "  0.9664040207862854,\n",
       "  0.8579233884811401,\n",
       "  0.8052545189857483,\n",
       "  0.8525750637054443,\n",
       "  0.8619648218154907,\n",
       "  0.8126577734947205,\n",
       "  0.8736927509307861,\n",
       "  0.8433619141578674,\n",
       "  0.7964085936546326,\n",
       "  0.942328155040741,\n",
       "  0.8201661109924316,\n",
       "  0.8737239241600037,\n",
       "  0.8194794058799744,\n",
       "  0.9452393651008606,\n",
       "  0.8517048358917236,\n",
       "  0.8201887607574463,\n",
       "  0.8537710905075073,\n",
       "  0.7611386179924011,\n",
       "  0.8016223907470703,\n",
       "  0.91547030210495,\n",
       "  0.8689212203025818,\n",
       "  0.9151250720024109,\n",
       "  0.8598529100418091,\n",
       "  0.8403234481811523,\n",
       "  0.8951767086982727,\n",
       "  0.7671897411346436,\n",
       "  0.871669590473175,\n",
       "  0.8620996475219727,\n",
       "  1.0,\n",
       "  0.8753607273101807,\n",
       "  0.8816028237342834,\n",
       "  0.9963844418525696,\n",
       "  0.9110861420631409,\n",
       "  0.9039052128791809,\n",
       "  0.8808128237724304,\n",
       "  0.8549370169639587,\n",
       "  0.8699873685836792,\n",
       "  0.8504809737205505,\n",
       "  0.8041418790817261,\n",
       "  0.9815808534622192,\n",
       "  0.852074146270752,\n",
       "  0.8407034873962402,\n",
       "  0.8944803476333618,\n",
       "  0.8289704322814941,\n",
       "  0.8529152274131775,\n",
       "  0.840557873249054,\n",
       "  0.940285325050354,\n",
       "  0.895191490650177,\n",
       "  0.8502610921859741,\n",
       "  0.7755723595619202,\n",
       "  0.8847522735595703,\n",
       "  0.8299744129180908,\n",
       "  0.8481268286705017,\n",
       "  0.8717395067214966,\n",
       "  0.8364191651344299,\n",
       "  0.8102384805679321,\n",
       "  0.7666143178939819,\n",
       "  0.8477561473846436,\n",
       "  0.8427650928497314,\n",
       "  0.8257719278335571,\n",
       "  0.848232626914978,\n",
       "  0.865119218826294,\n",
       "  0.7716432809829712,\n",
       "  0.8074761033058167,\n",
       "  0.8633500933647156,\n",
       "  0.8738446831703186,\n",
       "  0.820625901222229,\n",
       "  0.811098039150238,\n",
       "  0.8066677451133728,\n",
       "  0.8523927927017212,\n",
       "  0.9806185960769653,\n",
       "  0.8831028938293457,\n",
       "  0.8080035448074341,\n",
       "  0.8383116126060486,\n",
       "  0.8053019046783447,\n",
       "  0.8173373341560364,\n",
       "  0.8081139326095581,\n",
       "  0.8497765064239502,\n",
       "  0.8023123741149902,\n",
       "  0.8210093379020691,\n",
       "  0.9896037578582764,\n",
       "  0.8413333296775818,\n",
       "  0.7976039052009583,\n",
       "  0.8601182103157043,\n",
       "  ...],\n",
       " 'recall': [0.8562126755714417,\n",
       "  0.9099026918411255,\n",
       "  0.8981591463088989,\n",
       "  0.8605943322181702,\n",
       "  0.8743959665298462,\n",
       "  0.816523015499115,\n",
       "  0.84824138879776,\n",
       "  0.8460538983345032,\n",
       "  0.8840341567993164,\n",
       "  0.8943216800689697,\n",
       "  0.9586883187294006,\n",
       "  0.8825989961624146,\n",
       "  0.962297260761261,\n",
       "  0.8814043998718262,\n",
       "  0.9203082323074341,\n",
       "  0.900426983833313,\n",
       "  0.8631940484046936,\n",
       "  0.8449813723564148,\n",
       "  0.8636018633842468,\n",
       "  0.8585518598556519,\n",
       "  0.9070894122123718,\n",
       "  0.8582221269607544,\n",
       "  0.9145902991294861,\n",
       "  0.96352219581604,\n",
       "  0.8669617176055908,\n",
       "  0.8208748698234558,\n",
       "  0.9275931715965271,\n",
       "  0.885083019733429,\n",
       "  0.8666978478431702,\n",
       "  0.8321343064308167,\n",
       "  0.8906875252723694,\n",
       "  0.8643894791603088,\n",
       "  0.8596367835998535,\n",
       "  0.9479220509529114,\n",
       "  0.8500295877456665,\n",
       "  0.8925061225891113,\n",
       "  0.8334086537361145,\n",
       "  0.8982859253883362,\n",
       "  0.8807242512702942,\n",
       "  0.9197853803634644,\n",
       "  0.8260413408279419,\n",
       "  0.8647254109382629,\n",
       "  0.8325793147087097,\n",
       "  0.8804036378860474,\n",
       "  0.864714503288269,\n",
       "  0.8856687545776367,\n",
       "  0.8516689538955688,\n",
       "  0.9618238210678101,\n",
       "  0.8770922422409058,\n",
       "  0.8791826963424683,\n",
       "  0.8628578186035156,\n",
       "  0.8857882618904114,\n",
       "  0.8758504390716553,\n",
       "  0.8969756364822388,\n",
       "  0.8457131385803223,\n",
       "  0.8474474549293518,\n",
       "  0.8258634805679321,\n",
       "  0.8557462692260742,\n",
       "  0.8780107498168945,\n",
       "  0.8538564443588257,\n",
       "  0.8872965574264526,\n",
       "  0.8645129203796387,\n",
       "  0.8473517298698425,\n",
       "  0.854430079460144,\n",
       "  0.8478403091430664,\n",
       "  0.9813158512115479,\n",
       "  0.8480645418167114,\n",
       "  0.8814710378646851,\n",
       "  0.921082615852356,\n",
       "  0.8964235782623291,\n",
       "  0.839483916759491,\n",
       "  0.9332648515701294,\n",
       "  0.920324444770813,\n",
       "  0.8686858415603638,\n",
       "  0.887866199016571,\n",
       "  0.8550252914428711,\n",
       "  0.896872878074646,\n",
       "  0.8631425499916077,\n",
       "  0.8841003179550171,\n",
       "  0.9279683828353882,\n",
       "  0.9440492391586304,\n",
       "  0.8246803283691406,\n",
       "  0.8694313764572144,\n",
       "  0.8360706567764282,\n",
       "  0.9289687871932983,\n",
       "  0.7938522100448608,\n",
       "  0.873602032661438,\n",
       "  0.9999999403953552,\n",
       "  0.9129451513290405,\n",
       "  0.8087131381034851,\n",
       "  0.883189857006073,\n",
       "  0.8862884044647217,\n",
       "  0.8510788679122925,\n",
       "  0.8339108824729919,\n",
       "  0.9260572791099548,\n",
       "  0.8676832318305969,\n",
       "  0.8884901404380798,\n",
       "  0.8547707200050354,\n",
       "  0.8778359293937683,\n",
       "  0.8715546131134033,\n",
       "  0.9147030711174011,\n",
       "  0.8647693991661072,\n",
       "  0.9431771039962769,\n",
       "  0.9181575775146484,\n",
       "  0.8523277044296265,\n",
       "  0.8698365092277527,\n",
       "  0.886333703994751,\n",
       "  0.9801298379898071,\n",
       "  0.8829919695854187,\n",
       "  0.948335587978363,\n",
       "  0.8368350267410278,\n",
       "  0.9649327397346497,\n",
       "  0.8463976979255676,\n",
       "  0.9068000912666321,\n",
       "  0.8784504532814026,\n",
       "  0.9693858623504639,\n",
       "  0.8647966384887695,\n",
       "  0.9937011003494263,\n",
       "  0.8755149841308594,\n",
       "  0.8758450746536255,\n",
       "  0.8650010228157043,\n",
       "  0.8368757963180542,\n",
       "  0.8870862722396851,\n",
       "  0.9128245115280151,\n",
       "  0.8486565351486206,\n",
       "  0.9178915619850159,\n",
       "  0.9237734079360962,\n",
       "  0.9183553457260132,\n",
       "  0.885409951210022,\n",
       "  0.9598128795623779,\n",
       "  0.9338443875312805,\n",
       "  0.8944856524467468,\n",
       "  0.9096999168395996,\n",
       "  0.8903679847717285,\n",
       "  0.8960279226303101,\n",
       "  0.8317659497261047,\n",
       "  0.8967123031616211,\n",
       "  0.8720329403877258,\n",
       "  0.8556071519851685,\n",
       "  0.9057980179786682,\n",
       "  0.8678575754165649,\n",
       "  0.8213139772415161,\n",
       "  0.8561829328536987,\n",
       "  0.8559994101524353,\n",
       "  0.8612386584281921,\n",
       "  0.7877739667892456,\n",
       "  0.9205031394958496,\n",
       "  0.8906857967376709,\n",
       "  0.8478285074234009,\n",
       "  0.9046964049339294,\n",
       "  0.8516128063201904,\n",
       "  0.8653585910797119,\n",
       "  0.8553368449211121,\n",
       "  0.7969582080841064,\n",
       "  0.8638280034065247,\n",
       "  0.8579947352409363,\n",
       "  0.8153088092803955,\n",
       "  0.9020421504974365,\n",
       "  0.8777015209197998,\n",
       "  0.8139989376068115,\n",
       "  0.8845067024230957,\n",
       "  0.8727034330368042,\n",
       "  0.874165415763855,\n",
       "  0.8969695568084717,\n",
       "  0.8819882869720459,\n",
       "  0.876980185508728,\n",
       "  0.8631120324134827,\n",
       "  0.8993231058120728,\n",
       "  0.8615654110908508,\n",
       "  0.8041702508926392,\n",
       "  0.888903796672821,\n",
       "  0.8613354563713074,\n",
       "  0.8530805110931396,\n",
       "  0.8871891498565674,\n",
       "  0.8839991688728333,\n",
       "  0.8904626965522766,\n",
       "  0.8702437877655029,\n",
       "  0.8744999766349792,\n",
       "  0.8430576324462891,\n",
       "  0.8653028011322021,\n",
       "  0.9627458453178406,\n",
       "  0.9402754306793213,\n",
       "  0.8520747423171997,\n",
       "  0.919624388217926,\n",
       "  0.8759171366691589,\n",
       "  0.8981326818466187,\n",
       "  0.889668881893158,\n",
       "  0.9430011510848999,\n",
       "  0.8649209141731262,\n",
       "  0.9526938199996948,\n",
       "  0.8955968618392944,\n",
       "  0.8850512504577637,\n",
       "  0.952556848526001,\n",
       "  0.9170650839805603,\n",
       "  0.8562377691268921,\n",
       "  0.8350450396537781,\n",
       "  0.9129216074943542,\n",
       "  0.9344885349273682,\n",
       "  0.9868550300598145,\n",
       "  0.8805487155914307,\n",
       "  0.8524316549301147,\n",
       "  0.8622552752494812,\n",
       "  0.9038867950439453,\n",
       "  0.894849956035614,\n",
       "  0.8552356958389282,\n",
       "  0.8465787172317505,\n",
       "  0.8748927116394043,\n",
       "  0.8337308764457703,\n",
       "  0.8890277147293091,\n",
       "  0.8823561668395996,\n",
       "  0.8427073359489441,\n",
       "  0.8978361487388611,\n",
       "  0.8488979935646057,\n",
       "  0.8366233110427856,\n",
       "  0.843845546245575,\n",
       "  0.8644586801528931,\n",
       "  0.8666797876358032,\n",
       "  0.8368830680847168,\n",
       "  0.9854742884635925,\n",
       "  0.8615093231201172,\n",
       "  0.9874700903892517,\n",
       "  0.8638818860054016,\n",
       "  0.890022337436676,\n",
       "  0.8417191505432129,\n",
       "  0.9010485410690308,\n",
       "  0.9093087911605835,\n",
       "  0.9145716428756714,\n",
       "  0.8667294383049011,\n",
       "  0.8677761554718018,\n",
       "  0.9068548679351807,\n",
       "  0.8689182996749878,\n",
       "  0.8690837025642395,\n",
       "  0.9073832035064697,\n",
       "  0.9741895794868469,\n",
       "  0.9609667062759399,\n",
       "  0.8958536386489868,\n",
       "  0.7975116968154907,\n",
       "  0.9323043823242188,\n",
       "  0.9812214970588684,\n",
       "  0.8432900905609131,\n",
       "  0.8862255811691284,\n",
       "  0.873023271560669,\n",
       "  0.8999999165534973,\n",
       "  0.8265122175216675,\n",
       "  0.9331322908401489,\n",
       "  0.8728153109550476,\n",
       "  0.8272508382797241,\n",
       "  0.9865917563438416,\n",
       "  0.8841615319252014,\n",
       "  0.8798169493675232,\n",
       "  0.8458819389343262,\n",
       "  0.8204410076141357,\n",
       "  0.8601902723312378,\n",
       "  0.8393051028251648,\n",
       "  0.8757700324058533,\n",
       "  0.841792643070221,\n",
       "  0.8471163511276245,\n",
       "  0.8916438221931458,\n",
       "  1.0,\n",
       "  0.8130379915237427,\n",
       "  0.84281325340271,\n",
       "  0.8243687152862549,\n",
       "  0.9489604830741882,\n",
       "  0.8666658401489258,\n",
       "  0.9357724189758301,\n",
       "  0.8376995325088501,\n",
       "  0.8792325258255005,\n",
       "  0.9539778828620911,\n",
       "  0.8613359928131104,\n",
       "  0.8922042846679688,\n",
       "  0.9006779789924622,\n",
       "  0.8305047750473022,\n",
       "  0.8232652544975281,\n",
       "  0.8476101756095886,\n",
       "  0.8920892477035522,\n",
       "  0.8540409803390503,\n",
       "  0.8708282709121704,\n",
       "  0.8540056347846985,\n",
       "  0.8511929512023926,\n",
       "  0.9180580973625183,\n",
       "  0.9296696782112122,\n",
       "  0.8887841701507568,\n",
       "  0.8990967273712158,\n",
       "  0.8566817045211792,\n",
       "  0.8927816152572632,\n",
       "  0.9148369431495667,\n",
       "  0.9451742768287659,\n",
       "  0.925567626953125,\n",
       "  0.8607804775238037,\n",
       "  0.8817719221115112,\n",
       "  0.967719554901123,\n",
       "  0.9194082021713257,\n",
       "  0.8921170234680176,\n",
       "  0.9999999403953552,\n",
       "  0.851359486579895,\n",
       "  0.9523482918739319,\n",
       "  0.7893221378326416,\n",
       "  0.9089387059211731,\n",
       "  0.8345480561256409,\n",
       "  0.9453859329223633,\n",
       "  0.9515699148178101,\n",
       "  0.8478636741638184,\n",
       "  0.9883500933647156,\n",
       "  0.9774800539016724,\n",
       "  0.8248952627182007,\n",
       "  0.8844985365867615,\n",
       "  0.8413138389587402,\n",
       "  0.846085250377655,\n",
       "  0.8366976380348206,\n",
       "  0.9128322005271912,\n",
       "  0.896515965461731,\n",
       "  0.9999999403953552,\n",
       "  0.9267658591270447,\n",
       "  0.8139663934707642,\n",
       "  0.838231086730957,\n",
       "  0.87990403175354,\n",
       "  0.8770617246627808,\n",
       "  0.8480096459388733,\n",
       "  0.8395431041717529,\n",
       "  0.8834308981895447,\n",
       "  0.8877682089805603,\n",
       "  0.8906799554824829,\n",
       "  0.8750893473625183,\n",
       "  0.8706110715866089,\n",
       "  0.8531521558761597,\n",
       "  0.8366863131523132,\n",
       "  0.8374488949775696,\n",
       "  0.8492580652236938,\n",
       "  0.9348092675209045,\n",
       "  0.8964735269546509,\n",
       "  0.9087215662002563,\n",
       "  0.8428866267204285,\n",
       "  0.9514647722244263,\n",
       "  0.9006308913230896,\n",
       "  0.855169951915741,\n",
       "  0.9498113989830017,\n",
       "  0.8088328242301941,\n",
       "  0.8124953508377075,\n",
       "  0.8756101131439209,\n",
       "  0.8733687400817871,\n",
       "  0.9252800345420837,\n",
       "  0.8529237508773804,\n",
       "  0.8911242485046387,\n",
       "  0.8634470701217651,\n",
       "  0.852744460105896,\n",
       "  0.8995762467384338,\n",
       "  0.8557877540588379,\n",
       "  0.8045958280563354,\n",
       "  0.899900496006012,\n",
       "  0.8202612996101379,\n",
       "  0.8985317349433899,\n",
       "  0.8820301294326782,\n",
       "  0.8563442826271057,\n",
       "  0.8522623181343079,\n",
       "  0.8570203185081482,\n",
       "  0.8882877826690674,\n",
       "  0.8690163493156433,\n",
       "  0.8839816451072693,\n",
       "  1.0,\n",
       "  0.8258448839187622,\n",
       "  0.8675158023834229,\n",
       "  0.8170210123062134,\n",
       "  0.8607455492019653,\n",
       "  0.9200406074523926,\n",
       "  0.8201799392700195,\n",
       "  0.8962412476539612,\n",
       "  0.8662742972373962,\n",
       "  0.938422679901123,\n",
       "  0.9283322691917419,\n",
       "  0.8833630084991455,\n",
       "  0.9326233863830566,\n",
       "  0.8587795495986938,\n",
       "  0.8977831602096558,\n",
       "  0.8773410320281982,\n",
       "  0.8556081056594849,\n",
       "  0.9840723276138306,\n",
       "  0.8937994241714478,\n",
       "  0.9407160878181458,\n",
       "  0.8894984722137451,\n",
       "  0.8872154951095581,\n",
       "  0.9279724955558777,\n",
       "  0.8676137328147888,\n",
       "  0.8976004719734192,\n",
       "  0.8898004293441772,\n",
       "  0.9525635242462158,\n",
       "  0.8389657735824585,\n",
       "  0.9391036629676819,\n",
       "  0.8451555371284485,\n",
       "  0.8746243715286255,\n",
       "  0.8629124760627747,\n",
       "  0.8653184771537781,\n",
       "  0.8854022026062012,\n",
       "  0.8590085506439209,\n",
       "  0.8655431866645813,\n",
       "  0.8600826263427734,\n",
       "  0.8649201393127441,\n",
       "  0.9687681198120117,\n",
       "  0.8512721061706543,\n",
       "  0.910888135433197,\n",
       "  0.8758023977279663,\n",
       "  0.8564913272857666,\n",
       "  0.7948862910270691,\n",
       "  0.8746399283409119,\n",
       "  0.8467811942100525,\n",
       "  0.8661030530929565,\n",
       "  0.8338285684585571,\n",
       "  0.8820387125015259,\n",
       "  0.8802984356880188,\n",
       "  0.9864768981933594,\n",
       "  0.9234786033630371,\n",
       "  0.8637381196022034,\n",
       "  0.8121019601821899,\n",
       "  0.8684288263320923,\n",
       "  0.8446309566497803,\n",
       "  0.856279730796814,\n",
       "  0.86168372631073,\n",
       "  0.8857016563415527,\n",
       "  0.8415955305099487,\n",
       "  0.8791059255599976,\n",
       "  0.8757714033126831,\n",
       "  0.8794664144515991,\n",
       "  0.8663196563720703,\n",
       "  0.8128106594085693,\n",
       "  0.8534974455833435,\n",
       "  0.8532466292381287,\n",
       "  0.8263021111488342,\n",
       "  1.0,\n",
       "  0.8572731614112854,\n",
       "  0.9397609233856201,\n",
       "  0.8218281865119934,\n",
       "  0.8636608719825745,\n",
       "  0.9114938378334045,\n",
       "  0.8753569722175598,\n",
       "  0.8507236242294312,\n",
       "  0.9009212255477905,\n",
       "  0.9084557294845581,\n",
       "  0.8425055146217346,\n",
       "  0.8622241020202637,\n",
       "  0.8703526854515076,\n",
       "  0.8607956171035767,\n",
       "  0.8404494524002075,\n",
       "  0.9041856527328491,\n",
       "  0.8906382918357849,\n",
       "  0.863922119140625,\n",
       "  0.8798608779907227,\n",
       "  0.900393009185791,\n",
       "  0.8510938286781311,\n",
       "  0.9532748460769653,\n",
       "  0.8656654953956604,\n",
       "  0.7775649428367615,\n",
       "  0.9348563551902771,\n",
       "  0.8871060013771057,\n",
       "  0.9202280640602112,\n",
       "  0.868357241153717,\n",
       "  0.8899945616722107,\n",
       "  0.870954155921936,\n",
       "  0.8633344173431396,\n",
       "  0.8405421376228333,\n",
       "  0.8486084938049316,\n",
       "  0.8755158185958862,\n",
       "  0.8827498555183411,\n",
       "  0.8695907592773438,\n",
       "  0.8337867856025696,\n",
       "  0.9108437299728394,\n",
       "  0.8599294424057007,\n",
       "  0.8634945154190063,\n",
       "  0.837656557559967,\n",
       "  0.8856802582740784,\n",
       "  0.8461583852767944,\n",
       "  0.8859302997589111,\n",
       "  0.8447142839431763,\n",
       "  0.9313738942146301,\n",
       "  0.8159926533699036,\n",
       "  0.9724150896072388,\n",
       "  0.8976117968559265,\n",
       "  0.8718934059143066,\n",
       "  0.8438236713409424,\n",
       "  0.8971667289733887,\n",
       "  0.823352038860321,\n",
       "  0.8698577284812927,\n",
       "  0.881233274936676,\n",
       "  0.9749952554702759,\n",
       "  0.8646458387374878,\n",
       "  0.8952247500419617,\n",
       "  0.8545911908149719,\n",
       "  0.8232526779174805,\n",
       "  0.8611513376235962,\n",
       "  0.8666343688964844,\n",
       "  0.8059873580932617,\n",
       "  0.9863231778144836,\n",
       "  0.876417338848114,\n",
       "  0.8490456342697144,\n",
       "  0.8317304849624634,\n",
       "  0.9064713716506958,\n",
       "  0.8942946791648865,\n",
       "  0.8943533301353455,\n",
       "  0.9010025262832642,\n",
       "  0.8419197797775269,\n",
       "  0.8938140273094177,\n",
       "  0.9285072684288025,\n",
       "  0.7769016027450562,\n",
       "  0.9088793992996216,\n",
       "  0.9390438795089722,\n",
       "  0.9999998807907104,\n",
       "  0.8869203925132751,\n",
       "  0.8422703146934509,\n",
       "  0.869379460811615,\n",
       "  0.8763379454612732,\n",
       "  0.863368034362793,\n",
       "  0.9422932863235474,\n",
       "  0.8632344603538513,\n",
       "  0.8987288475036621,\n",
       "  0.8826166391372681,\n",
       "  0.8424946069717407,\n",
       "  0.8514294028282166,\n",
       "  0.8864222764968872,\n",
       "  0.8616045117378235,\n",
       "  0.8816497325897217,\n",
       "  1.0000001192092896,\n",
       "  0.8502708673477173,\n",
       "  0.8756515979766846,\n",
       "  0.8956594467163086,\n",
       "  0.8627182841300964,\n",
       "  0.8437114357948303,\n",
       "  0.842658281326294,\n",
       "  0.9360202550888062,\n",
       "  0.8648765087127686,\n",
       "  0.8398584127426147,\n",
       "  0.9196164608001709,\n",
       "  0.8695610165596008,\n",
       "  0.832909882068634,\n",
       "  0.851421058177948,\n",
       "  0.9346667528152466,\n",
       "  0.8276447057723999,\n",
       "  0.9693158864974976,\n",
       "  0.8908960223197937,\n",
       "  0.856022834777832,\n",
       "  0.8400470018386841,\n",
       "  0.902876615524292,\n",
       "  0.8107366561889648,\n",
       "  0.8486679792404175,\n",
       "  0.9319288730621338,\n",
       "  0.8979308605194092,\n",
       "  0.8755520582199097,\n",
       "  0.8798486590385437,\n",
       "  0.9487330913543701,\n",
       "  0.9137909412384033,\n",
       "  0.8671360015869141,\n",
       "  0.8610990047454834,\n",
       "  0.9176254272460938,\n",
       "  0.8563148975372314,\n",
       "  0.9546384811401367,\n",
       "  0.8727526664733887,\n",
       "  0.8451892137527466,\n",
       "  0.8532589673995972,\n",
       "  0.9334508776664734,\n",
       "  0.9200360774993896,\n",
       "  0.8203331232070923,\n",
       "  0.909830629825592,\n",
       "  0.8584776520729065,\n",
       "  0.847967267036438,\n",
       "  0.8950051665306091,\n",
       "  0.8931017518043518,\n",
       "  0.9036573171615601,\n",
       "  0.9893696308135986,\n",
       "  0.8352250456809998,\n",
       "  0.8998019099235535,\n",
       "  0.8580220937728882,\n",
       "  0.8811302185058594,\n",
       "  0.9087875485420227,\n",
       "  0.9081373810768127,\n",
       "  0.8998731374740601,\n",
       "  0.8667640686035156,\n",
       "  0.7893534898757935,\n",
       "  0.8396288156509399,\n",
       "  0.8876171708106995,\n",
       "  0.8519839644432068,\n",
       "  0.8590748906135559,\n",
       "  0.874634861946106,\n",
       "  0.8256949186325073,\n",
       "  0.8440849781036377,\n",
       "  0.9226188063621521,\n",
       "  0.915558397769928,\n",
       "  0.8479418158531189,\n",
       "  0.8913190960884094,\n",
       "  0.8318825960159302,\n",
       "  0.8757546544075012,\n",
       "  0.8888388872146606,\n",
       "  0.9182530641555786,\n",
       "  0.9515692591667175,\n",
       "  0.8291341066360474,\n",
       "  0.8443148136138916,\n",
       "  0.9670782089233398,\n",
       "  0.8020958304405212,\n",
       "  0.8234061002731323,\n",
       "  0.8508589863777161,\n",
       "  0.8550639152526855,\n",
       "  0.8613395690917969,\n",
       "  0.8532534241676331,\n",
       "  0.8819468021392822,\n",
       "  0.9471573829650879,\n",
       "  0.8895403146743774,\n",
       "  0.9585056304931641,\n",
       "  0.8045012354850769,\n",
       "  0.8100984692573547,\n",
       "  0.98247891664505,\n",
       "  0.8581269383430481,\n",
       "  0.8847111463546753,\n",
       "  0.949151337146759,\n",
       "  0.8855088353157043,\n",
       "  0.916597843170166,\n",
       "  0.9063277244567871,\n",
       "  0.8901311755180359,\n",
       "  0.9158292412757874,\n",
       "  0.9999999403953552,\n",
       "  0.859340250492096,\n",
       "  0.8049622774124146,\n",
       "  0.9255098104476929,\n",
       "  0.8561249375343323,\n",
       "  0.9018360376358032,\n",
       "  0.9228577017784119,\n",
       "  0.8861441016197205,\n",
       "  0.8496759533882141,\n",
       "  0.8710359930992126,\n",
       "  0.8874181509017944,\n",
       "  0.9976356625556946,\n",
       "  0.8818780779838562,\n",
       "  0.8933262228965759,\n",
       "  0.8137499690055847,\n",
       "  0.880514919757843,\n",
       "  0.8418357372283936,\n",
       "  0.8885263204574585,\n",
       "  0.9289499521255493,\n",
       "  0.8743444085121155,\n",
       "  1.0,\n",
       "  0.8821582794189453,\n",
       "  0.8791743516921997,\n",
       "  0.8076404929161072,\n",
       "  0.857998251914978,\n",
       "  0.9183317422866821,\n",
       "  0.9050357937812805,\n",
       "  0.8324111104011536,\n",
       "  0.8348441123962402,\n",
       "  0.8135662078857422,\n",
       "  0.8356834650039673,\n",
       "  0.9156392216682434,\n",
       "  0.84934401512146,\n",
       "  0.8597165942192078,\n",
       "  0.9851870536804199,\n",
       "  0.8521549701690674,\n",
       "  0.8985304236412048,\n",
       "  0.8700590133666992,\n",
       "  0.931729793548584,\n",
       "  0.8543214797973633,\n",
       "  0.8443745970726013,\n",
       "  0.9193134307861328,\n",
       "  0.9158142805099487,\n",
       "  0.8747752904891968,\n",
       "  0.9286409020423889,\n",
       "  0.8298825025558472,\n",
       "  0.8855347633361816,\n",
       "  0.9733803272247314,\n",
       "  0.8942318558692932,\n",
       "  0.8814243078231812,\n",
       "  0.9865880608558655,\n",
       "  0.8508895039558411,\n",
       "  0.9358363747596741,\n",
       "  0.868893027305603,\n",
       "  0.891427218914032,\n",
       "  0.8680529594421387,\n",
       "  0.8784940242767334,\n",
       "  0.8898923397064209,\n",
       "  0.9103307127952576,\n",
       "  0.879890501499176,\n",
       "  0.857664167881012,\n",
       "  0.898189902305603,\n",
       "  0.9076384902000427,\n",
       "  0.8857296705245972,\n",
       "  0.9084847569465637,\n",
       "  0.9782474040985107,\n",
       "  0.9465956687927246,\n",
       "  1.0,\n",
       "  0.8986101150512695,\n",
       "  0.784305989742279,\n",
       "  0.8963305354118347,\n",
       "  0.8518714904785156,\n",
       "  0.8726047873497009,\n",
       "  0.8575753569602966,\n",
       "  0.8524510264396667,\n",
       "  0.8650315999984741,\n",
       "  0.9020224809646606,\n",
       "  0.8658581376075745,\n",
       "  0.8702076077461243,\n",
       "  0.8894529938697815,\n",
       "  0.8412013649940491,\n",
       "  0.9074300527572632,\n",
       "  0.8977330923080444,\n",
       "  0.9598585963249207,\n",
       "  0.8644992709159851,\n",
       "  0.8611713647842407,\n",
       "  0.8615075945854187,\n",
       "  0.8524653911590576,\n",
       "  0.8687363862991333,\n",
       "  0.91889888048172,\n",
       "  0.8676545023918152,\n",
       "  0.9057886004447937,\n",
       "  0.86443692445755,\n",
       "  0.8780340552330017,\n",
       "  0.8490971326828003,\n",
       "  0.8974491357803345,\n",
       "  0.8840848803520203,\n",
       "  0.8563175201416016,\n",
       "  0.9618152379989624,\n",
       "  0.8704205751419067,\n",
       "  0.828736424446106,\n",
       "  1.0000001192092896,\n",
       "  0.864511251449585,\n",
       "  0.8920358419418335,\n",
       "  0.9701740145683289,\n",
       "  0.8002786040306091,\n",
       "  0.8951615691184998,\n",
       "  0.8658633232116699,\n",
       "  0.907359778881073,\n",
       "  0.8950952291488647,\n",
       "  0.8854156136512756,\n",
       "  0.8551584482192993,\n",
       "  1.0,\n",
       "  0.8723616600036621,\n",
       "  0.9469640254974365,\n",
       "  0.7662588357925415,\n",
       "  0.8829452395439148,\n",
       "  0.8691210150718689,\n",
       "  0.8698379993438721,\n",
       "  0.8316107988357544,\n",
       "  0.8525794744491577,\n",
       "  0.9219978451728821,\n",
       "  0.8842744827270508,\n",
       "  0.8376041650772095,\n",
       "  1.0,\n",
       "  0.8468363881111145,\n",
       "  0.8350337743759155,\n",
       "  0.9561070203781128,\n",
       "  0.7941015958786011,\n",
       "  0.911119818687439,\n",
       "  0.8766820430755615,\n",
       "  0.8577938079833984,\n",
       "  0.884136438369751,\n",
       "  0.8200595378875732,\n",
       "  0.9263380765914917,\n",
       "  0.8533732891082764,\n",
       "  0.8632413148880005,\n",
       "  0.8701256513595581,\n",
       "  0.9265254735946655,\n",
       "  0.8925803899765015,\n",
       "  0.8610613346099854,\n",
       "  0.9365806579589844,\n",
       "  0.8502277135848999,\n",
       "  0.8397012948989868,\n",
       "  0.8805267810821533,\n",
       "  0.894451916217804,\n",
       "  0.8207827210426331,\n",
       "  0.8817487359046936,\n",
       "  0.8719246983528137,\n",
       "  0.8656086921691895,\n",
       "  0.8726950287818909,\n",
       "  0.8906541466712952,\n",
       "  0.8120901584625244,\n",
       "  0.866211473941803,\n",
       "  0.8041810989379883,\n",
       "  0.8340792655944824,\n",
       "  0.8567482233047485,\n",
       "  0.8793479800224304,\n",
       "  0.8818814754486084,\n",
       "  0.8624112606048584,\n",
       "  0.9096830487251282,\n",
       "  0.8083913922309875,\n",
       "  0.9085737466812134,\n",
       "  0.8269889950752258,\n",
       "  0.8847692608833313,\n",
       "  0.882605791091919,\n",
       "  0.8234821557998657,\n",
       "  0.9371671676635742,\n",
       "  0.9888142347335815,\n",
       "  0.9485490322113037,\n",
       "  0.8653730750083923,\n",
       "  0.7802314162254333,\n",
       "  0.8674265146255493,\n",
       "  0.8568873405456543,\n",
       "  0.8895608186721802,\n",
       "  0.9056023359298706,\n",
       "  0.894813597202301,\n",
       "  0.973568856716156,\n",
       "  0.8008547425270081,\n",
       "  0.9894532561302185,\n",
       "  0.9754639863967896,\n",
       "  0.8639737367630005,\n",
       "  0.857572615146637,\n",
       "  0.9205185174942017,\n",
       "  0.8802234530448914,\n",
       "  0.9070991277694702,\n",
       "  0.9220204949378967,\n",
       "  1.0,\n",
       "  0.8386319279670715,\n",
       "  0.9779753684997559,\n",
       "  0.9999999403953552,\n",
       "  0.857738196849823,\n",
       "  0.8734750747680664,\n",
       "  1.0,\n",
       "  0.837996244430542,\n",
       "  0.8752920627593994,\n",
       "  0.852511465549469,\n",
       "  0.8158094882965088,\n",
       "  0.8683167695999146,\n",
       "  0.8349905014038086,\n",
       "  0.8562606573104858,\n",
       "  0.8401801586151123,\n",
       "  0.8704313039779663,\n",
       "  0.7591618299484253,\n",
       "  0.841079592704773,\n",
       "  0.9035434722900391,\n",
       "  0.891701340675354,\n",
       "  0.8871325254440308,\n",
       "  0.8568873405456543,\n",
       "  0.9249231815338135,\n",
       "  0.8976183533668518,\n",
       "  0.8719689249992371,\n",
       "  0.8243469595909119,\n",
       "  0.8655210733413696,\n",
       "  0.8337052464485168,\n",
       "  0.9999999403953552,\n",
       "  0.9098179936408997,\n",
       "  0.8751194477081299,\n",
       "  0.8730005621910095,\n",
       "  0.8088817596435547,\n",
       "  0.9190342426300049,\n",
       "  0.8699204921722412,\n",
       "  0.9109223484992981,\n",
       "  0.8711587190628052,\n",
       "  0.8686094284057617,\n",
       "  0.818255603313446,\n",
       "  0.9397739171981812,\n",
       "  0.9855726957321167,\n",
       "  0.8960864543914795,\n",
       "  0.9239559769630432,\n",
       "  0.9683141708374023,\n",
       "  0.9147260189056396,\n",
       "  0.9482272863388062,\n",
       "  0.8770325183868408,\n",
       "  0.8408855199813843,\n",
       "  0.8382265567779541,\n",
       "  0.8519791960716248,\n",
       "  0.8037188053131104,\n",
       "  0.8295124769210815,\n",
       "  0.7807523012161255,\n",
       "  0.8476580381393433,\n",
       "  0.8508253693580627,\n",
       "  0.8645941615104675,\n",
       "  0.8955571055412292,\n",
       "  0.8919124603271484,\n",
       "  0.8670254349708557,\n",
       "  0.8573520183563232,\n",
       "  0.8664349317550659,\n",
       "  0.8733298182487488,\n",
       "  0.8948915600776672,\n",
       "  0.8759390711784363,\n",
       "  0.8395805954933167,\n",
       "  0.8400442600250244,\n",
       "  0.878838837146759,\n",
       "  0.9053218960762024,\n",
       "  0.8948878049850464,\n",
       "  0.8792741298675537,\n",
       "  0.9148203134536743,\n",
       "  0.8031884431838989,\n",
       "  0.8874531388282776,\n",
       "  0.9028784036636353,\n",
       "  0.8978659510612488,\n",
       "  0.8550748825073242,\n",
       "  0.8334828019142151,\n",
       "  0.8267866969108582,\n",
       "  0.8247435092926025,\n",
       "  0.869199275970459,\n",
       "  0.906677782535553,\n",
       "  0.8839819431304932,\n",
       "  0.8071427345275879,\n",
       "  0.9032739400863647,\n",
       "  0.8136448264122009,\n",
       "  0.9811477661132812,\n",
       "  0.836376428604126,\n",
       "  0.7734789252281189,\n",
       "  0.8193942904472351,\n",
       "  0.9033181071281433,\n",
       "  0.8559242486953735,\n",
       "  0.8991837501525879,\n",
       "  0.8690690994262695,\n",
       "  0.8547122478485107,\n",
       "  0.8798229694366455,\n",
       "  0.8930922746658325,\n",
       "  0.8251239657402039,\n",
       "  0.8682591915130615,\n",
       "  0.9011644124984741,\n",
       "  0.8410134315490723,\n",
       "  0.8551182746887207,\n",
       "  0.8126585483551025,\n",
       "  0.8898485898971558,\n",
       "  0.8423725366592407,\n",
       "  0.8461489677429199,\n",
       "  0.8594770431518555,\n",
       "  0.9080460667610168,\n",
       "  0.827997624874115,\n",
       "  0.869107186794281,\n",
       "  0.8582090735435486,\n",
       "  0.8971683979034424,\n",
       "  0.8858928680419922,\n",
       "  0.8683875799179077,\n",
       "  0.8926242589950562,\n",
       "  0.8750207424163818,\n",
       "  0.9042425155639648,\n",
       "  0.8893901109695435,\n",
       "  0.8504210710525513,\n",
       "  0.8778516054153442,\n",
       "  0.858458399772644,\n",
       "  0.8705053925514221,\n",
       "  0.8929879069328308,\n",
       "  0.8947084546089172,\n",
       "  0.8152860403060913,\n",
       "  0.921420156955719,\n",
       "  0.8525827527046204,\n",
       "  0.8979555368423462,\n",
       "  0.8372384905815125,\n",
       "  0.9710854887962341,\n",
       "  0.9097420573234558,\n",
       "  0.8601686358451843,\n",
       "  0.8599495887756348,\n",
       "  0.8446228504180908,\n",
       "  0.857288658618927,\n",
       "  0.9840803146362305,\n",
       "  0.862844705581665,\n",
       "  0.9873305559158325,\n",
       "  0.9310011267662048,\n",
       "  0.910264253616333,\n",
       "  0.8812939524650574,\n",
       "  0.8852753639221191,\n",
       "  0.8452286720275879,\n",
       "  0.8720355033874512,\n",
       "  1.0,\n",
       "  0.8949123024940491,\n",
       "  0.9003032445907593,\n",
       "  0.9961069226264954,\n",
       "  0.9902627468109131,\n",
       "  0.9015172123908997,\n",
       "  0.8953314423561096,\n",
       "  0.8343319296836853,\n",
       "  0.8683379292488098,\n",
       "  0.8398593664169312,\n",
       "  0.8312860727310181,\n",
       "  0.9898403286933899,\n",
       "  0.8649556636810303,\n",
       "  0.8961220979690552,\n",
       "  0.8960174918174744,\n",
       "  0.8740552663803101,\n",
       "  0.9395372271537781,\n",
       "  0.843436598777771,\n",
       "  0.9188421964645386,\n",
       "  0.8992661237716675,\n",
       "  0.8523989319801331,\n",
       "  0.8643176555633545,\n",
       "  0.8528945446014404,\n",
       "  0.876319944858551,\n",
       "  0.8629640936851501,\n",
       "  0.8645011782646179,\n",
       "  0.8769012093544006,\n",
       "  0.8633263111114502,\n",
       "  0.8254338502883911,\n",
       "  0.8914812207221985,\n",
       "  0.9112275838851929,\n",
       "  0.8719971179962158,\n",
       "  0.8546415567398071,\n",
       "  0.8575053811073303,\n",
       "  0.9234320521354675,\n",
       "  0.9050445556640625,\n",
       "  0.8857448101043701,\n",
       "  0.9041460156440735,\n",
       "  0.812738835811615,\n",
       "  0.8993723392486572,\n",
       "  0.8332417607307434,\n",
       "  0.8987330794334412,\n",
       "  0.9730829000473022,\n",
       "  0.9005906581878662,\n",
       "  0.8435656428337097,\n",
       "  0.9085971117019653,\n",
       "  0.8487629890441895,\n",
       "  0.8452954292297363,\n",
       "  0.8653932809829712,\n",
       "  0.8481610417366028,\n",
       "  0.8407527804374695,\n",
       "  0.8798848986625671,\n",
       "  0.8623380661010742,\n",
       "  0.8572734594345093,\n",
       "  0.8450102806091309,\n",
       "  0.9236766695976257,\n",
       "  ...],\n",
       " 'f1': [0.836089015007019,\n",
       "  0.852530837059021,\n",
       "  0.8762206435203552,\n",
       "  0.8368500471115112,\n",
       "  0.8687484860420227,\n",
       "  0.8072596788406372,\n",
       "  0.8765089511871338,\n",
       "  0.8368763327598572,\n",
       "  0.8723433613777161,\n",
       "  0.8739153146743774,\n",
       "  0.9058845043182373,\n",
       "  0.8780691623687744,\n",
       "  0.9099521636962891,\n",
       "  0.8572510480880737,\n",
       "  0.9121316075325012,\n",
       "  0.8466978669166565,\n",
       "  0.8626823425292969,\n",
       "  0.8380419015884399,\n",
       "  0.8634289503097534,\n",
       "  0.8616670966148376,\n",
       "  0.8561880588531494,\n",
       "  0.8547549843788147,\n",
       "  0.913814902305603,\n",
       "  0.9334301352500916,\n",
       "  0.8291048407554626,\n",
       "  0.8367734551429749,\n",
       "  0.924726665019989,\n",
       "  0.8657793998718262,\n",
       "  0.8511435389518738,\n",
       "  0.8368195295333862,\n",
       "  0.8967851400375366,\n",
       "  0.8488585352897644,\n",
       "  0.8557892441749573,\n",
       "  0.932706356048584,\n",
       "  0.8303242325782776,\n",
       "  0.8768327832221985,\n",
       "  0.8432344794273376,\n",
       "  0.8332921266555786,\n",
       "  0.8680108189582825,\n",
       "  0.891564130783081,\n",
       "  0.8438816070556641,\n",
       "  0.8395897746086121,\n",
       "  0.8260663747787476,\n",
       "  0.8639822602272034,\n",
       "  0.8527190685272217,\n",
       "  0.8820748329162598,\n",
       "  0.829098105430603,\n",
       "  0.9494322538375854,\n",
       "  0.8440244793891907,\n",
       "  0.887122392654419,\n",
       "  0.8411685824394226,\n",
       "  0.853643000125885,\n",
       "  0.8610864877700806,\n",
       "  0.8779276609420776,\n",
       "  0.8304046392440796,\n",
       "  0.8393126726150513,\n",
       "  0.8246697187423706,\n",
       "  0.8359156250953674,\n",
       "  0.8365344405174255,\n",
       "  0.8520750999450684,\n",
       "  0.8655968904495239,\n",
       "  0.8598330020904541,\n",
       "  0.8350318074226379,\n",
       "  0.8471450209617615,\n",
       "  0.8434174060821533,\n",
       "  0.9392364621162415,\n",
       "  0.8520196676254272,\n",
       "  0.8602150678634644,\n",
       "  0.9173375964164734,\n",
       "  0.864438533782959,\n",
       "  0.8415380716323853,\n",
       "  0.9080812335014343,\n",
       "  0.9047411680221558,\n",
       "  0.8469411730766296,\n",
       "  0.8807488679885864,\n",
       "  0.8520565629005432,\n",
       "  0.885452389717102,\n",
       "  0.8520488142967224,\n",
       "  0.8536513447761536,\n",
       "  0.898004412651062,\n",
       "  0.9548432230949402,\n",
       "  0.8159101605415344,\n",
       "  0.8358070254325867,\n",
       "  0.8424580097198486,\n",
       "  0.8585426807403564,\n",
       "  0.8083207011222839,\n",
       "  0.8352233171463013,\n",
       "  0.9999999403953552,\n",
       "  0.8928627371788025,\n",
       "  0.8285326957702637,\n",
       "  0.8599106669425964,\n",
       "  0.8779503107070923,\n",
       "  0.8328137993812561,\n",
       "  0.8237462043762207,\n",
       "  0.9039025902748108,\n",
       "  0.8566785454750061,\n",
       "  0.841899037361145,\n",
       "  0.8197762966156006,\n",
       "  0.8806026577949524,\n",
       "  0.8670568466186523,\n",
       "  0.9164202809333801,\n",
       "  0.8406956195831299,\n",
       "  0.9182329177856445,\n",
       "  0.8666612505912781,\n",
       "  0.8706185817718506,\n",
       "  0.8601327538490295,\n",
       "  0.8487821817398071,\n",
       "  0.957852303981781,\n",
       "  0.8340720534324646,\n",
       "  0.9572346806526184,\n",
       "  0.8386728763580322,\n",
       "  0.9325511455535889,\n",
       "  0.8186373114585876,\n",
       "  0.8798646330833435,\n",
       "  0.8490642309188843,\n",
       "  0.9737827181816101,\n",
       "  0.8623334169387817,\n",
       "  0.9827779531478882,\n",
       "  0.8769651651382446,\n",
       "  0.8953598737716675,\n",
       "  0.8636515140533447,\n",
       "  0.8266207575798035,\n",
       "  0.8761935830116272,\n",
       "  0.8909462094306946,\n",
       "  0.8558305501937866,\n",
       "  0.8925851583480835,\n",
       "  0.872331976890564,\n",
       "  0.8785498738288879,\n",
       "  0.8665820360183716,\n",
       "  0.8725026249885559,\n",
       "  0.9161627292633057,\n",
       "  0.8974930047988892,\n",
       "  0.8960486650466919,\n",
       "  0.8761686086654663,\n",
       "  0.8831626176834106,\n",
       "  0.8483356237411499,\n",
       "  0.8637537360191345,\n",
       "  0.8791477084159851,\n",
       "  0.8204504251480103,\n",
       "  0.8810189962387085,\n",
       "  0.8734659552574158,\n",
       "  0.8241475820541382,\n",
       "  0.8378791213035583,\n",
       "  0.8497916460037231,\n",
       "  0.8495243787765503,\n",
       "  0.7921726107597351,\n",
       "  0.9087532758712769,\n",
       "  0.8643306493759155,\n",
       "  0.8251416683197021,\n",
       "  0.8793344497680664,\n",
       "  0.8397775292396545,\n",
       "  0.8595443964004517,\n",
       "  0.8416993021965027,\n",
       "  0.7995527982711792,\n",
       "  0.865290641784668,\n",
       "  0.8630806803703308,\n",
       "  0.8279669284820557,\n",
       "  0.8744806051254272,\n",
       "  0.8911835551261902,\n",
       "  0.8133331537246704,\n",
       "  0.8765869140625,\n",
       "  0.8337162733078003,\n",
       "  0.8608885407447815,\n",
       "  0.9019273519515991,\n",
       "  0.8867027163505554,\n",
       "  0.8329867124557495,\n",
       "  0.8603160977363586,\n",
       "  0.8729419112205505,\n",
       "  0.8434433341026306,\n",
       "  0.8460203409194946,\n",
       "  0.8673248291015625,\n",
       "  0.869249701499939,\n",
       "  0.8448569774627686,\n",
       "  0.8844155073165894,\n",
       "  0.8858100175857544,\n",
       "  0.8536601662635803,\n",
       "  0.8515284657478333,\n",
       "  0.8809159398078918,\n",
       "  0.8322716951370239,\n",
       "  0.8556548953056335,\n",
       "  0.9638039469718933,\n",
       "  0.9663190841674805,\n",
       "  0.8465105295181274,\n",
       "  0.9317118525505066,\n",
       "  0.8345532417297363,\n",
       "  0.8957943916320801,\n",
       "  0.8608943223953247,\n",
       "  0.9482018351554871,\n",
       "  0.8543461561203003,\n",
       "  0.9196233153343201,\n",
       "  0.9329017996788025,\n",
       "  0.8791453838348389,\n",
       "  0.9726813435554504,\n",
       "  0.9007693529129028,\n",
       "  0.8800244331359863,\n",
       "  0.8118908405303955,\n",
       "  0.8994332551956177,\n",
       "  0.8824365139007568,\n",
       "  0.9693741798400879,\n",
       "  0.8483155965805054,\n",
       "  0.8547165989875793,\n",
       "  0.8621564507484436,\n",
       "  0.8536593914031982,\n",
       "  0.8597517013549805,\n",
       "  0.8339053392410278,\n",
       "  0.8132011294364929,\n",
       "  0.8431121110916138,\n",
       "  0.8118013143539429,\n",
       "  0.8846641778945923,\n",
       "  0.8576073050498962,\n",
       "  0.814437210559845,\n",
       "  0.8878767490386963,\n",
       "  0.8352367281913757,\n",
       "  0.8410916328430176,\n",
       "  0.8589667081832886,\n",
       "  0.8600658178329468,\n",
       "  0.876442015171051,\n",
       "  0.8325353860855103,\n",
       "  0.9895017147064209,\n",
       "  0.8436951637268066,\n",
       "  0.9838880300521851,\n",
       "  0.8283363580703735,\n",
       "  0.8788111805915833,\n",
       "  0.8412051796913147,\n",
       "  0.8805708885192871,\n",
       "  0.8844971060752869,\n",
       "  0.835487425327301,\n",
       "  0.8630008697509766,\n",
       "  0.8419792056083679,\n",
       "  0.9020993113517761,\n",
       "  0.837397575378418,\n",
       "  0.8722642064094543,\n",
       "  0.8487943410873413,\n",
       "  0.978039562702179,\n",
       "  0.9535108208656311,\n",
       "  0.8448877334594727,\n",
       "  0.7984659671783447,\n",
       "  0.9269248843193054,\n",
       "  0.983616054058075,\n",
       "  0.8389021158218384,\n",
       "  0.8687253594398499,\n",
       "  0.8605399131774902,\n",
       "  0.8723907470703125,\n",
       "  0.826580286026001,\n",
       "  0.8856047987937927,\n",
       "  0.8537020683288574,\n",
       "  0.8184714913368225,\n",
       "  0.9841585159301758,\n",
       "  0.860811173915863,\n",
       "  0.883643388748169,\n",
       "  0.87571120262146,\n",
       "  0.8318049311637878,\n",
       "  0.8522699475288391,\n",
       "  0.844190239906311,\n",
       "  0.8153290748596191,\n",
       "  0.8384981155395508,\n",
       "  0.8336365818977356,\n",
       "  0.8631143569946289,\n",
       "  1.0,\n",
       "  0.8123020529747009,\n",
       "  0.8339998126029968,\n",
       "  0.824575662612915,\n",
       "  0.8986700177192688,\n",
       "  0.8525614738464355,\n",
       "  0.9160945415496826,\n",
       "  0.8351233601570129,\n",
       "  0.8762127161026001,\n",
       "  0.9207318425178528,\n",
       "  0.8432605266571045,\n",
       "  0.8869384527206421,\n",
       "  0.8827062249183655,\n",
       "  0.8090146780014038,\n",
       "  0.825953483581543,\n",
       "  0.8497905135154724,\n",
       "  0.8990873098373413,\n",
       "  0.8416867256164551,\n",
       "  0.8588281273841858,\n",
       "  0.8517959713935852,\n",
       "  0.8581006526947021,\n",
       "  0.8576458096504211,\n",
       "  0.91432124376297,\n",
       "  0.8507658839225769,\n",
       "  0.8763805031776428,\n",
       "  0.8257452845573425,\n",
       "  0.8786797523498535,\n",
       "  0.9066678881645203,\n",
       "  0.8918807506561279,\n",
       "  0.9032423496246338,\n",
       "  0.8443766832351685,\n",
       "  0.8837118744850159,\n",
       "  0.9427160620689392,\n",
       "  0.8849130272865295,\n",
       "  0.8601425886154175,\n",
       "  0.9999999403953552,\n",
       "  0.8098626136779785,\n",
       "  0.9064292311668396,\n",
       "  0.814004123210907,\n",
       "  0.8886477947235107,\n",
       "  0.844683051109314,\n",
       "  0.8999344110488892,\n",
       "  0.923454999923706,\n",
       "  0.8334223628044128,\n",
       "  0.9710109829902649,\n",
       "  0.9853259921073914,\n",
       "  0.8028770089149475,\n",
       "  0.8549289107322693,\n",
       "  0.8371893763542175,\n",
       "  0.8120744228363037,\n",
       "  0.8312062621116638,\n",
       "  0.8920419216156006,\n",
       "  0.8625447154045105,\n",
       "  0.9999999403953552,\n",
       "  0.9565747976303101,\n",
       "  0.7896623015403748,\n",
       "  0.8108048439025879,\n",
       "  0.8627416491508484,\n",
       "  0.8550719618797302,\n",
       "  0.8507565259933472,\n",
       "  0.8377590775489807,\n",
       "  0.8231591582298279,\n",
       "  0.8625550270080566,\n",
       "  0.8923426270484924,\n",
       "  0.8645210266113281,\n",
       "  0.8821824193000793,\n",
       "  0.8296131491661072,\n",
       "  0.8348699808120728,\n",
       "  0.8257638812065125,\n",
       "  0.862878143787384,\n",
       "  0.8841148614883423,\n",
       "  0.8717387318611145,\n",
       "  0.8482329845428467,\n",
       "  0.8347423076629639,\n",
       "  0.9347943663597107,\n",
       "  0.8923290967941284,\n",
       "  0.8617150187492371,\n",
       "  0.9399036765098572,\n",
       "  0.7935875654220581,\n",
       "  0.7952665090560913,\n",
       "  0.864210307598114,\n",
       "  0.8473915457725525,\n",
       "  0.9021181464195251,\n",
       "  0.8653855919837952,\n",
       "  0.8400540947914124,\n",
       "  0.852405846118927,\n",
       "  0.8321821093559265,\n",
       "  0.899803876876831,\n",
       "  0.8383798003196716,\n",
       "  0.7964111566543579,\n",
       "  0.8585651516914368,\n",
       "  0.8299683928489685,\n",
       "  0.8463588356971741,\n",
       "  0.8573530316352844,\n",
       "  0.8530533313751221,\n",
       "  0.8589696884155273,\n",
       "  0.8277950286865234,\n",
       "  0.864872932434082,\n",
       "  0.82645183801651,\n",
       "  0.9022204875946045,\n",
       "  1.0,\n",
       "  0.8443094491958618,\n",
       "  0.8688214421272278,\n",
       "  0.7957627773284912,\n",
       "  0.8430042266845703,\n",
       "  0.9270814657211304,\n",
       "  0.8168943524360657,\n",
       "  0.8708468079566956,\n",
       "  0.8456714749336243,\n",
       "  0.9279096126556396,\n",
       "  0.8831592202186584,\n",
       "  0.8750013113021851,\n",
       "  0.8993200659751892,\n",
       "  0.821805477142334,\n",
       "  0.9030686616897583,\n",
       "  0.8659058809280396,\n",
       "  0.8434837460517883,\n",
       "  0.9238450527191162,\n",
       "  0.876460075378418,\n",
       "  0.8851213455200195,\n",
       "  0.8607847690582275,\n",
       "  0.8915116190910339,\n",
       "  0.928188681602478,\n",
       "  0.8738143444061279,\n",
       "  0.875851571559906,\n",
       "  0.8834355473518372,\n",
       "  0.8763301968574524,\n",
       "  0.8508285284042358,\n",
       "  0.8838756680488586,\n",
       "  0.8525972366333008,\n",
       "  0.8434447646141052,\n",
       "  0.8372975587844849,\n",
       "  0.8391461372375488,\n",
       "  0.8718886375427246,\n",
       "  0.8673153519630432,\n",
       "  0.8480877876281738,\n",
       "  0.8573829531669617,\n",
       "  0.8543875217437744,\n",
       "  0.9319702386856079,\n",
       "  0.832273542881012,\n",
       "  0.8859967589378357,\n",
       "  0.8569750785827637,\n",
       "  0.8491442203521729,\n",
       "  0.7731518149375916,\n",
       "  0.866138219833374,\n",
       "  0.835273027420044,\n",
       "  0.851225733757019,\n",
       "  0.8355220556259155,\n",
       "  0.8369620442390442,\n",
       "  0.8687710165977478,\n",
       "  0.9570434093475342,\n",
       "  0.8636427521705627,\n",
       "  0.8637707829475403,\n",
       "  0.8321537971496582,\n",
       "  0.8377048969268799,\n",
       "  0.8448562622070312,\n",
       "  0.8679243326187134,\n",
       "  0.8430753946304321,\n",
       "  0.8721885681152344,\n",
       "  0.8159873485565186,\n",
       "  0.8556950688362122,\n",
       "  0.8778107762336731,\n",
       "  0.859941840171814,\n",
       "  0.8607488870620728,\n",
       "  0.7889025807380676,\n",
       "  0.8570947647094727,\n",
       "  0.8477758169174194,\n",
       "  0.827949583530426,\n",
       "  1.0,\n",
       "  0.8660868406295776,\n",
       "  0.934531569480896,\n",
       "  0.8301693201065063,\n",
       "  0.8406556844711304,\n",
       "  0.8902280330657959,\n",
       "  0.8585248589515686,\n",
       "  0.8470969200134277,\n",
       "  0.900073230266571,\n",
       "  0.8868542909622192,\n",
       "  0.8317157626152039,\n",
       "  0.8321738243103027,\n",
       "  0.8490459322929382,\n",
       "  0.8493388295173645,\n",
       "  0.8433752655982971,\n",
       "  0.8978384137153625,\n",
       "  0.8826582431793213,\n",
       "  0.8326060771942139,\n",
       "  0.8610637784004211,\n",
       "  0.892244279384613,\n",
       "  0.8490914106369019,\n",
       "  0.9338811635971069,\n",
       "  0.8769911527633667,\n",
       "  0.7958442568778992,\n",
       "  0.891232430934906,\n",
       "  0.8830968141555786,\n",
       "  0.9030663371086121,\n",
       "  0.8660590052604675,\n",
       "  0.8956822752952576,\n",
       "  0.8740345239639282,\n",
       "  0.8493444323539734,\n",
       "  0.8262986540794373,\n",
       "  0.8442702293395996,\n",
       "  0.8588511347770691,\n",
       "  0.8583658933639526,\n",
       "  0.8504042029380798,\n",
       "  0.8435772061347961,\n",
       "  0.935938835144043,\n",
       "  0.8440321683883667,\n",
       "  0.8842309713363647,\n",
       "  0.809705913066864,\n",
       "  0.886719822883606,\n",
       "  0.8559620976448059,\n",
       "  0.8829753398895264,\n",
       "  0.8433151245117188,\n",
       "  0.922926127910614,\n",
       "  0.8188772797584534,\n",
       "  0.9668381214141846,\n",
       "  0.8975334167480469,\n",
       "  0.8550369143486023,\n",
       "  0.8440064787864685,\n",
       "  0.8883161544799805,\n",
       "  0.8378400206565857,\n",
       "  0.8693215847015381,\n",
       "  0.8500890135765076,\n",
       "  0.9203798174858093,\n",
       "  0.8350245952606201,\n",
       "  0.8553173542022705,\n",
       "  0.8669540882110596,\n",
       "  0.8211981058120728,\n",
       "  0.8632310032844543,\n",
       "  0.8436699509620667,\n",
       "  0.791682243347168,\n",
       "  0.9828202128410339,\n",
       "  0.8779042363166809,\n",
       "  0.8520503044128418,\n",
       "  0.8338144421577454,\n",
       "  0.8904121518135071,\n",
       "  0.861588180065155,\n",
       "  0.895281195640564,\n",
       "  0.8945918083190918,\n",
       "  0.8286572098731995,\n",
       "  0.8661980032920837,\n",
       "  0.9061100482940674,\n",
       "  0.7913613319396973,\n",
       "  0.8709604740142822,\n",
       "  0.8844534158706665,\n",
       "  0.9999998807907104,\n",
       "  0.8773384690284729,\n",
       "  0.8399562239646912,\n",
       "  0.8605228662490845,\n",
       "  0.8428941369056702,\n",
       "  0.8511773943901062,\n",
       "  0.9301424026489258,\n",
       "  0.8387118577957153,\n",
       "  0.8511306047439575,\n",
       "  0.8754416108131409,\n",
       "  0.8519181609153748,\n",
       "  0.8660532236099243,\n",
       "  0.8326188921928406,\n",
       "  0.8486377000808716,\n",
       "  0.8546127676963806,\n",
       "  1.0000001192092896,\n",
       "  0.8507526516914368,\n",
       "  0.8477585315704346,\n",
       "  0.8595037460327148,\n",
       "  0.8615977764129639,\n",
       "  0.8422430157661438,\n",
       "  0.8514871001243591,\n",
       "  0.8850170373916626,\n",
       "  0.8496813774108887,\n",
       "  0.833060622215271,\n",
       "  0.8497569561004639,\n",
       "  0.8546833992004395,\n",
       "  0.8239662051200867,\n",
       "  0.8555863499641418,\n",
       "  0.901669979095459,\n",
       "  0.8222408890724182,\n",
       "  0.908441424369812,\n",
       "  0.8978646397590637,\n",
       "  0.8318539261817932,\n",
       "  0.8362732529640198,\n",
       "  0.8682995438575745,\n",
       "  0.8099725246429443,\n",
       "  0.8386299014091492,\n",
       "  0.8952146768569946,\n",
       "  0.8983157873153687,\n",
       "  0.8671368360519409,\n",
       "  0.8819711804389954,\n",
       "  0.9674609899520874,\n",
       "  0.8600296974182129,\n",
       "  0.8618814945220947,\n",
       "  0.8338766694068909,\n",
       "  0.8700482249259949,\n",
       "  0.8684157133102417,\n",
       "  0.930293083190918,\n",
       "  0.8494174480438232,\n",
       "  0.8498876094818115,\n",
       "  0.8268581628799438,\n",
       "  0.9130652546882629,\n",
       "  0.8958960771560669,\n",
       "  0.8239126205444336,\n",
       "  0.8925525546073914,\n",
       "  0.8383216857910156,\n",
       "  0.8409048318862915,\n",
       "  0.8981903791427612,\n",
       "  0.8650818467140198,\n",
       "  0.8696374297142029,\n",
       "  0.989222526550293,\n",
       "  0.8574417233467102,\n",
       "  0.9016648530960083,\n",
       "  0.8359116315841675,\n",
       "  0.8727172017097473,\n",
       "  0.8981846570968628,\n",
       "  0.8788925409317017,\n",
       "  0.8630770444869995,\n",
       "  0.877244234085083,\n",
       "  0.7546696066856384,\n",
       "  0.8389731645584106,\n",
       "  0.882138192653656,\n",
       "  0.8342995643615723,\n",
       "  0.8477840423583984,\n",
       "  0.862179160118103,\n",
       "  0.8229125738143921,\n",
       "  0.8560839295387268,\n",
       "  0.894708514213562,\n",
       "  0.902941107749939,\n",
       "  0.8690520524978638,\n",
       "  0.8952585458755493,\n",
       "  0.8635271191596985,\n",
       "  0.9032901525497437,\n",
       "  0.8565034866333008,\n",
       "  0.8835882544517517,\n",
       "  0.9202798008918762,\n",
       "  0.8332631587982178,\n",
       "  0.8579949140548706,\n",
       "  0.9388526082038879,\n",
       "  0.8127043843269348,\n",
       "  0.83745938539505,\n",
       "  0.8286629915237427,\n",
       "  0.8333545923233032,\n",
       "  0.869346559047699,\n",
       "  0.8300343155860901,\n",
       "  0.8697481751441956,\n",
       "  0.908656895160675,\n",
       "  0.8901127576828003,\n",
       "  0.8943976163864136,\n",
       "  0.8061111569404602,\n",
       "  0.8464826345443726,\n",
       "  0.9586167931556702,\n",
       "  0.8518286347389221,\n",
       "  0.8451838493347168,\n",
       "  0.9226639866828918,\n",
       "  0.872397243976593,\n",
       "  0.8770294189453125,\n",
       "  0.8687361478805542,\n",
       "  0.8783619999885559,\n",
       "  0.924294114112854,\n",
       "  0.9999999403953552,\n",
       "  0.8351004123687744,\n",
       "  0.8101023435592651,\n",
       "  0.8849156498908997,\n",
       "  0.8572811484336853,\n",
       "  0.8334032297134399,\n",
       "  0.864240825176239,\n",
       "  0.8867692947387695,\n",
       "  0.8173285722732544,\n",
       "  0.8215629458427429,\n",
       "  0.8715243339538574,\n",
       "  0.9975258111953735,\n",
       "  0.8504458069801331,\n",
       "  0.8698455095291138,\n",
       "  0.8302863240242004,\n",
       "  0.8707520365715027,\n",
       "  0.8415019512176514,\n",
       "  0.8446040153503418,\n",
       "  0.8703310489654541,\n",
       "  0.8723856806755066,\n",
       "  1.0,\n",
       "  0.8654008507728577,\n",
       "  0.8532114028930664,\n",
       "  0.8045240044593811,\n",
       "  0.8676397204399109,\n",
       "  0.8999422788619995,\n",
       "  0.8633584380149841,\n",
       "  0.8233615159988403,\n",
       "  0.8439563512802124,\n",
       "  0.8035848736763,\n",
       "  0.8346479535102844,\n",
       "  0.9072025418281555,\n",
       "  0.8379164338111877,\n",
       "  0.850597083568573,\n",
       "  0.9410753846168518,\n",
       "  0.8369494676589966,\n",
       "  0.8673548102378845,\n",
       "  0.8695522546768188,\n",
       "  0.9577322602272034,\n",
       "  0.8489952087402344,\n",
       "  0.8409113883972168,\n",
       "  0.8917872905731201,\n",
       "  0.8878660202026367,\n",
       "  0.8479601144790649,\n",
       "  0.9525105953216553,\n",
       "  0.8307480812072754,\n",
       "  0.8714020848274231,\n",
       "  0.9696255326271057,\n",
       "  0.8365554213523865,\n",
       "  0.8712660670280457,\n",
       "  0.9844908118247986,\n",
       "  0.8115577697753906,\n",
       "  0.9081318974494934,\n",
       "  0.8714564442634583,\n",
       "  0.8605644106864929,\n",
       "  0.8704045414924622,\n",
       "  0.855981707572937,\n",
       "  0.8343636393547058,\n",
       "  0.9056256413459778,\n",
       "  0.863198459148407,\n",
       "  0.8582200407981873,\n",
       "  0.8763113617897034,\n",
       "  0.8585666418075562,\n",
       "  0.8844460248947144,\n",
       "  0.8923357129096985,\n",
       "  0.9268913269042969,\n",
       "  0.9293854832649231,\n",
       "  1.0,\n",
       "  0.8762811422348022,\n",
       "  0.8031359314918518,\n",
       "  0.8684444427490234,\n",
       "  0.8442270755767822,\n",
       "  0.8827157616615295,\n",
       "  0.8267961144447327,\n",
       "  0.8453028202056885,\n",
       "  0.8466628193855286,\n",
       "  0.8937997817993164,\n",
       "  0.8593392372131348,\n",
       "  0.8638885617256165,\n",
       "  0.8599722385406494,\n",
       "  0.8449349403381348,\n",
       "  0.8931173086166382,\n",
       "  0.8847082257270813,\n",
       "  0.9166542291641235,\n",
       "  0.8541103601455688,\n",
       "  0.8527089357376099,\n",
       "  0.8519701361656189,\n",
       "  0.8535268306732178,\n",
       "  0.8246285915374756,\n",
       "  0.9417299628257751,\n",
       "  0.8296890258789062,\n",
       "  0.8597634434700012,\n",
       "  0.8522364497184753,\n",
       "  0.8722571730613708,\n",
       "  0.8400075435638428,\n",
       "  0.8731436133384705,\n",
       "  0.894262969493866,\n",
       "  0.8452872037887573,\n",
       "  0.9398923516273499,\n",
       "  0.8853464126586914,\n",
       "  0.8288644552230835,\n",
       "  1.0000001192092896,\n",
       "  0.8607180714607239,\n",
       "  0.8945278525352478,\n",
       "  0.9474139213562012,\n",
       "  0.8237723708152771,\n",
       "  0.8935341238975525,\n",
       "  0.8501232266426086,\n",
       "  0.8730216026306152,\n",
       "  0.8819910883903503,\n",
       "  0.8742787837982178,\n",
       "  0.849158763885498,\n",
       "  1.0,\n",
       "  0.8842893242835999,\n",
       "  0.904685914516449,\n",
       "  0.7889953255653381,\n",
       "  0.8797966241836548,\n",
       "  0.855415940284729,\n",
       "  0.8880761861801147,\n",
       "  0.8401100039482117,\n",
       "  0.8529970049858093,\n",
       "  0.8978129625320435,\n",
       "  0.8490826487541199,\n",
       "  0.8478215932846069,\n",
       "  1.0,\n",
       "  0.8411767482757568,\n",
       "  0.8289107084274292,\n",
       "  0.9327865839004517,\n",
       "  0.8024747371673584,\n",
       "  0.8942760825157166,\n",
       "  0.8545411229133606,\n",
       "  0.8729591965675354,\n",
       "  0.8786584138870239,\n",
       "  0.8171859979629517,\n",
       "  0.9216346740722656,\n",
       "  0.8455443978309631,\n",
       "  0.8660640716552734,\n",
       "  0.8605724573135376,\n",
       "  0.9120822548866272,\n",
       "  0.8838193416595459,\n",
       "  0.8454811573028564,\n",
       "  0.8837044835090637,\n",
       "  0.8470617532730103,\n",
       "  0.8245016932487488,\n",
       "  0.8707432150840759,\n",
       "  0.873454749584198,\n",
       "  0.8209933042526245,\n",
       "  0.879094123840332,\n",
       "  0.8375548124313354,\n",
       "  0.8399364352226257,\n",
       "  0.8512423038482666,\n",
       "  0.8569470047950745,\n",
       "  0.8276984691619873,\n",
       "  0.8530008792877197,\n",
       "  0.8258866667747498,\n",
       "  0.8310039043426514,\n",
       "  0.8401651382446289,\n",
       "  0.858491837978363,\n",
       "  0.8645069599151611,\n",
       "  0.8601896166801453,\n",
       "  0.86592036485672,\n",
       "  0.8017408847808838,\n",
       "  0.8932003974914551,\n",
       "  0.8267731666564941,\n",
       "  0.8691824674606323,\n",
       "  0.8659170269966125,\n",
       "  0.8084420561790466,\n",
       "  0.9228727221488953,\n",
       "  0.978609561920166,\n",
       "  0.9505877494812012,\n",
       "  0.842825710773468,\n",
       "  0.8009341955184937,\n",
       "  0.8609992861747742,\n",
       "  0.8619382381439209,\n",
       "  0.8909046649932861,\n",
       "  0.8864419460296631,\n",
       "  0.8947423696517944,\n",
       "  0.9792786836624146,\n",
       "  0.8101808428764343,\n",
       "  0.9449273347854614,\n",
       "  0.898041307926178,\n",
       "  0.8764237761497498,\n",
       "  0.8671205043792725,\n",
       "  0.9450560808181763,\n",
       "  0.8419232368469238,\n",
       "  0.8905469179153442,\n",
       "  0.9074088931083679,\n",
       "  1.0,\n",
       "  0.8225968480110168,\n",
       "  0.9826173186302185,\n",
       "  0.9999999403953552,\n",
       "  0.8590803742408752,\n",
       "  0.8506972193717957,\n",
       "  1.0,\n",
       "  0.8447993397712708,\n",
       "  0.8747673630714417,\n",
       "  0.8483026027679443,\n",
       "  0.8101497292518616,\n",
       "  0.8373832106590271,\n",
       "  0.8452190160751343,\n",
       "  0.8364670872688293,\n",
       "  0.8333613276481628,\n",
       "  0.8493128418922424,\n",
       "  0.7854689955711365,\n",
       "  0.8282357454299927,\n",
       "  0.8755492568016052,\n",
       "  0.8709222078323364,\n",
       "  0.853697657585144,\n",
       "  0.8432747721672058,\n",
       "  0.8737233281135559,\n",
       "  0.8849188685417175,\n",
       "  0.8786087036132812,\n",
       "  0.8280302286148071,\n",
       "  0.8493121862411499,\n",
       "  0.8447650671005249,\n",
       "  0.9999999403953552,\n",
       "  0.8930474519729614,\n",
       "  0.8237252235412598,\n",
       "  0.8632857203483582,\n",
       "  0.8161116242408752,\n",
       "  0.8890547752380371,\n",
       "  0.8441396951675415,\n",
       "  0.9025768041610718,\n",
       "  0.8689430356025696,\n",
       "  0.8621941208839417,\n",
       "  0.8246270418167114,\n",
       "  0.9501523971557617,\n",
       "  0.9554930329322815,\n",
       "  0.89351886510849,\n",
       "  0.8849489688873291,\n",
       "  0.9277713298797607,\n",
       "  0.8561742901802063,\n",
       "  0.8979887962341309,\n",
       "  0.8675855398178101,\n",
       "  0.8266505002975464,\n",
       "  0.8249109387397766,\n",
       "  0.8405511975288391,\n",
       "  0.7998396158218384,\n",
       "  0.8449615836143494,\n",
       "  0.7876518368721008,\n",
       "  0.8245818018913269,\n",
       "  0.8408851623535156,\n",
       "  0.8413172364234924,\n",
       "  0.8983243107795715,\n",
       "  0.8843062520027161,\n",
       "  0.8690923452377319,\n",
       "  0.8253685832023621,\n",
       "  0.8536232113838196,\n",
       "  0.8350974321365356,\n",
       "  0.8700544834136963,\n",
       "  0.8762845396995544,\n",
       "  0.8385034799575806,\n",
       "  0.8289968967437744,\n",
       "  0.87015300989151,\n",
       "  0.8910803198814392,\n",
       "  0.889147162437439,\n",
       "  0.8410418629646301,\n",
       "  0.8806170225143433,\n",
       "  0.8027535080909729,\n",
       "  0.8614093065261841,\n",
       "  0.89540034532547,\n",
       "  0.8659225702285767,\n",
       "  0.8696984052658081,\n",
       "  0.8277430534362793,\n",
       "  0.8081490397453308,\n",
       "  0.8319334387779236,\n",
       "  0.8642026782035828,\n",
       "  0.8791822791099548,\n",
       "  0.8479661345481873,\n",
       "  0.8051642179489136,\n",
       "  0.858866274356842,\n",
       "  0.8170061707496643,\n",
       "  0.9755794405937195,\n",
       "  0.8540967702865601,\n",
       "  0.7944062948226929,\n",
       "  0.816917359828949,\n",
       "  0.8861926198005676,\n",
       "  0.8294491171836853,\n",
       "  0.8693967461585999,\n",
       "  0.8526121377944946,\n",
       "  0.8505721092224121,\n",
       "  0.9025319814682007,\n",
       "  0.890981912612915,\n",
       "  0.8220130205154419,\n",
       "  0.8421509265899658,\n",
       "  0.886204719543457,\n",
       "  0.8349605798721313,\n",
       "  0.8642312288284302,\n",
       "  0.801724910736084,\n",
       "  0.8489486575126648,\n",
       "  0.8540171980857849,\n",
       "  0.8562495708465576,\n",
       "  0.8347989916801453,\n",
       "  0.9155381321907043,\n",
       "  0.8439658284187317,\n",
       "  0.845998227596283,\n",
       "  0.856631875038147,\n",
       "  0.8632347583770752,\n",
       "  0.8942162394523621,\n",
       "  0.8323686122894287,\n",
       "  0.8680725693702698,\n",
       "  0.8469052910804749,\n",
       "  0.9342904686927795,\n",
       "  0.8733733892440796,\n",
       "  0.827221691608429,\n",
       "  0.8650286793708801,\n",
       "  0.8602080941200256,\n",
       "  0.8405874967575073,\n",
       "  0.883234977722168,\n",
       "  0.8682767152786255,\n",
       "  0.805736780166626,\n",
       "  0.9317569136619568,\n",
       "  0.8360603451728821,\n",
       "  0.885673999786377,\n",
       "  0.8282637596130371,\n",
       "  0.9579881429672241,\n",
       "  0.8797672986984253,\n",
       "  0.8397030830383301,\n",
       "  0.8568492531776428,\n",
       "  0.8007104992866516,\n",
       "  0.8285215497016907,\n",
       "  0.948536217212677,\n",
       "  0.8658722639083862,\n",
       "  0.9498575925827026,\n",
       "  0.8940137624740601,\n",
       "  0.8738967180252075,\n",
       "  0.8881810903549194,\n",
       "  0.8220133185386658,\n",
       "  0.8582455515861511,\n",
       "  0.867039144039154,\n",
       "  1.0,\n",
       "  0.885028600692749,\n",
       "  0.8908548951148987,\n",
       "  0.9962456822395325,\n",
       "  0.9490259289741516,\n",
       "  0.9027096033096313,\n",
       "  0.8880127668380737,\n",
       "  0.8445087671279907,\n",
       "  0.86916184425354,\n",
       "  0.845136821269989,\n",
       "  0.8174886703491211,\n",
       "  0.9856932759284973,\n",
       "  0.8584665656089783,\n",
       "  0.8675286173820496,\n",
       "  0.895248293876648,\n",
       "  0.8509160876274109,\n",
       "  0.8941331505775452,\n",
       "  0.8419947028160095,\n",
       "  0.9294401407241821,\n",
       "  0.8972241282463074,\n",
       "  0.8513286709785461,\n",
       "  0.8175437450408936,\n",
       "  0.8685314059257507,\n",
       "  0.8525177836418152,\n",
       "  0.8554811477661133,\n",
       "  0.8681052923202515,\n",
       "  0.8561819195747375,\n",
       "  0.835940420627594,\n",
       "  0.7949375510215759,\n",
       "  0.8690690994262695,\n",
       "  0.8756601810455322,\n",
       "  0.8482552170753479,\n",
       "  0.851425051689148,\n",
       "  0.8612954616546631,\n",
       "  0.8407415747642517,\n",
       "  0.8534809947013855,\n",
       "  0.8744040727615356,\n",
       "  0.888737142086029,\n",
       "  0.8166633248329163,\n",
       "  0.8529573082923889,\n",
       "  0.8197394013404846,\n",
       "  0.8749498128890991,\n",
       "  0.9768362641334534,\n",
       "  0.8917610049247742,\n",
       "  0.825401782989502,\n",
       "  0.8720404505729675,\n",
       "  0.8264614343643188,\n",
       "  0.8310813307762146,\n",
       "  0.8357733488082886,\n",
       "  0.8489680886268616,\n",
       "  0.8210828900337219,\n",
       "  0.849428117275238,\n",
       "  0.9215980768203735,\n",
       "  0.8492286205291748,\n",
       "  0.8206230401992798,\n",
       "  0.8907651305198669,\n",
       "  ...],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  BertScore  ..\n",
    "bertscore = load(\"bertscore\")\n",
    "results = bertscore.compute(predictions=candidates, references=references, lang=\"en\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a295e39-4de6-43e5-a585-50db89ea6c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e715a3c684384cfd9b7a64dc87237653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd3f5f29c2a447882b675df81fde7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9749fc73bb4003bfa6a15238e54875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ded6cd397048cd9078255150bd73ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1f870d3c9148e5961d06cf252132c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'perplexities': [17.221481323242188, 21.034292221069336],\n",
       "  'mean_perplexity': 19.12788677215576},\n",
       " {'perplexities': [143.34274291992188, 35.562015533447266],\n",
       "  'mean_perplexity': 89.45237922668457},\n",
       " {'perplexities': [49.05657196044922, 31.306514739990234],\n",
       "  'mean_perplexity': 40.18154335021973},\n",
       " {'perplexities': [57.313507080078125, 6.009933948516846],\n",
       "  'mean_perplexity': 31.661720514297485},\n",
       " {'perplexities': [34.36557388305664, 37.05070114135742],\n",
       "  'mean_perplexity': 35.70813751220703}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\",  module_type= \"measurement\")\n",
    "perplexity_scores = []\n",
    "for i in range(5):\n",
    "    input_texts = [references[i], candidates[i]]\n",
    "    results = perplexity.compute(data=input_texts, model_id='gpt2')\n",
    "    perplexity_scores.append(results)\n",
    "    \n",
    "perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ecc883-ebe3-4082-8e57-bab4f8870d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76536b-3d8f-4a10-ade1-b9c76205264e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d2af1-336d-45d1-8361-d76238a7bb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639fb9e-7834-4147-9c6e-f1245a299bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc6f5b-c183-4f9a-9483-7809759217dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_data = pd.read_excel('/kaggle/input/my-data/Test data.xlsx')\n",
    "for i in range(my_data.shape[0]):\n",
    "    inputs = 'question: ' + my_data.iloc[i]['Rephrases'] + ' Context: ' + my_data.iloc[i]['Context']\n",
    "    orig_ans = my_data.iloc[i]['Answers']\n",
    "    model_input = tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\") \n",
    "\n",
    "    generated_answers_encoded = model_0.generate(input_ids=model_input[\"input_ids\"].to(device),\n",
    "                                                attention_mask=model_input[\"attention_mask\"].to(device),min_length=16, max_length=64,\n",
    "                                                do_sample=False, early_stopping=True, num_beams=8, temperature=1.0, top_k=None,\n",
    "                                                top_p=None, eos_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3,\n",
    "                                                num_return_sequences=1)\n",
    "    ans_0 = tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "\n",
    "    generated_answers_encoded = model.generate(input_ids=model_input[\"input_ids\"].to(device),\n",
    "                                                attention_mask=model_input[\"attention_mask\"].to(device),min_length=16, max_length=64,\n",
    "                                                do_sample=False, early_stopping=True, num_beams=8, temperature=1.0, top_k=None,\n",
    "                                                top_p=None, eos_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3,\n",
    "                                                num_return_sequences=1)\n",
    "    ans_1 = tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "  \n",
    "    \n",
    "    candidates_0.append(ans_0)\n",
    "    candidates_1.append(ans_1)\n",
    "    references.append(orig_ans)\n",
    "  \n",
    "    print (str(i)+'. '+inputs)\n",
    "    print('\\n')\n",
    "    print('Reference - ', orig_ans)\n",
    "    print('-------')\n",
    "    print('Candidate_0 - ', ans_0)\n",
    "    print('-------')\n",
    "    print('Candidate_1 - ', ans_1)\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e02fdc-fbc5-4c66-9b59-650e3cd637f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai,os,sys\n",
    "prompt = sys.argv[1]\n",
    "\n",
    "openai.api_key = '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'\n",
    "prompt = \"Where is capital of India?\"\n",
    "completions = openai.Completion.create(engine=\"text-davinci-003\",prompt=prompt, max_tokens=1024, n=1, stop=None, temperature=0.5)\n",
    "message = completions.choices[0].text\n",
    "print(message)\n",
    "\n",
    "gen_ans = []\n",
    "for i in range(500):\n",
    "    Question = df1.iloc[i]['Questions']\n",
    "    Context = df1.iloc[i]['Passages']\n",
    "    Answer = df1.iloc[i]['Answers']\n",
    "    prompt = Question+', '+'context : ' +Context\n",
    "    completions = openai.Completion.create(engine=\"text-davinci-003\",prompt=prompt, max_tokens=1024, n=1, stop=None, temperature=0.5)\n",
    "    message = completions.choices[0].text\n",
    "    gen_ans.append(message[2:])\n",
    "    \n",
    "    print('1. Question - ', Question)\n",
    "    print('2. Answer - ', Answer)\n",
    "    print('3. Generated Answer - ', message[2:])\n",
    "    print('4. Context - ', Context)\n",
    "    print('--------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c044bf-3a42-4af5-9bad-73c5c1465f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3e130-c279-496c-bae2-7ed74a38057d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff437f-e05a-4550-99a4-e764c64fea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "turabit_data = pd.read_excel('Test data.xlsx')\n",
    "turabit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50723734-2e5f-4fdc-8189-48ac37451560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = turabit_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca38e34b-2982-47c7-b36a-d5f0a64fd27f",
   "metadata": {},
   "source": [
    "for i in range(df1.shape[0]):\n",
    "    Question = df1.iloc[i]['Questions']\n",
    "    Context = df1.iloc[i]['Context']\n",
    "    Answer = df1.iloc[i]['Answers']\n",
    "    prompt = Question+', '+'context : ' +Context\n",
    "    completions = openai.Completion.create(engine=\"text-davinci-003\",prompt=prompt, max_tokens=1024, n=1, stop=None, temperature=0.5)\n",
    "    message = completions.choices[0].text\n",
    "        \n",
    "    print('1. Question - ', Question)\n",
    "    print('2. Answer - ', Answer)\n",
    "    print('3. Generated Answer - ', message[2:])\n",
    "    print('\\n')\n",
    "    print('4. Context - ', Context)\n",
    "    print('--------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4d9f8-fbd6-4c54-bbdd-6246c19d027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('Testing data.xlsx')\n",
    "df1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2dc028c-cfd8-4e11-921f-54b149f79745",
   "metadata": {},
   "source": [
    "for i in range(df1.shape[0]):\n",
    "    Question = df1.iloc[i]['Phrases']\n",
    "    Context = df1.iloc[i]['Original content']\n",
    "    Answer = df1.iloc[i]['Answer']\n",
    "    prompt = Question+', '+'context : ' +Context\n",
    "    completions = openai.Completion.create(engine=\"text-davinci-003\",prompt=prompt, max_tokens=1024, n=1, stop=None, temperature=0.5)\n",
    "    message = completions.choices[0].text\n",
    "        \n",
    "    print('1. Question - ', Question)\n",
    "    print('2. Answer - ', Answer)\n",
    "    print('3. Generated Answer - ', message[2:])\n",
    "    print('\\n')\n",
    "    print('4. Context - ', Context)\n",
    "    print('--------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0ccf4-abc6-4919-9385-a2e13adb0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bart_lfqa\")\n",
    "tokenizer(query_and_docs, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f231a8e8-1230-4779-a26a-ba430ab15ce2",
   "metadata": {},
   "source": [
    "query = \"Why does water heated to room temperature feel colder than the air around it?\"\n",
    "documents = [\"when the skin is completely wet. The body continuously loses water by...\",\n",
    "                  \"at greater pressures. There is an ambiguity, however, as to the meaning of the terms 'heating' and 'cooling'...\",\n",
    "                  \"are not in a relation of thermal equilibrium, heat will flow from the hotter to the colder, by whatever pathway...\",\n",
    "                  \"air condition and moving along a line of constant enthalpy toward a state of higher humidity. A simple example ...\",            \n",
    "                  \"Thermal contact conductance In physics, thermal contact conductance is the study of heat conduction between solid ...\"]\n",
    "\n",
    "conditioned_doc = \"<P> \" + \" <P> \".join([d for d in documents])\n",
    "query_and_docs = \"question: {} context: {}\".format(query, conditioned_doc)\n",
    "query_and_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
